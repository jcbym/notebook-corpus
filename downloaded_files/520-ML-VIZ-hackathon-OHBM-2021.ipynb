{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Core concepts\n",
    "\n",
    "In this notebook, we will review:\n",
    "- Estimators in _scikit-learn_, and some of their functions.\n",
    "- How estimators can be supervised models that perform classification or regression tasks, as well as unsupervised models.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some important concepts\n",
    "\n",
    "Let's quickly review some conceptual distinctions in Machine Learning (ML). This section is a refresher. If you are lacking some knowledge on these concepts, please consult our suggested reading in [Notebook 1](./01-preliminaries.ipynb).\n",
    "\n",
    "## What machine learning is about\n",
    "An excellent working definition of ML can be found in [Tal Yarkoni's tutorial](https://github.com/neurohackademy/nh2020-curriculum/blob/master/tu-machine-learning-yarkoni/02-core-concepts.ipynb): ML is the field of science/engineering that seeks to build systems capable of learning from experience. The goal of ML is to develop algorithms that can learn from data with a minimum set of explicitly programmed rules on how to do so.\n",
    "\n",
    "There are two main types of ML models depending on how they learn from data: supervised and unsupervised.\n",
    "\n",
    "## Supervised ML\n",
    "In supervised ML, we have available the real values of the variables we want to predict. The model can then use this information to train itself by comparing its predicted values with the real ones using a __loss function__, and an __optimization algorithm__ to iteratively make small adjustments and improve its perfomance. \n",
    "\n",
    "### Regression vs classification\n",
    "Supervised learning models can also be divided into regression and classification tasks. Regression models seek to predict a continuous variable (e.g. age), while classification models predict discrete labels (e.g. wine class).\n",
    "\n",
    "## Unsupervised ML\n",
    "In unsupervised ML, these labels are unkown. The algorithm instead seeks to find a pattern in the data that might be useful.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimators\n",
    "\n",
    "In _scikit-learn_ an [estimator](https://scikit-learn.org/stable/tutorial/statistical_inference/settings.html#estimators-objects) is a Python object that __learns from data__. That means, both supervised (classification or regression) and unsupervised models can be constructed and fitted using estimators. We will review some properties of estimators in _scikit-learn_ using an example for each of these types of models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "A linear regression is an example of a supervised regression model. Used as a machine learning tool, linear regression predicts the values of a continuous variable from a __linear combination__ of one or more features.\n",
    "\n",
    "For example, if we had a feature matrix $X$ contaning the values of features $x1$ and $x2$, the value $\\hat{y}$ predicted by linear regression could be expressed as:\n",
    "\n",
    "$$\\hat{y}_i = \\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2}$$\n",
    "\n",
    "> - where $\\beta$ are the parameters the model learns from the data to make the predictions\n",
    "> - $\\beta_1$ and $\\beta_2$ are also called the coefficients, and $\\beta_0$ the intercept\n",
    "\n",
    "Let's now see how we can fit a linear regression model using _scikit-learn_. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first need to create a dataset for this exercise. With _scikit-learn_ we can do so using the `make_regression()` function (read the documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html#sklearn.datasets.make_regression)). Let's create one containing 400 samples and 100 features. We will also define 20 of these features as informative, and add some gaussian noise to the data to make the task harder for the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "# Create fake dataset\n",
    "X, y = make_regression(\n",
    "    n_samples=400, n_features=100, n_informative=20, noise=10, random_state=0\n",
    ")\n",
    "\n",
    "# Print shape of feature matrix and labels\n",
    "print(f\"Shape of dataset: {np.shape(X)}\")\n",
    "print(f\"Shape of labels: {np.shape(y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since it's a regression problem, let's make sure the target of our model is a continuous variable. Let's print the first ten values of `y`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a linear regression estimator using `LinearRegression` (read documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html?highlight=linear%20regression#sklearn.linear_model.LinearRegression))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Create model\n",
    "reg = LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimator objects contain certain parameters that define how they will behave when learning the data, as well as their outputs. These are called __estimator parameters__. Let's inspect the ones of `reg`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print estimator parameters\n",
    "vars(reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These parameters can be changed by modifying their corresponding attributes when calling the estimator, or afterwards using `set_params()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set new model parameters\n",
    "reg.set_params(**{\"normalize\": True})\n",
    "vars(reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "\n",
    "Once the estimator object has been created, it can now learn the value of its parameters from the data. For this we need to call the `fit()` function, and pass our feature matrix (`X`) and true values (`y`) as input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit linear regression model\n",
    "reg = reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the attributes of `reg` again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the names of the attributes\n",
    "vars(reg).keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`reg` now contains new attributes. These are refered to as __estimated parameters__, because they have been learned from the data. In _scikit-learn_, these are indexed by an underscore (`_`) at the end. \n",
    "\n",
    "For example, we can now access the coefficients learned by our linear model. We should have as many coefficients as features in our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of coefficients: {reg.coef_.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also print the values of some of them, and the value of the intercept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define coefficients and intercept\n",
    "coefs = reg.coef_\n",
    "intercept = reg.intercept_\n",
    "\n",
    "# Print\n",
    "print(f\"Model coefficients (first 10):\\n {coefs[:10]} \\n\")\n",
    "print(f\"Model intercept: \\n {intercept}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be careful if your intention is to interpret the coefficients of the model. This process is far from straightforward. Read this very useful [example](https://scikit-learn.org/stable/auto_examples/inspection/plot_linear_model_coefficient_interpretation.html) to learn more about this issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making predictions with the model\n",
    "\n",
    "Now that our model is fitted, we can use it to make predictions. In _scikit-learn_, this is achieved by calling the function `predict()`. \n",
    "\n",
    "Let's predict the values of `X` using our fitted model, and visually compare them to their real values on the first ten samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Predict labels with trained model\n",
    "y_pred = reg.predict(X)\n",
    "\n",
    "# Create dataframe for printing the predictions\n",
    "df = pd.DataFrame({\"y_pred\": y_pred[:10], \"y_real\": y[:10]})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring the model\n",
    "\n",
    "We can use the predicted values to evaluate the performance of the model by quantifyng the difference between these and the real values.\n",
    "\n",
    "In _scikit-learn_ we can evaluate the performance of the estimator using the function `score()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score the model using r2\n",
    "score = reg.score(X, y)\n",
    "\n",
    "# Print score\n",
    "print(f\"Linear model R2: {np.round(score,3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, linear models are evaluated by calculating $R^2$, also called __coefficient of determination__. $R^2$ quantifies how much of the total variance of the outcome variable (`y`) is explained by the fitted model. The best possible value is 1. The higher the value, the best job the model does at explaining the data. You can read more about the implementation of $R^2$ in _scikit-learn_ [here](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html#sklearn.metrics.r2_score)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ✍️ Exercise\n",
    "\n",
    "There are other scoring metrics for regression problems. Check the module [sklearn.metrics](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics) for an overview of the alternatives. Pick one, and implement it in the cell below. Press the three dots to reveal the solution.\n",
    "\n",
    "_Hint!_ If you want to implement a scoring function that is not the default one, you won't be able to do so using the `score()` method. You will need to use a function specifically designed for the scoring metric, and pass the real and predicted values as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#### Answer using mean squared error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Compute mean squared error\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "\n",
    "# Print score\n",
    "print(f\"Mean squared error: {mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression\n",
    "Logistic regression is a very popular classification model. It uses a [logistic function](https://en.wikipedia.org/wiki/Logistic_function) to estimate the probability that an observation belongs to different classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement a logistic regression is _scikit-learn_.\n",
    "\n",
    "We will create a fake dataset ready for classification using the `make_classification()` method (read the documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html#sklearn.datasets.make_classification)): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Create fake dataset\n",
    "X, y = make_classification(\n",
    "    n_samples=400, n_features=100, n_informative=20, random_state=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our `y` should now be a categorical variable. Let's print 10 samples to make sure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classifiers are also estimators in _scikit-learn_. This means we can also use them with the functions illustrated for the linear regression case.\n",
    "\n",
    "Let's create a `LogisticRegression` estimator (read the documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression)), and fit it to our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Create model\n",
    "clf = LogisticRegression()\n",
    "\n",
    "# Fit model\n",
    "clf = clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ✍️ Exercise\n",
    "\n",
    "Can you compare the first 10 predictions of the fitted model to their real values?  Write your answer in the cell below, and press the three dots to reveal the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#### Answer\n",
    "# Predict labels with trained model\n",
    "y_pred = clf.predict(X[:10])\n",
    "y_real = y[:10]\n",
    "\n",
    "# Create dataframe for printing the predictions\n",
    "df = pd.DataFrame({\"y_pred\": y_pred, \"y_real\": y_real})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probabilistic predictions\n",
    "\n",
    "Logistic Regression is a [probabilistic classifier](https://en.wikipedia.org/wiki/Probabilistic_classification), meaning it predicts a probability distribution over the classes.\n",
    "\n",
    "In _scikit-learn_ we can inspect the probabilities assigned to each class using `predict_proba()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the probability of each class\n",
    "y_pred_proba = clf.predict_proba(X[:10])\n",
    "\n",
    "# Create dataframe for printing the predictions for each class\n",
    "df = pd.DataFrame(y_pred_proba, columns=[\"class 0\", \"class 1\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the predictions made by `LogisticRegression` when calling `score()` are evaluated by computing the __mean accuracy__ of the predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score predictions\n",
    "score = clf.score(X, y)\n",
    "print(f\"Mean accuracy: {np.round(score, 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means\n",
    "\n",
    "Unsupervised models are also estimators in _scikit-learn_, since they also learn from data. One type of unsupervised models are __clustering algorithms__. These learn to group the data from their feature values so that observations within a group are more similar than those between groups. You can read more about clustering [here](https://github.com/martinagvilas/intro_stat_learning/blob/master/notebooks/lab2_clustering.ipynb).\n",
    "\n",
    "A very popular clustering algorithm is __k-means__. This method partitions the data into __$k$ pre-specified__ clusters in a way that minimizes the within-cluster variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement k-means using _scikit-learn_. We first need to generate a dataset suitable for clustering using `make_blobs()` (read documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html#sklearn.datasets.make_blobs)) which generates Gaussian shaped blobs. We will create a very simple dataset with only two features, to simplify visualization of the clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# Create fake dataset\n",
    "X, y = make_blobs(\n",
    "    n_samples=400, n_features=2, random_state=0, cluster_std=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize our dataset with a scatterplot, and color the observations according to their real labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot dataset\n",
    "sns.scatterplot(\n",
    "    x=X[:, 0], y=X[:, 1], hue=y,\n",
    "    marker='o', s=25, edgecolor='k', legend=True\n",
    ").set_title(\"Data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 3 clusters in our fake dataset. Usually we don't have this information available and we need to select an arbitrary number of clusters for the algorithm to find.\n",
    "\n",
    "Let's see an example of this and perform k-means clustering by calling `KMeans` (read documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans)) with 5 clusters ($k=5$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Create model\n",
    "kmeans = KMeans(n_clusters=5)\n",
    "\n",
    "# Fit model\n",
    "kmeans = kmeans.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is an unsupervised method, we don't need to provide `y` as input to `fit()`. We also cannot compute the accuracy of the fitted model. But we can compute the average distance of the labeled example to the center of their assigned cluster using the `score()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute average distance\n",
    "score = kmeans.score(X, y)\n",
    "print(f\"Average distance: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to read more about the meaning behind the returned value, read [this answer](https://stackoverflow.com/questions/32370543/understanding-score-returned-by-scikit-learn-kmeans) on stackoverflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More importantly, we can now use our fitted model to predict to which cluster the observations belong to. Let's predict the assignment of the first 10 observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict cluster label\n",
    "y_pred = kmeans.predict(X)\n",
    "print(f\"Predicted labels (first 10): {y_pred[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use a scatterplot to inspect the predicted labels from the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predicted labels\n",
    "sns.scatterplot(\n",
    "    x=X[:, 0], y=X[:, 1], hue=y_pred,\n",
    "    marker='o', s=25, edgecolor='k', legend=False\n",
    ").set_title(\"Data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the same number of predicted labels as the number of $k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✍️ Exercise \n",
    "\n",
    "Can you create a `KMeans` model specifying the correct number of clusters (`k=3`) and plot its predictions? Compare it with the plot of the real labels. Write your code in the cell below and press the three dots to see the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#### Answer\n",
    "# Create model\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "\n",
    "# Fit model\n",
    "kmeans = kmeans.fit(X, y)\n",
    "\n",
    "# Predict labels\n",
    "y_pred = kmeans.predict(X)\n",
    "\n",
    "# Plot predicted labels\n",
    "sns.scatterplot(\n",
    "    x=X[:, 0], y=X[:, 1], hue=y_pred,\n",
    "    marker='o', s=25, edgecolor='k', legend=False\n",
    ").set_title(\"Data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "# ✏️ Check your knowledge\n",
    "\n",
    "Load the ABIDE 2 dataset and:\n",
    "\n",
    "1. Use logistic regression to predict `group` from the features encoding brain data. How accurate is the model? Play around with different accuracy metrics and inspect their results.\n",
    "2. Select two features encoding brain data, run a clustering analysis on them, and plot the predicted labels as shown in this notebook. Compare it with a similar plot displaying the true group labels.\n",
    "\n",
    "If you forgot how to load your data and define `X` and `y`, go back to [Notebook 1](./01-preliminaries.ipynb) and refresh this knowledge.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Additional reading\n",
    "\n",
    "- [Choosing the right estimator](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html): A useful map to decide which estimator is best given your dataset and learning goal.\n",
    "- [Core concepts of machine learning](https://github.com/neurohackademy/nh2020-curriculum/blob/master/tu-machine-learning-yarkoni/02-core-concepts.ipynb)  by _Tal Yarkoni_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
