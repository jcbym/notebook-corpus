{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from absl import flags\n",
    "from pathlib import Path\n",
    "from operator import itemgetter\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.metrics import classification_report, recall_score, precision_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "import model as nn\n",
    "importlib.reload(nn)\n",
    "\n",
    "elsa_architecture = nn.elsa_architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
    "\n",
    "lang = \"ar\"\n",
    "batch_size = 250\n",
    "lr = 1e-3\n",
    "epochs = 100\n",
    "patience = 3\n",
    "data_dir = \"/data/elsa2\"\n",
    "checkpoint_dir = \"./ckpt\"\n",
    "optimizer = \"adam\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_hidden = 512\n",
    "lstm_drop = 0.5\n",
    "final_drop = 0.5\n",
    "embed_drop = 0.0\n",
    "highway = False\n",
    "compute_class_weight = False\n",
    "multilabel = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 20, 200)      53440800    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 20, 200)      0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bi_lstm_0 (Bidirectional)       (None, 20, 1024)     2920448     activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bi_lstm_1 (Bidirectional)       (None, 20, 1024)     6295552     bi_lstm_0[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 20, 2248)     0           bi_lstm_1[0][0]                  \n",
      "                                                                 bi_lstm_0[0][0]                  \n",
      "                                                                 activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "attlayer (AttentionWeightedAver (None, 2248)         2248        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 2248)         0           attlayer[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "softmax (Dense)                 (None, 64)           143936      dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 62,802,984\n",
      "Trainable params: 62,802,984\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Train on 510183 samples, validate on 145766 samples\n",
      "Epoch 1/100\n",
      "510183/510183 [==============================] - 464s 910us/step - loss: 2.5592 - acc: 0.3794 - val_loss: 2.4156 - val_acc: 0.4065\n",
      "Epoch 2/100\n",
      "510183/510183 [==============================] - 439s 861us/step - loss: 2.2628 - acc: 0.4351 - val_loss: 2.4443 - val_acc: 0.4099\n",
      "Epoch 3/100\n",
      "510183/510183 [==============================] - 441s 865us/step - loss: 1.9185 - acc: 0.5161 - val_loss: 2.7007 - val_acc: 0.3870\n",
      "Epoch 4/100\n",
      "510183/510183 [==============================] - 440s 862us/step - loss: 1.6465 - acc: 0.5755 - val_loss: 3.0065 - val_acc: 0.3833\n",
      "0.3847758089643587\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           ðŸ˜‚       0.39      0.73      0.50     13164\n",
      "           â¤       0.27      0.46      0.34      6679\n",
      "           â™»       0.99      1.00      1.00      7248\n",
      "           ðŸ˜­       0.43      0.36      0.39      4105\n",
      "           ðŸ’”       0.26      0.26      0.26      4358\n",
      "           â™¥       0.23      0.16      0.19      2846\n",
      "           ðŸ’™       0.24      0.15      0.18      2428\n",
      "           ðŸ’›       0.22      0.15      0.18      1791\n",
      "           ðŸŒ¹       0.27      0.40      0.32      1894\n",
      "           ðŸ˜       0.19      0.15      0.17      1667\n",
      "           ðŸ¤£       0.24      0.11      0.15      1099\n",
      "           ðŸ™‚       0.11      0.05      0.07      1231\n",
      "           ðŸ˜”       0.12      0.06      0.08      1152\n",
      "           ðŸ’•       0.24      0.07      0.11      1006\n",
      "           â™€       0.53      0.53      0.53      1144\n",
      "           ðŸ–¤       0.15      0.10      0.12       998\n",
      "           ðŸ’œ       0.30      0.11      0.16       789\n",
      "           ðŸ˜…       0.06      0.01      0.02       859\n",
      "           ðŸŒš       0.05      0.01      0.02       803\n",
      "           âœ¨       0.22      0.15      0.18       619\n",
      "           ðŸ˜       0.03      0.01      0.02       744\n",
      "           ðŸ¤”       0.14      0.07      0.09       741\n",
      "           ðŸ”¥       0.44      0.34      0.38       619\n",
      "           ðŸŒ¸       0.16      0.11      0.13       615\n",
      "           â™‚       0.44      0.24      0.31       689\n",
      "           ðŸ‘Œ       0.16      0.10      0.12       585\n",
      "           ðŸ™„       0.05      0.01      0.02       605\n",
      "           ðŸ‘       0.20      0.16      0.18       564\n",
      "           ðŸ™       0.18      0.14      0.16       533\n",
      "           ðŸ’—       0.34      0.09      0.14       471\n",
      "           ðŸ’–       0.39      0.15      0.21       421\n",
      "           âœ…       0.50      0.40      0.44       274\n",
      "           ðŸ˜Š       0.04      0.02      0.02       526\n",
      "           ðŸ¥º       0.14      0.03      0.05       499\n",
      "           ðŸ˜‰       0.15      0.05      0.08       482\n",
      "           ðŸ’˜       0.22      0.07      0.10       399\n",
      "           ðŸ’š       0.18      0.11      0.14       401\n",
      "           ðŸ˜¢       0.08      0.02      0.03       443\n",
      "           ðŸŒ·       0.25      0.06      0.10       387\n",
      "           ðŸ˜Œ       0.05      0.02      0.03       404\n",
      "           ðŸ˜’       0.05      0.01      0.02       411\n",
      "           ðŸ˜Ž       0.07      0.03      0.05       386\n",
      "           ðŸ’ž       0.27      0.04      0.08       321\n",
      "           âœ”       0.55      0.30      0.39       248\n",
      "           ðŸŒ¿       0.34      0.28      0.30       305\n",
      "           ðŸ˜˜       0.13      0.04      0.06       329\n",
      "           â™        1.00      0.99      0.99       380\n",
      "           â™£       1.00      0.33      0.50         3\n",
      "           ðŸŒº       0.29      0.07      0.12       307\n",
      "           ðŸ’       0.24      0.06      0.09       292\n",
      "           ðŸ‘‡       0.43      0.40      0.42       239\n",
      "           âŒ       0.58      0.31      0.41       128\n",
      "           âœ‹       0.14      0.05      0.07       302\n",
      "           ðŸ™ˆ       0.11      0.03      0.05       295\n",
      "           ðŸ˜       0.05      0.02      0.02       311\n",
      "           â˜º       0.12      0.02      0.03       294\n",
      "           â˜¹       0.08      0.01      0.02       286\n",
      "           ðŸ‘       0.27      0.29      0.28       237\n",
      "           ðŸ’“       0.21      0.05      0.09       237\n",
      "           ðŸŽ¶       0.14      0.09      0.11       281\n",
      "           ðŸ§¡       0.23      0.05      0.08       239\n",
      "           ðŸ˜ž       0.05      0.01      0.01       270\n",
      "           ðŸŒ™       0.63      0.77      0.69       248\n",
      "           ðŸ¤²       0.21      0.09      0.13       253\n",
      "\n",
      "    accuracy                           0.38     72884\n",
      "   macro avg       0.26      0.18      0.20     72884\n",
      "weighted avg       0.35      0.38      0.35     72884\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_dir = Path(data_dir)\n",
    "wv_path = (data_dir / \"{:s}_wv.npy\".format(lang)).__str__()\n",
    "X_path = (data_dir / \"{:s}_X.npy\".format(lang)).__str__()\n",
    "y_path = (data_dir / \"{:s}_y.npy\".format(lang)).__str__()\n",
    "emoji_path = (data_dir / \"{:s}_emoji.txt\".format(lang)).__str__()\n",
    "\n",
    "wv = np.load(wv_path, allow_pickle=True)\n",
    "input_vec = np.load(X_path, allow_pickle=True)\n",
    "input_label = np.load(y_path, allow_pickle=True)\n",
    "\n",
    "nb_tokens = len(wv)\n",
    "embed_dim = wv.shape[1]\n",
    "input_len = len(input_label)\n",
    "nb_classes = input_label.shape[1]\n",
    "maxlen = input_vec.shape[1]\n",
    "\n",
    "train_end = int(input_len*0.7)\n",
    "val_end = int(input_len*0.9)\n",
    "\n",
    "(X_train, y_train) = (input_vec[:train_end], input_label[:train_end])\n",
    "(X_val, y_val) = (input_vec[train_end:val_end], input_label[train_end:val_end])\n",
    "(X_test, y_test) = (input_vec[val_end:], input_label[val_end:])\n",
    "\n",
    "if multilabel:\n",
    "    def to_multilabel(y):\n",
    "        outputs = []\n",
    "        for i in range(nb_classes):\n",
    "            outputs.append(y[:, i])\n",
    "        return outputs\n",
    "\n",
    "    y_train = to_multilabel(y_train)\n",
    "    y_val = to_multilabel(y_val)\n",
    "    y_test = to_multilabel(y_test)\n",
    "\n",
    "model = elsa_architecture(nb_classes=nb_classes,\n",
    "                          nb_tokens=nb_tokens,\n",
    "                          maxlen=maxlen,\n",
    "                          final_dropout_rate=final_drop,\n",
    "                          embed_dropout_rate=embed_drop,\n",
    "                          load_embedding=True,\n",
    "                          pre_embedding=wv,\n",
    "                          high=highway,\n",
    "                          embed_dim=embed_dim,\n",
    "                          multilabel=multilabel)\n",
    "model.summary()\n",
    "\n",
    "computed_class_weight = None\n",
    "\n",
    "if multilabel:\n",
    "    loss = \"binary_crossentropy\"\n",
    "else:\n",
    "    loss = \"categorical_crossentropy\"\n",
    "    if compute_class_weight:\n",
    "        y_train_sps = []\n",
    "        for row in y_train:\n",
    "            y_train_sps.extend(np.where(row)[0].tolist())\n",
    "        computed_class_weight = class_weight.compute_class_weight(\n",
    "            'balanced', list(range(nb_classes)), y_train_sps)\n",
    "        print(\"computed class weight = {:s}\".format(str(computed_class_weight)))\n",
    "\n",
    "if optimizer == 'adam':\n",
    "    adam = Adam(clipnorm=1, lr=lr)\n",
    "    model.compile(loss=loss, optimizer=adam, metrics=['accuracy'])\n",
    "elif optimizer == 'rmsprop':\n",
    "    model.compile(loss=loss, optimizer='rmsprop', metrics=['accuracy'])\n",
    "\n",
    "checkpoint_dir = Path(checkpoint_dir)\n",
    "if not checkpoint_dir.exists():\n",
    "    checkpoint_dir.mkdir()\n",
    "checkpoint_weight_path = (checkpoint_dir / \"elsa_{:s}.hdf5\".format(lang)).__str__()\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', min_delta=0, patience=patience, verbose=0, mode='auto'),\n",
    "    keras.callbacks.ModelCheckpoint(checkpoint_weight_path, monitor='val_loss',\n",
    "                                    verbose=0, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "]\n",
    "model.fit(X_train,\n",
    "          y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(X_val, y_val),\n",
    "          class_weight=computed_class_weight,\n",
    "          callbacks=callbacks,\n",
    "          verbose=1)\n",
    "\n",
    "freq = {line.split()[0]: int(line.split()[1]) for line in open(emoji_path).readlines()}\n",
    "freq_topn = sorted(freq.items(), key=itemgetter(1), reverse=True)[:nb_classes]\n",
    "\n",
    "if multilabel:\n",
    "    y_pred = model.predict([X_test], batch_size=batch_size)\n",
    "    y_pred = [np.squeeze(p) for p in y_pred]\n",
    "\n",
    "    y_test_1d = np.array(y_test).flatten()\n",
    "    y_pred_1d = np.array(y_pred).flatten()\n",
    "    print(f1_score(y_test_1d, y_pred_1d > 0.5))\n",
    "    print(classification_report(y_test_1d, y_pred_1d > 0.5))\n",
    "\n",
    "    gold, pred = [], []\n",
    "    for i in range(len(X_test)):\n",
    "        each_gold, each_pred = [], []\n",
    "        for c in range(nb_classes):\n",
    "            if y_test[c][i] == 1.0:\n",
    "                each_gold.append(c+1)\n",
    "            else:\n",
    "                each_gold.append(0)\n",
    "            if y_pred[c][i] > 0.5:\n",
    "                each_pred.append(c+1)\n",
    "            else:\n",
    "                each_pred.append(0)\n",
    "        gold.extend(each_gold)\n",
    "        pred.extend(each_pred)\n",
    "\n",
    "    target_name = [\"\"] + [e[0] for e in freq_topn]\n",
    "    print(classification_report(gold, pred, target_names=target_name))\n",
    "else:\n",
    "    _, acc = model.evaluate(X_test, y_test, batch_size=batch_size, verbose=0)\n",
    "    print(acc)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(classification_report(y_test.argmax(axis=1), y_pred.argmax(\n",
    "        axis=1), target_names=[e[0] for e in freq_topn]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
