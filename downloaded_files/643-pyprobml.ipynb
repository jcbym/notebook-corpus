{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2021 Google LLC\n",
    "# Use of this source code is governed by an MIT-style\n",
    "# license that can be found in the LICENSE file or at\n",
    "# https://opensource.org/licenses/MIT.\n",
    "\n",
    "# Author(s): Kevin P. Murphy (murphyk@gmail.com) and Mahmoud Soliman (mjs@aucegypt.edu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://opensource.org/licenses/MIT\" target=\"_parent\"><img src=\"https://img.shields.io/github/license/probml/pyprobml\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/probml/pyprobml/blob/master/notebooks/figures//chapter14_figures.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cloning the pyprobml repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/probml/pyprobml \n",
    "%cd pyprobml/scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%load_ext autoreload \n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "\n",
    "DISCLAIMER = 'WARNING : Editing in VM - changes lost after reboot!!'\n",
    "from google.colab import files\n",
    "\n",
    "def interactive_script(script, i=True):\n",
    "  if i:\n",
    "    s = open(script).read()\n",
    "    if not s.split('\\n', 1)[0]==\"## \"+DISCLAIMER:\n",
    "      open(script, 'w').write(\n",
    "          f'## {DISCLAIMER}\\n' + '#' * (len(DISCLAIMER) + 3) + '\\n\\n' + s)\n",
    "    files.view(script)\n",
    "    %run $script\n",
    "  else:\n",
    "      %run $script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 14.1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  We can classify a digit by looking for certain discriminative features (image templates) occuring in the correct (relative) locations. From Figure 5.1 of <a href='#keras'>[Cho]</a> . Used with kind permission of Francois Chollet. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 14.2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Discrete convolution of $\\mathbf  x =[1,2,3,4]$ with $\\mathbf  w =[5,6,7]$ to yield $\\mathbf  z =[5,16,34,52,45,28]$. We see that this operation consists of ``flipping'' $\\mathbf  w $ and then ``dragging'' it over $\\mathbf  x $, multiplying elementwise, and adding up the results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 14.3:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  1d cross correlation. Adapted from Figure 13.10.1 of <a href='#dive'>[Zha+19]</a> . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 14.4:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Illustration of 2d cross correlation. Adapted from Figure 6.2.1 of <a href='#dive'>[Zha+19]</a> . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 14.5:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Convolving a 2d image (left) with a $3 \\times 3$ filter (middle) produces a 2d response map (right). The bright spots of the response map correspond to locations in the image which contain diagonal lines sloping down and to the right. From Figure 5.3 of <a href='#keras'>[Cho]</a> . Used with kind permission of Francois Chollet. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 14.6:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Illustration of padding and strides in 2d convolution. (a) We apply ``same convolution'' to a $5 \\times 7$ input (with zero padding) using a $3 \\times 3$ filter to create a $5 \\times 7$ output. (b) Now we use a stride of 2, so the output has size $3 \\times 4$. Adapted from Figures 14.3--14.4 of <a href='#Geron2019'>[Aur19]</a> . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 14.7:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Illustration of 2d convolution applied to an input with 2 channels. Adapted from Figure 6.4.1 of <a href='#dive'>[Zha+19]</a> . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 14.8:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Illustration of a CNN with 2 convolutional layers. The input has 3 color channels. The feature maps at internal layers have multiple channels. The cylinders correspond to hypercolumns, which are feature vectors at a certain location. Adapted from Figure 14.6 of <a href='#Geron2019'>[Aur19]</a> . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 14.9:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Illustration of $1 \\times 1 \\times 3 \\times 2$ convolution. Adapted from Figure 6.4.2 of <a href='#dive'>[Zha+19]</a> . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 14.10:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Illustration of maxpooling with a 2x2 filter and a stride of 1. Adapted from Figure 6.5.1 of <a href='#dive'>[Zha+19]</a> . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 14.11:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Illustration of different activation normalization methods for a CNN. Each subplot shows a feature map tensor, with N as the batch axis, C as the channel axis, and (H, W) as the spatial axes. The pixels in blue are normalized by the same mean and variance, computed by aggregating the values of these pixels. Left to right: batch norm, layer norm, instance norm, and group norm (with 2 groups of 3 channels). From Figure 2 of <a href='#Wu2018GN'>[YK18]</a> . Used with kind permission of Kaiming He. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 14.12:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  A simple CNN for classifying MNIST images. The model has 2 convolutional layers, and 2 fully connected layers, with a softmax output. The square boxes denote the receptive fields. The two convolutional filters have size $5 \\times 5 \\times 1 \\times n_1$ and $5 \\times 5 \\times n_1 \\times n_2$. The two fully connected layers have weight matrices of size $(16 n_2) \\times n_3$ and $n_3 \\times 10$. The use of a $5 \\times 5$ filter with ``valid'' convolution means we reduce the input size from 28 pixels per side to $28-5+1=24$ (see \\cref  sec:convValid  for details). The use of $2 \\times 2$ max-pooling with stride 2 reduces the 24 pixels to 12. The second convolution reduces this to $12-5+1=8$ pixels per side, and the second pooling layer reduces this to 4. As we reduce the spatial size of each layer, we typically increase the number of feature channels (so $n_2 \\approx 2 n_1$). See \\cref  sec:CNN  for further details. From   https://bit.ly/2YB9o0H . Used with kind permission of Sumit Saha. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 14.13:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  (a) Visualization of the  \\bf FashionMNIST  dataset <a href='#fashion'>[XRV17]</a> . Each image is $28 \\times 28 \\times 1$, where the final dimension of size 1 refers to gray scale. There are 60k training examples and 10k test examples. There are 10 classes: 'T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'. We show the first 25 images from the training set.  \n",
    "Figure(s) generated by [fashion_viz_tf.py](https://github.com/probml/pyprobml/blob/master/scripts/fashion_viz_tf.py) [cifar_viz_tf.py](https://github.com/probml/pyprobml/blob/master/scripts/cifar_viz_tf.py) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_script(\"fashion_viz_tf.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_script(\"cifar_viz_tf.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 14.14:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  (a) Sample images from the  \\bf ImageNet  dataset <a href='#ILSVRC15'>[Rus+15]</a> . This subset consists of 1.3M color training images, each of which is 256x256 pixels in size. There are 1000 possible labels, one per image, and the task is to minimize the top 5 error rate, i.e., to ensure the correct label is within the 5 most probable predictions. Below each image we show the true label, and a distribution over the top 5 predicted labels. If the true label is in the top 5, its probability bar is colored red. Predictions are generated by a convolutional neural network (CNN) called ``AlexNet'' (\\cref  sec:alexNet ). From Figure 4 of <a href='#Krizhevsky12'>[KSH12]</a> . Used with kind permission of Alex Krizhevsky. (b) Misclassification rate (top 5) on the ImageNet competition over time. Used with kind permission of Andrej Karpathy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 14.15:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Illustration of common failure modes of CNN classifiers on some images from ImageNet. The top row (blue) shows the ground truth label; the next 5 rows are the top 5 predictions from GoogLeNet (red=wrong, green=right). From left to right: Images that contain multiple objects, images of extreme closeups and uncharacteristic views, images with filters, images that significantly benefit from the ability to read text, images that contain very small and thin objects, images with abstract representations, and example of a fine-grained image that GoogLeNet correctly identifies but a human would have significant difficulty with. From   https://bit.ly/3cFbALk . Used with kind permission of Andrej Karpathy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 14.16:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Some sample images from the ``celebrities with attributes'' dataset. From   http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html . Used with kind permission of Ziwei Liu. See   https://github.com/probml/pyprobml/blob/master/book1/cnn/celeba\\_viz.ipynb  cnn/celeba\\_viz.ipynb  for a smaller version of this data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 14.17:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  (a) LeNet5. (b) AlexNet. Adapted from Figures 6.6.2 and 7.1.2 of <a href='#dive'>[Zha+19]</a> . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 14.18:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Results of applying a CNN to some MNIST images (cherry picked to include some errors). Red is incorrect, blue is correct. (a) After 1 epoch of training. (b) After 2 epochs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 14.19:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Inception module. The $1 \\times 1$ convolutional layers reduce the number of channels, keeping the spatial dimensions the same. The parallel pathways through convolutions of different sizes allows the model to learn which filter size to use for each layer. The final depth concatenation block combines the outputs of all the different pathways (which all have the same spatial size). Adapted from Figure 2b of <a href='#InceptionV1'>[Chr+15]</a> . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 14.20:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  GoogLeNet (slightly simplified from the original). Input is on the left. Adapted from Figure 3 of <a href='#InceptionV1'>[Chr+15]</a> . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 14.21:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  The ResNet architecture. The notation $64, 3 \\times 3 + 1(S)$ means a convolutional layer with 64 channels, using $3 \\times 3$ kernels with stride 1 and same convolution. Every time the resolution is halved (by using a stride of size 2), the number of channels is doubled. The black curved edges are skip connections, that bypass the nonlinear pathway. The dotted red curved edges are skip connections combined with $1 \\times 1$ convolution so that the number of input channels matches the output of the nonlinear pathway. Adapted from Figure 14.17 of <a href='#Geron2019'>[Aur19]</a> . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 14.22:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  (a) Illustration of face detection, a special case of object detection. (Photo of author and his wife Margaret, taken at Filoli in California in Feburary, 2018. Image processed by Jonathan Huang using SSD face model.) (b) Illustration of anchor boxes. Adapted from \\citep [Sec 12.5] dive . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 14.23:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  (a) Illustration of keypoint detection for body, hands and face using the OpenPose system. From Figure 8 of <a href='#openPose'>[Zhe+18]</a> . Used with kind permission of Yaser Sheikh. (b) Illustration of object detection and instance segmentation using Mask R-CNN. From   https://github.com/matterport/Mask_RCNN . Used with kind permission of Waleed Abdulla. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 14.24:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Illustration of an  \\bf encoder-decoder  (aka  \\bf U-net ) CNN for semantic segmentation The encoder uses convolution (which downsamples), and the decoder uses transposed convolution (which upsamples). From Figure 1 of <a href='#segnet'>[VAR17]</a> . Used with kind permission of Alex Kendall. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 14.25:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Illustration of a multi-task dense prediction problem. From Based on Figure 1 of <a href='#Eigen2015'>[DR15]</a> . Used with kind permission of Rob Fergus. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 14.26:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Dilated convolution with a 3x3 filter using rate 1, 2 and 3. From Figure 1 of <a href='#Cui2019cnn'>[Xim+19]</a> . Used with kind permission of Ximin Cui. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 14.27:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Transposed convolution with a filter of size $3 \\times 3$ and stride of 2 to map a $2 \\times 3$ input to a $5 \\times 7$ output. Adapted from Figure 14.27 of <a href='#Geron2019'>[Aur19]</a> . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 14.28:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Images that maximize the probability of ImageNet classes ``goose'' and ``ostrich'' under a simple Gaussian prior. From   http://yosinski.com/deepvis . Used with kind permission of Jeff Clune. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 14.29:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Illustration of total variation norm. (a) Input image: a green sea turtle (Used with kind permission of Wikimedia author P. Lindgren). (b) Horizontal deltas. (c) Vertical deltas. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 14.30:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Images that maximize the probability of certain ImageNet classes under a TV prior. From   https://bit.ly/2ICkueV . Used with kind permission of Alexander Mordvintsev. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 14.31:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  We visualize ``optimal stimuli'' for neurons in layers Conv 1, 3, 5 and fc8 in the AlexNet architecture, trained on the ImageNet dataset. For Conv5, we also show retrieved real images (under the column ``data driven'') that produce similar activations. Based on the method in <a href='#Mahendran16ijcv'>[MV16]</a> , as implemented in the code at   http://vision03.csail.mit.edu/cnn_art/ . Used with kind permission of Donglai Wei. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 14.32:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Illustration of DeepDream. The CNN is an Inception classifier trained on ImageNet. (a) Starting image of an Aurelia aurita (also called moon jelly). (b) Image generated after 10 iterations. (c) Image generated after 50 iterations. From   https://en.wikipedia.org/wiki/DeepDream . Used with kind permission of Wikipedia author Martin Thoma. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 14.33:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Example output from a neural style transfer system (a) Content image: a green sea turtle (Used with kind permission of Wikimedia author P. Lindgren). (b) Style image: a painting by Wassily Kandinsky called ``Composition 7''. (c) Output of neural style generation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 14.34:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Illustration of how neural style transfer works. Adapted from Figure 12.12.2 of <a href='#dive'>[Zha+19]</a> . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 14.35:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Schematic representation of 3 kinds of feature maps for 3 different input images. Adapted from Figure 5.16 of <a href='#Foster2019'>[Dav19]</a> . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 14.36:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Example of an adversarial attack on an image classifier. Left column: original image which is correctly classified. Middle column: small amount of structured noise which is added to the input (magnitude of noise is magnified by $10 \\times $). Right column: new image, which is confidently misclassified as a ``gibbon'', even though it looks just like the original ``panda'' image. Here $\\epsilon =0.007$ From Figure 1 of <a href='#Goodfellow2015adversarial'>[IJC15]</a> . Used with kind permission of Ian Goodfellow. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 14.37:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Images that look like random noise but which cause the CNN to confidently predict a specific class. From Figure 1 of <a href='#Nguyen2015'>[AJJ15]</a> . Used with kind permission of Jeff Clune. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 14.38:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Synthetic images that cause the CNN to confidently predict a specific class. From Figure 1 of <a href='#Nguyen2015'>[AJJ15]</a> . Used with kind permission of Jeff Clune. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 14.39:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  An adversarially modified image to evade spam detectors. The image is constructed from scratch, and does not involve applying a small perturbation to any given image. This is an illustrative example of how large the space of possible adversarial inputs $\\Delta $ can be when the attacker has full control over the input. From <a href='#biggio2011survey'>[Bat+11]</a> . Used with kind permission of Battista Biggio. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 14.40:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Illustration of robust vs non-robust features for separating two classes, represented by Gaussian blobs. The green line is the decision boundary induced by empirical risk minimization (ERM). The red line is the decision boundary induced by robust training --- it ignores the vertical feature $x_2$, which is less robust than the horizontal feature $x_1$. From Figure 14 of <a href='#Ilyas2019'>[And+19]</a> . Used with kind permission of Andrew Ilyas. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 14.41:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Effect of Gaussian noise of increasing magnitude on an image classifier. (The model is a ResNet-50 CNN (\\cref  sec:resnet ) trained on ImageNet with added Gaussian noise of magnitude $\\sigma =0.6$.) From Figure 23 of <a href='#Ford2019'>[Nic+19]</a> . Used with kind permission of Justin Gilmer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 14.42:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  (a) When the input dimension $n$ is large and the decision boundary is locally linear, even a small error rate in random noise will imply the existence of small adversarial perturbations. Here, $d(\\mathbf  x _0, E)$ denotes the distance from a clean input $\\mathbf  x _0$ to an adversarial example (A) while the distance from $\\mathbf  x _0$ to a random sample $N(0; \\sigma ^2 I$ (B) will be approximately $\\sigma \\sqrt  n $. As $n \\rightarrow \\infty $ the ratio of $d(x_0, A)$ to $d(\\mathbf  x _0, B)$ goes to 0. (b) A 2d slice of the InceptionV3 decision boundary through three points: a clean image (black), an adversarial example (red), and an error in random noise (blue). The adversarial example and the error in noise lie in the same region of the error set which is misclassified as ``miniature poodle'', which closely resembles a halfspace as in Figure (a). Used with kind permission of Justin Gilmer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References:\n",
    " <a name='Nguyen2015'>[AJJ15]</a> N. Anh, Y. Jason and C. Jeff. \"Deep Neural Networks are Easily Fooled: High ConfidencePredictions for Unrecognizable Images\". (2015). \n",
    "\n",
    "<a name='Ilyas2019'>[And+19]</a> I. Andrew, S. Shibani, T. D. Logan, T. Brandon and M. Aleksander. \"Adversarial Examples Are Not Bugs, They Are Features\". abs/1905.02175 (2019). arXiv: 1905.02175 \n",
    "\n",
    "<a name='Geron2019'>[Aur19]</a> G. Aur'elien \"Hands-On Machine Learning with Scikit-Learn and TensorFlow: Concepts, Tools, and Techniques for BuildingIntelligent Systems (2nd edition)\". (2019). \n",
    "\n",
    "<a name='biggio2011survey'>[Bat+11]</a> B. Battista, F. Giorgio, P. Ignazio and R. Fabio. \"A survey and experimental evaluation of image spam filtering techniques\". In: Pattern recognition letters (2011). \n",
    "\n",
    "<a name='keras'>[Cho]</a> F. Chollet \"Keras\". \n",
    "\n",
    "<a name='InceptionV1'>[Chr+15]</a> S. Christian, L. Wei, J. Yangqing, S. Pierre, R. Scott, A. Dragomir, E. Dumitru, V. Vincent and R. Andrew. \"Going Deeper with Convolutions\". (2015). \n",
    "\n",
    "<a name='Eigen2015'>[DR15]</a> E. David and F. Rob. \"Predicting Depth, Surface Normals and Semantic Labels with aCommon Multi-Scale Convolutional Architecture\". (2015). \n",
    "\n",
    "<a name='Foster2019'>[Dav19]</a> F. David \"Generative Deep Learning: Teaching Machines to Paint, WriteCompose, and Play\". (2019). \n",
    "\n",
    "<a name='Goodfellow2015adversarial'>[IJC15]</a> G. IanJ, S. Jonathon and S. Christian. \"Explaining and Harnessing Adversarial Examples\". (2015). \n",
    "\n",
    "<a name='Krizhevsky12'>[KSH12]</a> A. Krizhevsky, I. Sutskever and G. Hinton. \"Imagenet classification with deep convolutional neural networks\". (2012). \n",
    "\n",
    "<a name='Mahendran16ijcv'>[MV16]</a> M. MahendranA and V. VedaldiA. \"Visualizing Deep Convolutional Neural Networks Using Natural Pre-images\". In: ijcv (2016). \n",
    "\n",
    "<a name='Ford2019'>[Nic+19]</a> F. Nic, G. Justin, C. Nicolas and C. CubukDogus. \"Adversarial Examples Are a Natural Consequence of Test Errorin Noise\". abs/1901.10513 (2019). arXiv: 1901.10513 \n",
    "\n",
    "<a name='ILSVRC15'>[Rus+15]</a> O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. Berg and L. Fei-Fei. \"ImageNet Large Scale Visual Recognition Challenge\". In: ijcv (2015). \n",
    "\n",
    "<a name='segnet'>[VAR17]</a> B. Vijay, K. Alex and C. Roberto. \"SegNet: A Deep Convolutional Encoder-Decoder Architecture forImage Segmentation\". In: pami (2017). \n",
    "\n",
    "<a name='fashion'>[XRV17]</a> H. Xiao, K. Rasul and R. Vollgraf. \"Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms\". abs/1708.07747 (2017). arXiv: 1708.07747 \n",
    "\n",
    "<a name='Cui2019cnn'>[Xim+19]</a> C. Ximin, Z. Ke, G. Lianru, Z. Bing, Y. Dong and R. Jinchang. \"Multiscale Spatial-Spectral Convolutional Network withImage-Based Framework for Hyperspectral Imagery Classification\". In: Remote Sensing (2019). \n",
    "\n",
    "<a name='Wu2018GN'>[YK18]</a> W. Yuxin and H. Kaiming. \"Group Normalization\". (2018). \n",
    "\n",
    "<a name='dive'>[Zha+19]</a> A. Zhang, Z. Lipton, M. Li and A. Smola. \"Dive into deep learning\". (2019). \n",
    "\n",
    "<a name='openPose'>[Zhe+18]</a> C. Zhe, H. Gines, S. Tomas, W. WeiShih-En and S. Yaser. \"OpenPose: Realtime Multi-Person 2D Pose Estimationusing Part Affinity Fields\". abs/1812.08008 (2018). arXiv: 1812.08008 \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
