{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redes Neurais 01\n",
    "\n",
    "Fala galera! Nessa aula, finalmente vamos aprender um dos tópicos mais esperados do curso: redes neurais (ou se quiser usar o buzzword, deep learning!). Serão duas aulas sobre a base do assunto: hoje iremos entender o funcionamento geral de uma rede neural, e na próxima como customizar cada aspecto seu e o porquê de cada um. Após essas duas aulas, vamos abordar problemas antes não tão tangíveis sem redes neurais: NLP (Processamento de Linguagem Natural, aka ensinar computadores a entender e reproduzir linguagem humana) e Visão Computacional (aka ensinar computadores a enxergar o mundo como energamos e tomar decisões com base em padrões visuais). Bora pra aula!\n",
    "\n",
    "## Alguns detalhes antes da aula\n",
    "\n",
    "Estudar Deep Learning tem alguns desdobramentos devido à natureza do assunto, então gostaria de discutir isso antes de seguirmos para a matéria em si. Você pode pular essa seção se já vem estudando o assunto há um tempo, mas creio que isso poupará muita dor de cabeça para os iniciantes. <br>\n",
    "Primeiramente, vamos falar sobre ferramentas. Há diversas bibliotecas para desenvolvimento de redes neurais: TensorFlow, TensorFlow 2.0 (Google), Keras, Theano, Caffe, PyTorch, MXNet etc. Tem até no MatLab (risos risos, ninguém usa essa). Embora elas tenham diferenças entre si, desde sintaxe até performance, basicamente o importante é dominar a biblioteca que você escolher. Entendendo bem os princípios do funcionamento de redes neurais, a transição entre bibliotecas fica muito mais fluida. Alguns papers são esritos en PyTorch, outros em Keras, mas o importante é ter uma noção mínima para saber ler qualquer um. Minha recomendação de escolha é baseada em 2 princípios: performance e comunidade. Para uma bibioteca ser boa, não somente de deep learning, ela precisa cumprir os requisitos do seu projeto, bem como possuir uma comunidade forte e ativa, pois somente assim ela continuará crescendo e se aprimorando. Por causa disso, aqui na Digital House usaremos Tensorflow. Recentemente, o Google disponibilizou a versão Alpha do TensorFlow 2.0, que é mais intuitiva e completa para se utilizar. Caso você queira rodar esse notebook na sua máquina, segue aqui o __[link para instalação geral do TensorFlow 2.0](https://www.tensorflow.org/install/pip)__ e o __[link para instalação do TensorFlow 2.0 Alpha com GPU](https://www.tensorflow.org/install/gpu)__. Falando em GPU, vamos para o próximo tópico: hardware. <br>\n",
    "Deep Learning consome muitos recursos computacionais. Não sei se vocês já perceberam, mas o curso inteiro até agora foi rodado em CPU. Pela natureza do funcionamento de redes neurais, hoje utilizamos GPUs (Graphics Processing Unit), NPUs (Neural Processing Unit) ou até TPUs (Tensor Processing Unit), pois essas são arquiteturas pensadas para cálculos em paralelo de tensores. Mas aí vem uma questão: sua máquina pode não ter uma GPU (no caso do TensorFlow, uma GPU NVidia). E agora? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/nvidia_gpu.jpg\" align=\"right\" width=\"50%\">\n",
    "Uma GPU modelo Titan X da NVidia. Sua arquitetura é diferente de uma CPU (Von Neumann), e é especializada em realizar operações em paralelo. Ela é composta por vários núcleos chamados CUDA Cores, responsáveis pelos cálculos de tensores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/tpu.jpeg\" align=\"right\" width=\"50%\">\n",
    "Uma TPU do Google. Várias empresas estão investindo em arquiteturas de hardware mais potentes e energeticamente mais eficientes do que as GPUs. Ela é uma arquitetura especializada em cálculo de tensores para deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bom, se seu computador não tem uma dessas arquiteturas (geralmente são gaming laptops que as têm), não há problema: você pode usar serviços na nuvem! AWS Cloud, Google Cloud, Microsoft Azure e Kaggle Kernels são exemplos de serviços com GPU para uso. Pessoalmente, recomendo rodar esse notebook ou num __[Google Colab](https://colab.research.google.com/notebooks/welcome.ipynb#recent=true)__ ou num Kaggle Kernel, mas é possível do Kaggle não ter o TF 2.0. <br>\n",
    "Finalmente, gostaria de deixar um mantra aqui: cuidado com redes neurais. É sério. Esse é um assunto que todos querem aprender porque é \"cool\" mas se esquecem de ter o cuidado e domínio necessário. Redes Neurais são black boxes, e o mindset para utilizar esses modelos é um tanto quanto diferente. Quando implementando essa tecnologia num projeto profissional, minha sugestão é já ter um benchmarking executado com algum modelo mais simples (Naive Bayes, CARTs etc), pois assim você terá um norte de performance. Ok, com tudo isso dito, vamos para a teoria de fato!\n",
    "\n",
    "## O que é uma Rede Neural?\n",
    "\n",
    "OK, vamos a uma pergunta um pouco mais profunda para entender de onde Redes Neurais, ou Artificial Neural Networks (ANNs), vieram. No campo de machine learning, nós queremos desenvolver algoritmos e estruturas que conseguem aprender com experiências e com isso obter uma performance desejada na execução de uma certa tarefa. O que já faz isso na natureza? Bem, parando pra pensar, nossos cérebros fazem justamente isso! Somos máquinas de aprendizado, então faz sentido se inspirar na nossa estrutura biológica para, então, construir uma nova tecnologia capaz de aprender. Então vamos tentar entender um neurônio biológico:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/neuron.png\" align=\"right\" width=\"50%\">\n",
    "Representação de um neurônio biológico. Num neurônio, temos basicamente canais de input (dendritos), um centro de processamento desse input (corpo celular), uma via de propagação de sinal elétrico (axônio) e canais de output (terminais do axônio). Isso parece muito com alguns pipelines que já vimos nesse curso! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, então faz sentido pensar que a estrutura fundamental do nosso sistema nervoso, o neurônio, seja capaz de realizar algum tipo de aprendizado. Pensando nisso, Frank Rosenblatt desenvolveu o modelo mais básico de rede neural, que consiste de um único neurônio, chamado **Perceptron**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/perceptron.jpg\" align=\"right\" width=\"50%\">\n",
    "\n",
    "\n",
    "Análogo ao neurônio, vemos canais de input xi, que são somados de acordo com um conjunto de pesos wi. Eles passam por uma **função de ativação** no corpo celular (no caso, uma step function) e então saem por um output. A esse movimento dos inputs, sendo multiplicados por seus pesos e passando pela função de ativação como uma combinação linear damos o nome de movimento **feedfoward**. O machine learning por trás de um perceptron torna-se, então, um problema de otimização dos pesos wi. Você pode ler mais sobre a história das redes neurais __[nesse link](http://www.andreykurenkov.com/writing/ai/a-brief-history-of-neural-nets-and-deep-learning/)__ e seu funcionamento __[nesse outro link](http://neuralnetworksanddeeplearning.com/chap1.html)__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, então precisamos entender como escolher os pesos wi certos para que o output seja correto. Queremos entender como que a variação de cada peso wi afetará o output, além de termos que atualizar os pesos de acordo com as conclusões que chegarmos. Se temos uma função custo, uma **loss function** definida (RMSE, MAE etc), podemos, então, aplicar um algoritmo já bem conhecido por nós em machine learning: o gradiente descendente! Ao movimento de volta do sinal, comparando os outputs y com nossas predições yhat e atualizando os pesos de cada conexão input de neurônios, damos o nome de **backpropagation**. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/neural_net_inner_working.gif\" align=\"right\" width=\"50%\">\n",
    "\n",
    "Aqui podemos ver o funcionamento de uma rede neural genérica. Repare na ordem dos acontecimentos. Um input entra e realiza o seu movimento de feedfoward. Resultados são gerados e comparados com os valores reais. Então, o sinal faz o caminho contrário na rede, atualizando os valores dos pesos num movimento de backpropagation. A cada ciclo desses, damos o nome de **epoch**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! Entendemos então como a rede neural mais básica funciona. Bem, ela não é beeeem uma rede, pois só tem 1 neurônio. Podemos adicionar mais camadas, mais neurônios, mais funções de ativação. No momento que o sistema se torna mais complexo, aí sim estamos falando de uma ANN! E o funcionamento de uma ANN é exatamente o mesmo: defina o número de epochs e sua arquitetura, e então fique repetindo o ciclo de feedfoward-backpropagation. <br>\n",
    "Ufa, muita coisa, eu imagino. Separei alguns links para vocês entenderem melhor o assunto, pois ele é muito visual:\n",
    " - __[NNs and backpropagation explained in a simple way](https://medium.com/datathings/neural-networks-and-backpropagation-explained-in-a-simple-way-f540a3611f5e)__\n",
    " - __[How backprop works](https://medium.com/free-code-camp/build-a-flexible-neural-network-with-backpropagation-in-python-acffeb7846d0)__\n",
    " - __[Essa coletânea fantástica do Welch Labs, NNs desmistificadas. Inclusive eu assistiria esses vídeos antes de todos](https://www.youtube.com/watch?v=bxe2T-V8XRs&list=PLiaHhY2iBX9hdHaRr6b7XevZtgZRa1PoU)__\n",
    " - __[Essa playslist do 3Blue1Brown sobre NNS](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi)__\n",
    "<br>\n",
    "Você pode entender arquiteturas de redes neurais quase como blocos de lego: cada neurônio diferente (ou seja, com estrutura e funções de ativação diferentes) realizará uma função diferente na rede. Elas são tão diversas quanto isso:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/neural_net_zoo.png\" align=\"left\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algumas últimas palavras antes de partirmos para a ação\n",
    "\n",
    "Agora sabemos como uma ANN funciona. Se for pra entender uma ANN de algum modo, veja-a como uma *aproximado universal de mappings, ou funções*. Por que ela é um aproximador universal, e por que ela funciona? __[Este artigo](https://towardsdatascience.com/understanding-neural-networks-what-how-and-why-18ec703ebd31)__ pode dar um ajudada nessa noção, mas eu gostaria de descer um pouco mais na matemática por trás de uma ANN. Para os curiosos, uma rede neural consegue naturalmente aproximar funções mesmo que não lineares, segundo o __[Teorema da Aproximação Universal](https://en.wikipedia.org/wiki/Universal_approximation_theorem)__. Você pode conferir sua prova __[aqui](https://hackernoon.com/illustrative-proof-of-universal-approximation-theorem-5845c02822f6)__. De novo, relembre nossa primeira aula de machine learning lá no módulo 3: machine learning é descobrir o mapping interno de um dataset. Ao entender que redes neurais são aproximadores universais de funções, você pode entender porque elas são tão poderosas, principalmente em casos complexos e não lineares, como visão computacional e NLP.\n",
    "\n",
    "## Brincando com redes neurais e seus hiperparâmetros\n",
    "\n",
    "Exercício 0: entre __[nesse playground](https://playground.tensorflow.org/)__ e comece a variar os hiperparâmetros disponíveis. Experimente trocar de dataset, colocar mais neurônios, menos neurônios, mudar camadas, learning rate, tudo. Quais conclusões podemos chegar? Para ajudar vocês a entender os hiperparâmetros de uma rede neural, gostaria de relembrar para vocês a santa trindade: dados (o que estou aprendendo), arquitetura (quem está aprendendo) e otimizador (como está aprendendo). No caso desse playground, podemos mexer nos 2 primeiros itens da trindade (e um pouco no terceiro ao regular a learning rate). Ao construir uma rede neural, podemos escolher:\n",
    " - **Modelo**: sequencial, recorrente etc. É o tipo geral de rede que estamos criando. Por enquanto, veremos as redes sequencias (aka fully connected)\n",
    " - **Layers**: quantas, quais e em qual ordem de camadas teremos em nossa arquitetura. Encare isso como blocos de lego que você vai encaixandoe construindo seu modelo. Quanto mais blocos, maior a capacidade do modelo de aprender, mas também maior sua chance de overfitting. Existe uma tonelada de tipos de layers que você pode colocar na sua rede, e aprenderemos as mais famosas nas próximas aulas.\n",
    " - **Neurônios ou Activation Functions**: para cada camada (ou até cada neurônio), qual será sua função interna. Sigmoide, ReLU, Leaky ReLu, step etc, você escolhe. É importante que sua diferenciação esteja definida sempre, pois queremos que o gradiente descendente funcione. Na dúvida, use ReLU.\n",
    " - **Otimizador**: Batch Gradient Descent, SGD, Mini Batch GD, Nesterov Momentum, Adagrad, Adam, qual otimizador? Na dúvida, use Adam\n",
    " - **Regularização**: terá regularização e com qual peso. Isso ajuda a prever overfitting\n",
    " - **Dropout**: terá ou não esquecimento temporário de neurônios para mais robustez. Isso ajuda a prever overfitting.\n",
    " - **Noise**: quanto de ruído será produzido internamente na rede. Isso ajuda a torná-la mais robusta.\n",
    " - **Epochs**: quantas epochs a rede realizará. Também é possível realizar early-stopping caso sua rede alcance a performance desejada\n",
    " - **Loss Function**: qual será a função custo que norteará a otimização do seu modelo\n",
    "<br>\n",
    "Existem mais alguns hiperparâmetros, mas por enquanto isso está suficiente. Vamos então construir nossa primeira rede neural com TF 2.0.\n",
    "\n",
    "## Nossa primeira rede neural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow version 2.0.0-alpha0\n"
     ]
    }
   ],
   "source": [
    "# Vamos importar o tensorflow\n",
    "import tensorflow as tf\n",
    "\n",
    "# E checar qual versão estamos utilizando.\n",
    "# Escolhemos a 2.0 pois sua sintaxe e funcionamento sem session.run() e \n",
    "# construção com base em Keras o torna muito mais intuitivo\n",
    "print(\"Using TensorFlow version\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 4s 61us/sample - loss: 0.2197 - accuracy: 0.9354\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 4s 58us/sample - loss: 0.0962 - accuracy: 0.9711\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 4s 61us/sample - loss: 0.0679 - accuracy: 0.9790\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 4s 60us/sample - loss: 0.0535 - accuracy: 0.9830\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 4s 59us/sample - loss: 0.0429 - accuracy: 0.9862\n",
      "10000/10000 [==============================] - 0s 25us/sample - loss: 0.0583 - accuracy: 0.9823\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0583485455690301, 0.9823]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# O Tensorflow 2.0 já vem com alguns datasets famosos dentro de si.\n",
    "# Vamos importar o MNIST\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "# O MNIST do TF 2.0 já vem quebrado em treino e test\n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalizando os valores de cada pixel\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# Aqui vamos montar nossa rede. Primeiramente, instanciamos um modelo\n",
    "# sequencial. Dentro do modelo, nós podemos colocar as camadas que quisermos\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)), # camada para transformar a imagem 28x28 em um vetor unidimensional\n",
    "  tf.keras.layers.Dense(512, activation=tf.nn.relu), # 512 neuronios fully connected com ativação por ReLU\n",
    "  tf.keras.layers.Dropout(0.2), # Dropout de 20% da última camada\n",
    "  tf.keras.layers.Dense(10, activation=tf.nn.softmax) # 10 neurônios softmax, para classificação. Um para cada categoria\n",
    "])\n",
    "\n",
    "# Depois de construida a arquitetura,\n",
    "# precisamos definir otimizador e loss function\n",
    "model.compile(optimizer='adam', # Estamos utilizando o ADAM\n",
    "              loss='sparse_categorical_crossentropy', # nossa loss function é entropia cross-categórica\n",
    "              metrics=['accuracy']) # nossa métrica base será acc. Podemos utilizar mais de uma métrica\n",
    "\n",
    "# Agora que tudo está instanciado, podemos realizar o fit do modelo.\n",
    "# No caso, vamos utilizar 5 epochs.\n",
    "model.fit(x_train, y_train, epochs=5)\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! Com poucas linhas de código, chegamos numa ótima acurácia. Repare na evolução da loss e accuracy ao longo das epochs. O que pode ser observado sobre sua variação e velocidade de variação? Qual a causa disso? <br>\n",
    "Na célula abaixo, faça o mesmo que fizemos no playground: experimente mudar todos os hiperparâmetros (de preferência 1 por vez). O que pode ser observado? Como chegar numa acurácia melhor? Como material de estudo, deixo aqui o __[link](https://www.tensorflow.org/tutorials)__ dos tutoriais oficiais do TF 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos testar um pouco a performance de redes neurais. Nas células abaixo, construa uma fully-connected para o dataset iris (importe do sci kit direto) e para o diabetes (pegue de aulas passadas).<br>\n",
    "DISCLAIMER: NAO ESQUEÇA DE NORMALIZAR COM ALGUM SCALER ANTES DE ALIMENTAR A ANN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
