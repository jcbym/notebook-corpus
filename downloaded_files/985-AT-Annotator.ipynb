{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import codecs\n",
    "import os\n",
    "import random\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Input, Dense, Lambda, Permute, Dropout\n",
    "from keras.layers import Conv2D, MaxPooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(fpath, label):\n",
    "    data = []\n",
    "    with codecs.open(fpath, 'r', 'utf-8', errors='ignore') as f:\n",
    "        lines = f.readlines()\n",
    "        for l in lines:\n",
    "            l = l.rstrip()\n",
    "            data.append((l.split(' '), label))\n",
    "    return data\n",
    "pos = load_data('./dataset/rt-polaritydata/rt-polarity.pos', 1)\n",
    "neg = load_data('./dataset/rt-polaritydata/rt-polarity.neg', 0)\n",
    "data = pos + neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence maxlen 60\n"
     ]
    }
   ],
   "source": [
    "sentence_maxlen = max(map(len, (d for d, _ in data)))\n",
    "print('sentence maxlen', sentence_maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab examples: ['', '!', '\"', '#3', '#9', '$1', '$100', '$20', '$40', '$50-million']\n"
     ]
    }
   ],
   "source": [
    "vocab = []\n",
    "for d, _ in data:\n",
    "    for w in d:\n",
    "        if w not in vocab: vocab.append(w)\n",
    "vocab = sorted(vocab)\n",
    "vocab_size = len(vocab)\n",
    "print('vocab examples:', vocab[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size 21384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3550"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('vocab size', len(vocab))\n",
    "w2i = {w:i for i,w in enumerate(vocab)}\n",
    "# i2w = {i:w for i,w in enumerate(vocab)}\n",
    "w2i['character']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorize(data, sentence_maxlen, w2i):\n",
    "    vec_data = []\n",
    "    labels = []\n",
    "    for d, label in data:\n",
    "        vec = [w2i[w] for w in d if w in w2i]\n",
    "        pad_len = max(0, sentence_maxlen - len(vec))\n",
    "        vec += [0] * pad_len\n",
    "        vec_data.append(vec)\n",
    "        \n",
    "        labels.append(label)\n",
    "    vec_data = np.array(vec_data)\n",
    "    labels = np.array(labels)\n",
    "    return vec_data, labels\n",
    "random.shuffle(data)\n",
    "vecX, vecY = vectorize(data, sentence_maxlen, w2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_data = len(vecX)\n",
    "split_ind = (int)(n_data * 0.9)\n",
    "trainX, trainY = vecX[:split_ind], vecY[:split_ind]\n",
    "testX, testY = vecX[split_ind:], vecY[split_ind:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n",
      "embed_matrix.shape (21384, 300)\n"
     ]
    }
   ],
   "source": [
    "def load_glove_weights(glove_dir, embd_dim, vocab_size, word_index):\n",
    "    embeddings_index = {}\n",
    "    f = open(os.path.join(glove_dir, 'glove.6B.' + str(embd_dim) + 'd.txt'))\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "\n",
    "    print('Found %s word vectors.' % len(embeddings_index)) \n",
    "    embedding_matrix = np.zeros((vocab_size, embd_dim))\n",
    "    print('embed_matrix.shape', embedding_matrix.shape)\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    return embedding_matrix\n",
    "\n",
    "embd_dim = 300\n",
    "glove_embd_w = load_glove_weights('./dataset', embd_dim, vocab_size, w2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "SentenceInput (InputLayer)   (None, 60)                0         \n",
      "_________________________________________________________________\n",
      "shared_embd (Embedding)      (None, 60, 300)           6415200   \n",
      "_________________________________________________________________\n",
      "permute_1 (Permute)          (None, 300, 60)           0         \n",
      "_________________________________________________________________\n",
      "lambda_1 (Lambda)            (None, 300, 60, 1)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 298, 1, 1)         181       \n",
      "_________________________________________________________________\n",
      "lambda_2 (Lambda)            (None, 298, 1)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 99, 1)             0         \n",
      "_________________________________________________________________\n",
      "lambda_3 (Lambda)            (None, 99)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 100       \n",
      "=================================================================\n",
      "Total params: 6,415,481\n",
      "Trainable params: 281\n",
      "Non-trainable params: 6,415,200\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def Net(vocab_size, embd_size, sentence_maxlen, glove_embd_w):\n",
    "    sentence = Input((sentence_maxlen,), name='SentenceInput')\n",
    "    \n",
    "    # embedding\n",
    "    embd_layer = Embedding(input_dim=vocab_size, \n",
    "                           output_dim=embd_size, \n",
    "                           weights=[glove_embd_w], \n",
    "                           trainable=False,\n",
    "                           name='shared_embd')\n",
    "    embd_sentence = embd_layer(sentence)\n",
    "    embd_sentence = Permute((2,1))(embd_sentence)\n",
    "    embd_sentence = Lambda(lambda x: K.expand_dims(x, -1))(embd_sentence)\n",
    "    \n",
    "    # cnn\n",
    "    cnn = Conv2D(1, \n",
    "                 kernel_size=(3, sentence_maxlen),\n",
    "                 activation='relu')(embd_sentence)\n",
    "    cnn =  Lambda(lambda x: K.sum(x, axis=3))(cnn)\n",
    "    cnn = MaxPooling1D(3)(cnn)\n",
    "    cnn = Lambda(lambda x: K.sum(x, axis=2))(cnn)\n",
    "    out = Dense(1, activation='sigmoid')(cnn)\n",
    "\n",
    "    model = Model(inputs=sentence, outputs=out, name='sentence_claccification')\n",
    "    model.compile(optimizer='adagrad', loss='binary_crossentropy', metrics=['accuracy']) \n",
    "    return model\n",
    "\n",
    "model = Net(vocab_size, embd_dim, sentence_maxlen, glove_embd_w)\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9595 samples, validate on 1067 samples\n",
      "Epoch 1/10\n",
      "9595/9595 [==============================] - 1s - loss: 0.6949 - acc: 0.5102 - val_loss: 0.6925 - val_acc: 0.5136\n",
      "Epoch 2/10\n",
      "9595/9595 [==============================] - 0s - loss: 0.6900 - acc: 0.5349 - val_loss: 0.6897 - val_acc: 0.5361\n",
      "Epoch 3/10\n",
      "9595/9595 [==============================] - 0s - loss: 0.6867 - acc: 0.5520 - val_loss: 0.6866 - val_acc: 0.5464\n",
      "Epoch 4/10\n",
      "9595/9595 [==============================] - 0s - loss: 0.6829 - acc: 0.5660 - val_loss: 0.6816 - val_acc: 0.5642\n",
      "Epoch 5/10\n",
      "9595/9595 [==============================] - 0s - loss: 0.6785 - acc: 0.5820 - val_loss: 0.6764 - val_acc: 0.5792\n",
      "Epoch 6/10\n",
      "9595/9595 [==============================] - 0s - loss: 0.6731 - acc: 0.5883 - val_loss: 0.6706 - val_acc: 0.5904\n",
      "Epoch 7/10\n",
      "9595/9595 [==============================] - 0s - loss: 0.6672 - acc: 0.6006 - val_loss: 0.6633 - val_acc: 0.6167\n",
      "Epoch 8/10\n",
      "9595/9595 [==============================] - 0s - loss: 0.6610 - acc: 0.6137 - val_loss: 0.6569 - val_acc: 0.6223\n",
      "Epoch 9/10\n",
      "9595/9595 [==============================] - 0s - loss: 0.6548 - acc: 0.6261 - val_loss: 0.6513 - val_acc: 0.6204\n",
      "Epoch 10/10\n",
      "9595/9595 [==============================] - 0s - loss: 0.6492 - acc: 0.6326 - val_loss: 0.6455 - val_acc: 0.6382\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f55f7740780>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trainX, trainY,\n",
    "            batch_size=32,\n",
    "            epochs=10,\n",
    "            validation_data=(testX, testY)\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
