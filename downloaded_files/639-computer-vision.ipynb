{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pneumonia Classification from X-Ray Images.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "t676FNRBFpfb"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snm29kztgtc4"
      },
      "source": [
        "Data loading and augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYIMd1rKrl_4"
      },
      "source": [
        "TRAIN_DIR = '/content/birds_species/train'\n",
        "TEST_DIR = '/content/birds_species/test'\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "IMG_SIZE = (224, 224)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVzNvtidHwkR"
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "class DataGenerator(object):\n",
        "    def __init__(\n",
        "        self, size=(224, 224),\n",
        "        color_mode='rgb',\n",
        "        batch_size=32,\n",
        "        class_mode='binary',\n",
        "        validation_split=0.15\n",
        "    ):\n",
        "        self.size = size\n",
        "        self.color_mode = color_mode\n",
        "        self.batch_size = batch_size\n",
        "        self.class_mode = class_mode\n",
        "        self.train_datagen = ImageDataGenerator(\n",
        "            shear_range=0.2,\n",
        "            zoom_range=0.2,\n",
        "            rotation_range=20,\n",
        "            height_shift_range=0.1,\n",
        "            width_shift_range=0.1,\n",
        "            brightness_range=[0.5, 1.5],\n",
        "            horizontal_flip=True,\n",
        "            vertical_flip=True,\n",
        "            validation_split=validation_split)\n",
        "        self.test_datagen = ImageDataGenerator()\n",
        "        \n",
        "    def load_train_data(self, dir):\n",
        "        train_generator = self.train_datagen.flow_from_directory(\n",
        "            dir,\n",
        "            target_size=self.size,\n",
        "            color_mode=self.color_mode,\n",
        "            batch_size=self.batch_size,\n",
        "            class_mode=self.class_mode,\n",
        "            subset= 'training')  \n",
        "        \n",
        "        validation_generator = self.train_datagen.flow_from_directory(\n",
        "            dir,\n",
        "            target_size=self.size,\n",
        "            color_mode=self.color_mode,\n",
        "            batch_size=self.batch_size,\n",
        "            class_mode=self.class_mode,\n",
        "            subset= 'validation')\n",
        "        \n",
        "        return train_generator, validation_generator\n",
        "\n",
        "    def load_test_data(self, dir):\n",
        "        test_generator = self.test_datagen.flow_from_directory(\n",
        "            dir,\n",
        "            target_size=self.size,\n",
        "            color_mode=self.color_mode,\n",
        "            batch_size=self.batch_size,\n",
        "            class_mode=self.class_mode,\n",
        "            shuffle=False)\n",
        "        \n",
        "        return test_generator\n",
        "\n",
        "\n",
        "datagen = DataGenerator(size=IMG_SIZE, batch_size=BATCH_SIZE)\n",
        "train_generator, validation_generator = datagen.load_train_data(TRAIN_DIR)\n",
        "test_generator = datagen.load_test_data(TEST_DIR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebN3vQ-Mhn-h"
      },
      "source": [
        "Model definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uAgRVMILhpRR"
      },
      "source": [
        "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
        "from tensorflow.keras import Input, Model\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
        "\n",
        "LAST_LAYER = 'block5_conv3'\n",
        "\n",
        "class VGGNet(object):\n",
        "    def __init__(self):\n",
        "        pass\n",
        "    \n",
        "    def __new__(cls, input_shape):\n",
        "        inputs = Input(shape=input_shape)\n",
        "        processed_input = preprocess_input(inputs)\n",
        "\n",
        "        base_model = VGG16(\n",
        "            input_tensor=processed_input,\n",
        "            include_top=False,\n",
        "            weights='imagenet')\n",
        "        # Fine-tune last conv block\n",
        "        fine_tune_at = 17\n",
        "        for layer in base_model.layers[:fine_tune_at]:\n",
        "            layer.trainable =  False\n",
        "        \n",
        "        gap = GlobalAveragePooling2D()\n",
        "        dropout = Dropout(0.2)\n",
        "        prediction_layer = Dense(1, activation='sigmoid')\n",
        "\n",
        "        x = base_model.output\n",
        "        x = gap(x)\n",
        "        x = dropout(x)\n",
        "        outputs = prediction_layer(x)\n",
        "\n",
        "        return Model(inputs, outputs)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sItzGdg2j5cT"
      },
      "source": [
        "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
        "from tensorflow.keras import Input, Model\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
        "\n",
        "LAST_LAYER = 'conv5_block3_3_conv'\n",
        "\n",
        "class ResNet(object):\n",
        "    def __init__(self):\n",
        "        pass\n",
        "    \n",
        "    def __new__(cls, input_shape):\n",
        "        inputs = Input(shape=input_shape)\n",
        "        processed_input = preprocess_input(inputs)\n",
        "\n",
        "        base_model = ResNet50(\n",
        "            input_tensor=processed_input,\n",
        "            include_top=False,\n",
        "            weights='imagenet')\n",
        "        # Fine-tune conv5_x onwards\n",
        "        fine_tune_at = 145\n",
        "        for layer in base_model.layers[:fine_tune_at]:\n",
        "            layer.trainable =  False\n",
        "        \n",
        "        gap = GlobalAveragePooling2D()\n",
        "        dropout = Dropout(0.2)\n",
        "        prediction_layer = Dense(1, activation='sigmoid')\n",
        "\n",
        "        x = base_model.output\n",
        "        x = gap(x)\n",
        "        x = dropout(x)\n",
        "        outputs = prediction_layer(x)\n",
        "\n",
        "        return Model(inputs, outputs)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m--x6RSAa9Fs"
      },
      "source": [
        "from tensorflow.keras.optimizers import SGD, Adam\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "class MyNet(object):\n",
        "    def __init__(self, model='VGG', input_shape=(224,224,3)):\n",
        "        if model == 'VGG':\n",
        "            self.model = VGGNet(input_shape)\n",
        "        elif model == 'ResNet':\n",
        "            self.model = ResNet(input_shape)\n",
        "        else:\n",
        "            print('Model not found. Available models = [VGG, ResNet]')\n",
        "\n",
        "    def train(self, train_generator, validation_generator, optimizer, learning_rate, epochs):\n",
        "        self._compile(optimizer, learning_rate)\n",
        "        callbacks = self._callbacks()\n",
        "        class_weight = self._class_weight(train_generator)\n",
        "        H = self.model.fit(\n",
        "            train_generator,\n",
        "            epochs=epochs,\n",
        "            steps_per_epoch=len(train_generator),\n",
        "            validation_data=validation_generator,\n",
        "            validation_steps=len(validation_generator),\n",
        "            callbacks=callbacks,\n",
        "            class_weight=class_weight)\n",
        "        \n",
        "        return H\n",
        "\n",
        "    def decode_predictions(self, predictions):\n",
        "        return (predictions > 0.5).flatten().astype(np.int8)\n",
        "\n",
        "    def _compile(self, optimizer='sgd', learning_rate=0.0005):\n",
        "        if optimizer == 'sgd':\n",
        "            optimizer = SGD(learning_rate=learning_rate, momentum=0.9)\n",
        "        else:\n",
        "            optimizer = Adam(learning_rate=learning_rate)\n",
        "        self.model.compile(\n",
        "            optimizer=optimizer,\n",
        "            loss='binary_crossentropy',\n",
        "            metrics=['accuracy'])\n",
        "        \n",
        "    def _callbacks(self):\n",
        "        rl = ReduceLROnPlateau(\n",
        "            monitor='val_loss', factor=0.1, patience=3, verbose=1)\n",
        "        es = EarlyStopping(\n",
        "            monitor='val_loss', patience=5, verbose=1, restore_best_weights=True)\n",
        "\n",
        "        return [rl, es]\n",
        "\n",
        "    def _class_weight(self, generator):\n",
        "        classes = np.unique(generator.classes)\n",
        "        weights = compute_class_weight(\n",
        "            class_weight='balanced',\n",
        "            classes=classes,\n",
        "            y=generator.classes)\n",
        "        \n",
        "        return {k : v for k, v in zip(classes, weights)}"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8M8w4ilKnFH"
      },
      "source": [
        "Model training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCklubFhKxaN"
      },
      "source": [
        "net = MyNet('ResNet', train_generator.image_shape)\n",
        "net.model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9NJH8Evg1-S"
      },
      "source": [
        "EPOCHS = 25\n",
        "\n",
        "# VGG\n",
        "# LR = 0.0005\n",
        "# OPTIMIZER = 'sgd'\n",
        "\n",
        "# RESNET\n",
        "LR = 0.001\n",
        "OPTIMIZER = 'Adam'\n",
        "\n",
        "H = net.train(train_generator, validation_generator, OPTIMIZER, LR, EPOCHS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cpAVEGGgspgg"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def train_plot(H, figsize=(10, 5)):\n",
        "    acc = H.history['accuracy']\n",
        "    val_acc = H.history['val_accuracy']\n",
        "    loss = H.history['loss']\n",
        "    val_loss = H.history['val_loss']\n",
        "\n",
        "    N = list(range(1, len(acc) + 1))\n",
        "    plt.figure(figsize=figsize)\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(N, acc, label='Training')\n",
        "    plt.plot(N, val_acc, label='Validation')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.ylim([0,1.0])\n",
        "    plt.title('Training and Validation Accuracy')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(N, loss, label='Training')\n",
        "    plt.plot(N, val_loss, label='Validation')\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.ylim([0,1.0])\n",
        "    plt.title('Training and Validation Loss')\n",
        "    \n",
        "    plt.show()\n",
        "\n",
        "train_plot(H)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5D0VrpSHWlI"
      },
      "source": [
        "Quantitative Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ld25gg6TTvlA"
      },
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def plot_confusion_matrix(y_true, y_pred, classes=None, normalize=False, k=3):\n",
        "    \"\"\"\n",
        "    Display confusion matrix as a heatmap.\n",
        "    \"\"\"\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    r, c = cm.shape\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "    df = pd.DataFrame(cm)\n",
        "    if classes:\n",
        "        df.columns = classes\n",
        "        df.index = classes\n",
        "    fig, ax = plt.subplots(figsize=(k*r, k*c))\n",
        "    ax = sns.heatmap(df, annot=True, cmap='YlGnBu')\n",
        "    plt.show()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEL9cDPiLLsT"
      },
      "source": [
        "from time import time\n",
        "\n",
        "y_true = test_generator.classes\n",
        "classes = test_generator.class_indices.keys()\n",
        "label2class = {v:k for k, v in test_generator.class_indices.items()}\n",
        "\n",
        "s = time()\n",
        "predictions = net.model.predict(test_generator)\n",
        "print('Inference time: {}'.format(time() - s))\n",
        "\n",
        "y_pred = net.decode_predictions(predictions)\n",
        "\n",
        "score = accuracy_score(y_true, y_pred)\n",
        "print('Test Accuracy = ', score)\n",
        "\n",
        "plot_confusion_matrix(y_true, y_pred, classes, normalize=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3Pp2hsBHqDq"
      },
      "source": [
        "Qualitative Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUr-5fVIWkNR"
      },
      "source": [
        "from tensorflow.keras import Model\n",
        "from skimage.transform import resize\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def extract_images(generator):\n",
        "    test_images = []\n",
        "    for i, (x, _) in enumerate(generator):\n",
        "        test_images.extend(x)\n",
        "        if i >= len(generator) - 1:\n",
        "            break\n",
        "    \n",
        "    return np.array(test_images, dtype=np.uint8)\n",
        "\n",
        "def gallery(net, collection, layer_name, indices=None, labels2class=None, nrows=1, ncols=4, k=8):\n",
        "    \"\"\"\n",
        "    Display a random nrows x ncols gallery from collection.\n",
        "    \"\"\"\n",
        "    if indices is None:\n",
        "        indices = len(collection)\n",
        "        \n",
        "    rng = np.random.default_rng()\n",
        "    fig, axs = plt.subplots(nrows, ncols, figsize=(k*nrows, k*ncols), squeeze=False)\n",
        "    for r in range(nrows):\n",
        "        for c in range(ncols):\n",
        "            ax = axs[r][c]\n",
        "            i = rng.choice(indices, replace=False)\n",
        "            img = collection[i]\n",
        "\n",
        "            array = np.expand_dims(img, axis=0)\n",
        "            preds = net.model.predict(array)\n",
        "            label = net.decode_predictions(preds)[0]\n",
        "\n",
        "            heatmap = make_gradcam_heatmap(array, net.model, layer_name)\n",
        "            upsample = resize(heatmap, IMG_SIZE, preserve_range=True)\n",
        "\n",
        "            ax.imshow(img)\n",
        "            ax.imshow(upsample, alpha=0.4, cmap='jet')\n",
        "\n",
        "            title = label\n",
        "            if labels2class is not None:\n",
        "                title = labels2class[title]\n",
        "            ax.set_title(title)\n",
        "\n",
        "    fig.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n",
        "    # First, we create a model that maps the input image to the activations\n",
        "    # of the last conv layer as well as the output predictions\n",
        "    grad_model = Model(\n",
        "        [model.inputs],\n",
        "        [model.get_layer(last_conv_layer_name).output, model.output])\n",
        "\n",
        "    # Then, we compute the gradient of the top predicted class for our input image\n",
        "    # with respect to the activations of the last conv layer\n",
        "    with tf.GradientTape() as tape:\n",
        "        last_conv_layer_output, preds = grad_model(img_array)\n",
        "        if pred_index is None:\n",
        "            pred_index = tf.argmax(preds[0])\n",
        "        class_channel = preds[:, pred_index]\n",
        "\n",
        "    # This is the gradient of the output neuron (top predicted or chosen)\n",
        "    # with regard to the output feature map of the last conv layer\n",
        "    grads = tape.gradient(class_channel, last_conv_layer_output)\n",
        "\n",
        "    # This is a vector where each entry is the mean intensity of the gradient\n",
        "    # over a specific feature map channel\n",
        "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
        "\n",
        "    # We multiply each channel in the feature map array\n",
        "    # by \"how important this channel is\" with regard to the top predicted class\n",
        "    # then sum all the channels to obtain the heatmap class activation\n",
        "    last_conv_layer_output = last_conv_layer_output[0]\n",
        "    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n",
        "    heatmap = tf.squeeze(heatmap)\n",
        "\n",
        "    # For visualization purpose, we will also normalize the heatmap between 0 & 1\n",
        "    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n",
        "    return heatmap.numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQ-P94QUJ2Fv"
      },
      "source": [
        "test_images = extract_images(test_generator)\n",
        "\n",
        "print('Correctly classified images and their predicted labels:')\n",
        "correct_indices = np.argwhere(y_true == y_pred).flatten()\n",
        "gallery(net, test_images, LAST_LAYER, correct_indices, label2class)\n",
        "\n",
        "print('Incorrectly classified images and their predicted labels:')\n",
        "wrong_indices = np.argwhere(y_true != y_pred).flatten()\n",
        "gallery(net, test_images, LAST_LAYER, wrong_indices, label2class)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}