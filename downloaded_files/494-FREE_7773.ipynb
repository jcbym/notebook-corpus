{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "underlying-payroll",
   "metadata": {},
   "source": [
    "# Intro to Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fc4063",
   "metadata": {},
   "source": [
    "In this notebook we experiment with methods to transform sentences into vectors, and use vectors for text classification.\n",
    "\n",
    "While a lot of contemporary NLP is neural, discussing simpler techniques first will help us introduce some terminology and build some intuition on what (doesn't) work(s) and why.\n",
    "\n",
    "You are encouraged to play around with the code and modify / re-built parts of it as you fit: there is NO substitute for \"tinkering with code\" to understand how all the concepts fit together (corollary: all this code is written for pedagogical purposes, so some functions are re-used from previous lectures to provide a self-sufficient script)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diagnostic-buying",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some global import\n",
    "# we import specific libraries in due time\n",
    "import json\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from matplotlib import pyplot as plt\n",
    "from random import choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "identical-disability",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebe1e72",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c86446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure you have the datasets library installed\n",
    "# see: https://github.com/huggingface/datasets\n",
    "\n",
    "# !pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c286cfaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "# some utils function\n",
    "def get_finance_sentiment_dataset(split: str='sentences_allagree'):\n",
    "    # load financial dataset from HF\n",
    "    from datasets import load_dataset\n",
    "    # https://huggingface.co/datasets/financial_phrasebank\n",
    "    # by default, load just sentences for which all annotators agree\n",
    "    dataset = load_dataset(\"financial_phrasebank\", split)\n",
    "    \n",
    "    return dataset['train']\n",
    "\n",
    "\n",
    "def get_finance_sentences():\n",
    "    dataset = get_finance_sentiment_dataset()\n",
    "    cleaned_dataset = [[pre_process_sentence(_['sentence']), _['label']] for _ in dataset]\n",
    "    # debug \n",
    "    print(\"{} cleaned sentences from finance dataset\\n\".format(len(cleaned_dataset)))\n",
    "    \n",
    "    return cleaned_dataset\n",
    "\n",
    "\n",
    "def pre_process_sentence(sentence: str):\n",
    "    # this choices are VERY important. Here, we take a simplified \n",
    "    # view, remove the punctuations and just lower case everything\n",
    "    lower_sentence = sentence.lower()\n",
    "    # remove punctuation\n",
    "    # nice suggestion from https://stackoverflow.com/questions/265960/best-way-to-strip-punctuation-from-a-string\n",
    "    # if we change the exclude set, we can control what to exclude\n",
    "    exclude = set(string.punctuation)\n",
    "    return ''.join(ch for ch in lower_sentence if ch not in exclude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bcd289",
   "metadata": {},
   "outputs": [],
   "source": [
    "finance_dataset = get_finance_sentences()\n",
    "# print out the first items in the dataset, to check the format\n",
    "finance_dataset[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ed414d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get sentences without label for vectorizer part\n",
    "finance_dataset_sentences = [_[0] for _ in finance_dataset]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "japanese-season",
   "metadata": {},
   "source": [
    "## Introducing count and tf-idf vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "internal-seller",
   "metadata": {},
   "source": [
    "_Computers do NOT understand words, so the first thing to do is to map the sentences found in the dataset to vectors of numbers._\n",
    "\n",
    "The oldest trick for manipulating text is the so-called \"vector space model\" (still very much in Information Retrieval), which is simply the idea that you can map documents to vectors and use \"geometric notions\" to compute interesting metrics, such as semantic similarity. The oldest mapping between words and vectors is a simple count transformation:\n",
    "\n",
    "D1: 'The bill is passed.'\n",
    "D2: 'Jesse proposed the bill.'\n",
    "\n",
    "D1: The=1, bill=1, is=1, passed=1, Jesse=0, proposed=0\n",
    "D2: The=1, bill=1, is=0, passed=0, Jesse=1, proposed=1\n",
    "\n",
    "A more sophisticated version is TF-IDF, which is the foundation of most search engines (SOLR, Elastic etc.): vectors are not counts, but they weights, as we discussed in class. \n",
    "\n",
    "Let's see the difference with some Python examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "institutional-channel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# difference between count and tf-idf vectors\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "# example docs\n",
    "docs = finance_dataset_sentences[:2]\n",
    "# instantiate the vectorizer object\n",
    "countvectorizer = CountVectorizer(analyzer='word')\n",
    "tfidfvectorizer = TfidfVectorizer(analyzer='word')\n",
    "# convert the documents into a matrix\n",
    "count_wm = countvectorizer.fit_transform(docs)\n",
    "tfidf_wm = tfidfvectorizer.fit_transform(docs)\n",
    "# display the difference\n",
    "count_tokens = countvectorizer.get_feature_names()\n",
    "tfidf_tokens = tfidfvectorizer.get_feature_names()\n",
    "df_countvect = pd.DataFrame(data=count_wm.toarray(),\n",
    "                            index=['Doc{}'.format(_) for _ in range(len(docs))], \n",
    "                            columns=count_tokens)\n",
    "df_tfidfvect = pd.DataFrame(data=tfidf_wm.toarray(),\n",
    "                            index=['Doc{}'.format(_) for _ in range(len(docs))], \n",
    "                            columns=tfidf_tokens)\n",
    "print(\"Count Vectorizer\\n\")\n",
    "print(df_countvect)\n",
    "print(\"\\nTD-IDF Vectorizer\\n\")\n",
    "print(df_tfidfvect)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "directed-expert",
   "metadata": {},
   "source": [
    "* Important observation # 1: the size of the vector for each doc is proportional to vocabulary size.\n",
    "* Important observation # 2: the vectors are mostly empty (it is a \"sparse\" representation). As we shall see in future lectures, neural network produces \"dense\" representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proved-emission",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see how vector dimension changes with more docs\n",
    "docs = finance_dataset_sentences[:100]\n",
    "countvectorizer = CountVectorizer(analyzer='word')\n",
    "count_wm = countvectorizer.fit_transform(docs)\n",
    "count_tokens = countvectorizer.get_feature_names()\n",
    "df_countvect = pd.DataFrame(data=count_wm.toarray(), \n",
    "                            index=['Doc{}'.format(_) for _ in range(len(docs))], \n",
    "                            columns=count_tokens)\n",
    "print(\"Count Vectorizer\\n\")\n",
    "print(df_countvect)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c9b527",
   "metadata": {},
   "source": [
    "If documents are points in a |V| dimensional space, their similarity is given by the angle between them: we show how to compute similarity with cosine first, and then wrap it up in a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caroline-machinery",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate similarity using cosine similarity\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "docs = finance_dataset_sentences[:5]\n",
    "tfidfvectorizer = TfidfVectorizer(analyzer='word')\n",
    "tfidf_wm = tfidfvectorizer.fit_transform(docs)\n",
    "cosine_similarities = linear_kernel(tfidf_wm, tfidf_wm).flatten()\n",
    "# print out for debug - the diagonal is 1 as all documents are perfectly\n",
    "# similar to themselves\n",
    "cosine_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cutting-designer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrap up similarity calculation in a function\n",
    "def most_similar_docs_tf_idf(target_doc_index, docs, tfidf, top_k=3, debug=False):\n",
    "    sims = linear_kernel(tfidf[target_doc_index:target_doc_index + 1], tfidf).flatten()\n",
    "    indices = sims.argsort()[:-top_k-2:-1]\n",
    "    \n",
    "    if debug:\n",
    "        print(indices)\n",
    "    \n",
    "    return [docs[i] for i in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quarterly-yield",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, d in enumerate(docs):\n",
    "    print(\"\\n======== Most similar docs to: {}\\n\".format(d))\n",
    "    print(most_similar_docs_tf_idf(idx, docs, tfidf_wm)[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "front-scholarship",
   "metadata": {},
   "source": [
    "_Let's run on all the dataset now!!!_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alternate-speed",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text_data = finance_dataset_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd033d2",
   "metadata": {},
   "source": [
    "* Important observation: the scikit vectorizer comes with MANY options (https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html). As we discussed in class, there are non-equivalent ways of cleaning text, tokenize it, etc. The removal of stop words, or how frequent a word should be to be included in the model, or what ngram size to use, are governed by parameters that can be tweaked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "angry-girlfriend",
   "metadata": {},
   "outputs": [],
   "source": [
    "finance_tfidfvectorizer = TfidfVectorizer(analyzer='word', stop_words='english')\n",
    "finance_tfidf_wm = tfidfvectorizer.fit_transform(all_text_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56de759",
   "metadata": {},
   "source": [
    "We pick a random sentence from our data, and compute the most similar sentences according to TF-IDF..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "current-basket",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "rnd_idx = randint(0, len(all_text_data))\n",
    "print(\"\\n======== Most similar docs to:\\n {}-{}\\n\\n\".format(rnd_idx, all_text_data[rnd_idx]))\n",
    "top_docs = most_similar_docs_tf_idf(rnd_idx, all_text_data, finance_tfidf_wm, debug=True)\n",
    "for t in top_docs[1:]:\n",
    "    print(t, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a093e5",
   "metadata": {},
   "source": [
    "## Applied Vectorization: Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435f3ef0",
   "metadata": {},
   "source": [
    "Once we have vectors, we can proceed to build a classifier pretty much exactly like you did for other supervised learning projects in the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4ae758",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "finance_dataset_text = [_[0] for _ in finance_dataset]\n",
    "finance_dataset_label = [_[1] for _ in finance_dataset]\n",
    "all_labels = set(finance_dataset_label)\n",
    "print(\"All labels are: {}\".format(all_labels))\n",
    "X_train, X_test, y_train, y_test = train_test_split(finance_dataset_text, \n",
    "                                                    finance_dataset_label, \n",
    "                                                    test_size=0.1, \n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460cbc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X_train), len(X_test), X_train[:3], y_train[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bf32ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_tfidfvectorizer = TfidfVectorizer(analyzer='word', stop_words='english')\n",
    "final_tfidf_train = final_tfidfvectorizer.fit_transform(X_train)\n",
    "final_tfidf_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6f924c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_transformed = final_tfidfvectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd44500",
   "metadata": {},
   "source": [
    "#### Bonus: Chi-Square selection\n",
    "\n",
    "If the model is too big (remember: the model grows with vocabulary size!), we can check which words are most predictive of the target label, and trim the model accordingly (i.e. we are doing feature selection on our text data - see for example https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.chi2.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292f951b",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = final_tfidfvectorizer.get_feature_names()\n",
    "print(len(feature_names), feature_names[-3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ff5c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "ch2 = SelectKBest(chi2, k=25)\n",
    "X_chi_train = ch2.fit_transform(final_tfidf_train, y_train)\n",
    "X_chi_test = ch2.transform(X_test_transformed)\n",
    "new_feature_names =  np.array(feature_names)[ch2.get_support()]\n",
    "print(len(new_feature_names), new_feature_names[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967add6b",
   "metadata": {},
   "source": [
    "_We now instantiate a classifier, train it and then predicting unseen test cases as usual._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424e2b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "# from https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB\n",
    "\n",
    "model = MultinomialNB()\n",
    "# train and test the classifier\n",
    "model.fit(final_tfidf_train, y_train)\n",
    "predicted = model.predict(X_test_transformed)\n",
    "predicted_prob = model.predict_proba(X_test_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e64b8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug\n",
    "print(predicted[:3], predicted_prob[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bb489d",
   "metadata": {},
   "source": [
    "* Important observation: thanks to scikit great API, we can re-run the code below swapping in and out different classifiers, and compare their performance easily - for example, we could try a LinearSVC (https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electoral-retro",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "def calculate_confusion_matrix_and_report(y_predicted, y_golden, with_plot=True):\n",
    "    # calculate confusion matrix: \n",
    "    cm = confusion_matrix(y_golden, y_predicted)\n",
    "    # build a readable report;\n",
    "    # https://scikit-learn.org/stable/modules/model_evaluation.html#classification-report\n",
    "    print('\\nClassification Report')\n",
    "    print(classification_report(y_golden, y_predicted))\n",
    "    # plot the matrix\n",
    "    if with_plot:\n",
    "        plot_confusion_matrix(cm)\n",
    "                                          \n",
    "    return\n",
    "                                          \n",
    "def plot_confusion_matrix(c_matrix):\n",
    "    plt.imshow(c_matrix, cmap=plt.cm.Blues)\n",
    "    plt.xlabel(\"Predicted labels\")\n",
    "    plt.ylabel(\"True labels\")\n",
    "    plt.xticks([], [])\n",
    "    plt.yticks([], [])\n",
    "    plt.title(\"Confusion matrix\")\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9022af0a",
   "metadata": {},
   "source": [
    "As usual, we first try to understand the model behavior on unseen data through some rough quantitative measures and visualization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mathematical-northern",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total of # {} test cases\".format(len(y_test)))\n",
    "calculate_confusion_matrix_and_report(predicted, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instrumental-drive",
   "metadata": {},
   "source": [
    "### Error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "radical-singapore",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(X_test) == len(predicted)\n",
    "# manual inspection\n",
    "mistakes = [(x, p, y, prob) for x, p, y, prob in zip(X_test, predicted, y_test, predicted_prob) if p != y]\n",
    "print(\"Total of mistakes: {}\".format(len(mistakes)))\n",
    "# debug\n",
    "print(\"Sentence: {}\\nPredicted: {}, but it was: {}\\nProbs: {}\".format(*mistakes[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba25085",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(3):\n",
    "    rnd_mistake = choice(mistakes)\n",
    "    print(\"Sentence: {}\\nPredicted: {}, but it was: {}\\nProbs: {}\\n=======\\n\".format(*rnd_mistake))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4a7154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data slicing: instead of considering the performances on all dataset, \n",
    "# we split it according to categories important for our business\n",
    "\n",
    "# for example, let's say we slice queries by quarter\n",
    "slices = {\n",
    "    \"first quarter\": [[], []],\n",
    "    \"second quarter\": [[], []],\n",
    "    \"third quarter\": [[], []],\n",
    "    \"fourth quarter\": [[], []]\n",
    "}\n",
    "\n",
    "for x, p, y in zip(X_test, predicted, y_test):\n",
    "    for _s in slices.keys():\n",
    "        if _s in x:\n",
    "            slices[_s][0].append(p)\n",
    "            slices[_s][1].append(y)\n",
    "            \n",
    "# debug \n",
    "# print(slices[\"first quarter\"])\n",
    "            \n",
    "for _slice, test_set in slices.items():\n",
    "    if test_set[0]:\n",
    "        print(\"Total of # {} cases in slice: {}\".format(len(test_set[0]), _slice))\n",
    "        calculate_confusion_matrix_and_report(test_set[0], test_set[1], with_plot=False)\n",
    "        print(\"\\n===========\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d040753",
   "metadata": {},
   "source": [
    "We should be able to adapt the “black box testing” from traditional software systems to ML systems: it should be possible to evaluate the performance of a complex system by treating it as a black box, and only supply input-output pairs that are relevant for our qualitative understanding.\n",
    "\n",
    "(see for example the excellent paper: https://arxiv.org/abs/2005.04118)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a28720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for edge cases / interesting cases / regression errors\n",
    "\n",
    "# CASE 1:\n",
    "# I'm particularly interesting in some company, say\n",
    "# https://en.wikipedia.org/wiki/Comptel\n",
    "# and want to make sure we are doing well there!\n",
    "\n",
    "companies_I_care_about = ['comptel']\n",
    "\n",
    "for company in companies_I_care_about:\n",
    "    print(\"\\n======\\nFocus on target company: {}\\n\".format(company))\n",
    "    for x, p, y in zip(X_test, predicted, y_test):\n",
    "        if company in x:\n",
    "            print(\"For '{}' =>\\ngolden {}, predicted {}\\n\".format(\n",
    "                x,\n",
    "                y, \n",
    "                p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027d4fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CASE 2:\n",
    "# Assuming we have some specific sentences to monitor, let's check for that!\n",
    "sentences_I_care_about = [\n",
    "    'the company slipped to an operating loss of eur 26 million from a profit of eur 13 million',\n",
    "    'revenue in the quarter fell 8 percent to  euro  24 billion compared to a year earlier'\n",
    "]\n",
    "labels_I_care_about = [0, 0]\n",
    "\n",
    "for x, p in zip(X_test, predicted):\n",
    "     if x.strip() in sentences_I_care_about:\n",
    "        print(\"For '{}', I expect {}, it was {}\\n\".format(\n",
    "            x,\n",
    "            p, \n",
    "            labels_I_care_about[sentences_I_care_about.index(x.strip())]\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96275a4",
   "metadata": {},
   "source": [
    "In test classification, we desire to have a system robust to alternative specifications of the input string, for example we expect that the model response to:\n",
    "\n",
    "\"revenue in the quarter fell 8 percent to  euro  24 billion compared to a year earlier\"\n",
    "\n",
    "AND\n",
    "\n",
    "\"revenue in the quarter diminished by 8 percent to  euro  24 billion compared to the previous year\"\n",
    "\n",
    "would be identical.\n",
    "\n",
    "While a full-fledge treatment of this problem is out of scope, let's see how this intuition works with working code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852d3bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test for perturbations\n",
    "test_sentences = [\n",
    "    'the company slipped to an operating loss of eur 26 million from a profit of eur 13 million',\n",
    "    'revenue in the quarter fell 8 percent to  euro  24 billion compared to a year earlier'\n",
    "]\n",
    "\n",
    "perturbated_sentences = [\n",
    "    'operating loss surged to eur 26 million from a profit of eur 13 million',\n",
    "    'revenue in the quarter diminished by 8 percent to  euro  24 billion compared to the previous year'\n",
    "]\n",
    "\n",
    "test_predicted = model.predict(final_tfidfvectorizer.transform(test_sentences))\n",
    "perturbated_predicted = model.predict(final_tfidfvectorizer.transform(perturbated_sentences))\n",
    "\n",
    "for s, t, p in zip(test_sentences, test_predicted, perturbated_predicted):\n",
    "    print(\"\\nFor sentence '{}', prediction was: {}, under perturbation: {}\".format(\n",
    "        s, t, p\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc58715",
   "metadata": {},
   "source": [
    "Bonus: a quick and easy way to generate perturbation is called back-translation.\n",
    "\n",
    "The idea is that you can use machine translation to go:\n",
    "\n",
    "SOURCE -> TARGET -> NEW_SOURCE\n",
    "\n",
    "where NEW_SOURCE is a semantically equivalent, but not identical version of source. For example:\n",
    "\n",
    "'hi' -> Italian target: 'ciao' -> 'hello'\n",
    "\n",
    "'hi' and 'hello' have the same meaning so, 'hello' may be considered a perturbation of the original text.\n",
    "\n",
    "A quick and dirty example follows (not \"real\" code!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a2a656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install BackTranslation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7232ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from BackTranslation import BackTranslation\n",
    "trans = BackTranslation(url=[\n",
    "      'translate.google.com',\n",
    "      'translate.google.co.kr',\n",
    "    ])\n",
    "for t in test_sentences:\n",
    "    result = trans.translate(t, src='en', tmp = 'zh-cn')\n",
    "    print(\"Original is: {}\\nNew sentence is: {}\\n\\n\".format(t, result.result_text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
