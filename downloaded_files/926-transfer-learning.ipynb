{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 7. 텍스트 문서의 범주화 - (4) IMDB 데이터에 전이학습 적용하기\n",
    "\n",
    "- 작업에 필요한 <b>레이블이 있는 데이터</b>가 적은데 비슷한 <b>다른 도메인의 훈련 데이터</b>는 많을 경우, 전이학습이 유용하다.\n",
    "- 이제 전이학습을 이용하여 불충분한 데이터셋으로도 CNN 모델을 학습시켜보자.\n",
    "    - IMDB 영화리뷰 데이터셋에서 일부(5%)만 추출하여 불충분한 데이터셋으로 사용한다\n",
    "- IMDB 영화 리뷰 데이터를 다운로드 받아 data 디렉토리에 압축 해제한다\n",
    "    - 다운로드 : http://ai.stanford.edu/~amaas/data/sentiment/\n",
    "    - 저장경로 : data/aclImdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import config\n",
    "from dataloader.loader import Loader\n",
    "from preprocessing.utils import Preprocess, remove_empty_docs\n",
    "from dataloader.embeddings import GloVe\n",
    "from model.cnn_document_model import DocumentModel, TrainingParameters\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 파라미터 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습된 모델을 저장할 디렉토리 생성\n",
    "if not os.path.exists(os.path.join(config.MODEL_DIR, 'imdb')):\n",
    "    os.makedirs(os.path.join(config.MODEL_DIR, 'imdb'))\n",
    "\n",
    "# 학습 파라미터 설정\n",
    "train_params = TrainingParameters('imdb_transfer_tanh_activation', \n",
    "                                  model_file_path = config.MODEL_DIR+ '/imdb/transfer_model_10.hdf5',\n",
    "                                  model_hyper_parameters = config.MODEL_DIR+ '/imdb/transfer_model_10.json',\n",
    "                                  model_train_parameters = config.MODEL_DIR+ '/imdb/transfer_model_10_meta.json',\n",
    "                                  num_epochs=30,\n",
    "                                  batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMDB 데이터셋 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df.shape : (1250, 2)\n",
      "test_df.shape : (25000, 2)\n",
      "corpus size : 1250\n",
      "target size : 1250\n"
     ]
    }
   ],
   "source": [
    "# 다운받은 IMDB 데이터 로드: 학습셋은 5%만 취한다 (전체는 2만5천개)\n",
    "train_df = Loader.load_imdb_data(directory = 'train')\n",
    "train_df = train_df.sample(frac=0.05, random_state = train_params.seed)\n",
    "print(f'train_df.shape : {train_df.shape}')\n",
    "\n",
    "test_df = Loader.load_imdb_data(directory = 'test')\n",
    "print(f'test_df.shape : {test_df.shape}')\n",
    "\n",
    "# 텍스트 데이터, 레이블 추출\n",
    "corpus = train_df['review'].tolist()\n",
    "target = train_df['sentiment'].tolist()\n",
    "corpus, target = remove_empty_docs(corpus, target)\n",
    "print(f'corpus size : {len(corpus)}')\n",
    "print(f'target size : {len(target)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 인덱스 시퀀스 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4917 unique tokens.\n",
      "All documents processed.cessed."
     ]
    }
   ],
   "source": [
    "# 학습셋을 인덱스 시퀀스로 변환\n",
    "preprocessor = Preprocess(corpus=corpus)\n",
    "corpus_to_seq = preprocessor.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus_to_seq size : 1250\n",
      "corpus_to_seq[0] size : 300\n"
     ]
    }
   ],
   "source": [
    "print(f'corpus_to_seq size : {len(corpus_to_seq)}')\n",
    "print(f'corpus_to_seq[0] size : {len(corpus_to_seq[0])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All documents processed.ocessed."
     ]
    }
   ],
   "source": [
    "# 테스트셋을 인덱스 시퀀스로 변환\n",
    "test_corpus = test_df['review'].tolist()\n",
    "test_target = test_df['sentiment'].tolist()\n",
    "test_corpus, test_target = remove_empty_docs(test_corpus, test_target)\n",
    "test_corpus_to_seq = preprocessor.transform(test_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_corpus_to_seq size : 25000\n",
      "test_corpus_to_seq[0] size : 300\n"
     ]
    }
   ],
   "source": [
    "print(f'test_corpus_to_seq size : {len(test_corpus_to_seq)}')\n",
    "print(f'test_corpus_to_seq[0] size : {len(test_corpus_to_seq[0])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape : (1250, 300)\n",
      "y_train.shape : (1250,)\n",
      "x_test.shape : (25000, 300)\n",
      "y_test.shape : (25000,)\n"
     ]
    }
   ],
   "source": [
    "# 학습셋, 테스트셋 준비\n",
    "x_train = np.array(corpus_to_seq)\n",
    "x_test = np.array(test_corpus_to_seq)\n",
    "y_train = np.array(target)\n",
    "y_test = np.array(test_target)\n",
    "\n",
    "print(f'x_train.shape : {x_train.shape}')\n",
    "print(f'y_train.shape : {y_train.shape}')\n",
    "print(f'x_test.shape : {x_test.shape}')\n",
    "print(f'y_test.shape : {y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe 임베딩 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 50 dim GloVe vectors\n",
      "Found 400000 word vectors.\n",
      "words not found in embeddings: 10\n",
      "initial_embeddings.shape : (4919, 50)\n"
     ]
    }
   ],
   "source": [
    "# GloVe 임베딩 초기화 - glove.6B.50d.txt pretrained 벡터 사용\n",
    "glove = GloVe(50)\n",
    "initial_embeddings = glove.get_embedding(preprocessor.word_index)\n",
    "print(f'initial_embeddings.shape : {initial_embeddings.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  훈련된 모델 로드\n",
    "\n",
    "- HandsOn03에서 아마존 리뷰 데이터로 학습한 CNN 모델을 로드한다.\n",
    "- DocumentModel 클래스의 load_model로 모델을 로드하고, load_model_weights로 학습된 가중치를 가져온다. \n",
    "- 그 후, GloVe.update_embeddings 함수로 GloVe 초기화 임베딩을 업데이트한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size = 43197  and the index of vocabulary words passed has 43195 words\n",
      "WARNING:tensorflow:From /Users/dhkdn9192/venv/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/dhkdn9192/venv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "# 모델 하이퍼파라미터 로드\n",
    "model_json_path = os.path.join(config.MODEL_DIR, 'amazonreviews/model_06.json')\n",
    "amazon_review_model = DocumentModel.load_model(model_json_path)\n",
    "\n",
    "# 모델 가중치 로드\n",
    "model_hdf5_path = os.path.join(config.MODEL_DIR, 'amazonreviews/model_06.hdf5')\n",
    "amazon_review_model.load_model_weights(model_hdf5_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learned_embeddings size : 43197\n",
      "4839 words are updated out of 4917\n"
     ]
    }
   ],
   "source": [
    "# 모델 임베딩 레이어 추출\n",
    "learned_embeddings = amazon_review_model.get_classification_model().get_layer('imdb_embedding').get_weights()[0]\n",
    "print(f'learned_embeddings size : {len(learned_embeddings)}')\n",
    "\n",
    "# 기존 GloVe 모델을 학습된 임베딩 행렬로 업데이트한다\n",
    "glove.update_embeddings(preprocessor.word_index, \n",
    "                        np.array(learned_embeddings), \n",
    "                        amazon_review_model.word_index)\n",
    "\n",
    "# 업데이트된 임베딩을 얻는다\n",
    "initial_embeddings = glove.get_embedding(preprocessor.word_index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  IMDB 전이학습 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size = 4919  and the index of vocabulary words passed has 4917 words\n"
     ]
    }
   ],
   "source": [
    "# 분류 모델 생성 : IMDB 리뷰 데이터를 입력받아 이진분류를 수행하는 모델 생성\n",
    "imdb_model = DocumentModel(vocab_size=preprocessor.get_vocab_size(),\n",
    "                           word_index = preprocessor.word_index,\n",
    "                           num_sentences=Preprocess.NUM_SENTENCES,     \n",
    "                           embedding_weights=initial_embeddings,\n",
    "                           embedding_regularizer_l2 = 0.0,\n",
    "                           conv_activation = 'tanh',\n",
    "                           train_embedding = False,  # 임베딩 레이어의 가중치 학습 안 함\n",
    "                           learn_word_conv = False,  # 단어 수준 conv 레이어의 가중치 학습 안 함\n",
    "                           learn_sent_conv = False,  # 문장 수준 conv 레이어의 가중치 학습 안 함\n",
    "                           hidden_dims=64,                                        \n",
    "                           input_dropout=0.1, \n",
    "                           hidden_layer_kernel_regularizer=0.01,\n",
    "                           final_layer_kernel_regularizer=0.01)\n",
    "\n",
    "# 가중치 업데이트 : 생성한 imdb_model 모델에서 다음의 각 레이어들의 가중치를 위에서 로드한 가중치로 갱신한다\n",
    "for l_name in ['word_conv','sentence_conv','hidden_0', 'final']:\n",
    "    new_weights = amazon_review_model.get_classification_model().get_layer(l_name).get_weights()\n",
    "    imdb_model.get_classification_model().get_layer(l_name).set_weights(weights=new_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 학습 및 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/dhkdn9192/venv/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 1237 samples, validate on 13 samples\n",
      "Epoch 1/30\n",
      " - 1s - loss: 1.5930 - acc: 0.8424 - val_loss: 1.5610 - val_acc: 0.8462\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.56104, saving model to ./checkpoint/imdb/transfer_model_10.hdf5\n",
      "Epoch 2/30\n",
      " - 0s - loss: 1.4717 - acc: 0.8383 - val_loss: 1.4601 - val_acc: 0.8462\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.56104 to 1.46013, saving model to ./checkpoint/imdb/transfer_model_10.hdf5\n",
      "Epoch 3/30\n",
      " - 0s - loss: 1.3991 - acc: 0.8448 - val_loss: 1.3791 - val_acc: 0.8462\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.46013 to 1.37906, saving model to ./checkpoint/imdb/transfer_model_10.hdf5\n",
      "Epoch 4/30\n",
      " - 0s - loss: 1.3127 - acc: 0.8472 - val_loss: 1.3162 - val_acc: 0.8462\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.37906 to 1.31624, saving model to ./checkpoint/imdb/transfer_model_10.hdf5\n",
      "Epoch 5/30\n",
      " - 0s - loss: 1.2514 - acc: 0.8529 - val_loss: 1.2432 - val_acc: 0.9231\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.31624 to 1.24319, saving model to ./checkpoint/imdb/transfer_model_10.hdf5\n",
      "Epoch 6/30\n",
      " - 0s - loss: 1.1924 - acc: 0.8399 - val_loss: 1.1831 - val_acc: 0.9231\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.24319 to 1.18306, saving model to ./checkpoint/imdb/transfer_model_10.hdf5\n",
      "Epoch 7/30\n",
      " - 0s - loss: 1.1394 - acc: 0.8399 - val_loss: 1.1294 - val_acc: 0.8462\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.18306 to 1.12943, saving model to ./checkpoint/imdb/transfer_model_10.hdf5\n",
      "Epoch 8/30\n",
      " - 0s - loss: 1.0839 - acc: 0.8472 - val_loss: 1.0796 - val_acc: 0.8462\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.12943 to 1.07955, saving model to ./checkpoint/imdb/transfer_model_10.hdf5\n",
      "Epoch 9/30\n",
      " - 0s - loss: 1.0411 - acc: 0.8383 - val_loss: 1.0332 - val_acc: 0.8462\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.07955 to 1.03325, saving model to ./checkpoint/imdb/transfer_model_10.hdf5\n",
      "Epoch 10/30\n",
      " - 0s - loss: 0.9815 - acc: 0.8416 - val_loss: 0.9908 - val_acc: 0.8462\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.03325 to 0.99080, saving model to ./checkpoint/imdb/transfer_model_10.hdf5\n",
      "Epoch 11/30\n",
      " - 0s - loss: 0.9516 - acc: 0.8464 - val_loss: 0.9503 - val_acc: 0.8462\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.99080 to 0.95029, saving model to ./checkpoint/imdb/transfer_model_10.hdf5\n",
      "Epoch 12/30\n",
      " - 0s - loss: 0.8993 - acc: 0.8480 - val_loss: 0.9129 - val_acc: 0.8462\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.95029 to 0.91286, saving model to ./checkpoint/imdb/transfer_model_10.hdf5\n",
      "Epoch 13/30\n",
      " - 0s - loss: 0.8652 - acc: 0.8399 - val_loss: 0.8756 - val_acc: 0.8462\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.91286 to 0.87559, saving model to ./checkpoint/imdb/transfer_model_10.hdf5\n",
      "Epoch 14/30\n",
      " - 0s - loss: 0.8285 - acc: 0.8464 - val_loss: 0.8505 - val_acc: 0.8462\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.87559 to 0.85054, saving model to ./checkpoint/imdb/transfer_model_10.hdf5\n",
      "Epoch 15/30\n",
      " - 0s - loss: 0.7963 - acc: 0.8537 - val_loss: 0.8168 - val_acc: 0.8462\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.85054 to 0.81679, saving model to ./checkpoint/imdb/transfer_model_10.hdf5\n",
      "Epoch 16/30\n",
      " - 0s - loss: 0.7713 - acc: 0.8488 - val_loss: 0.7899 - val_acc: 0.8462\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.81679 to 0.78991, saving model to ./checkpoint/imdb/transfer_model_10.hdf5\n",
      "Epoch 17/30\n",
      " - 0s - loss: 0.7409 - acc: 0.8432 - val_loss: 0.7597 - val_acc: 0.9231\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.78991 to 0.75969, saving model to ./checkpoint/imdb/transfer_model_10.hdf5\n",
      "Epoch 18/30\n",
      " - 0s - loss: 0.7151 - acc: 0.8464 - val_loss: 0.7394 - val_acc: 0.8462\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.75969 to 0.73940, saving model to ./checkpoint/imdb/transfer_model_10.hdf5\n",
      "Epoch 19/30\n",
      " - 0s - loss: 0.6983 - acc: 0.8432 - val_loss: 0.7237 - val_acc: 0.8462\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.73940 to 0.72375, saving model to ./checkpoint/imdb/transfer_model_10.hdf5\n",
      "Epoch 20/30\n",
      " - 0s - loss: 0.6665 - acc: 0.8504 - val_loss: 0.6984 - val_acc: 0.8462\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.72375 to 0.69838, saving model to ./checkpoint/imdb/transfer_model_10.hdf5\n",
      "Epoch 21/30\n",
      " - 0s - loss: 0.6541 - acc: 0.8488 - val_loss: 0.6704 - val_acc: 0.8462\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.69838 to 0.67045, saving model to ./checkpoint/imdb/transfer_model_10.hdf5\n",
      "Epoch 22/30\n",
      " - 0s - loss: 0.6335 - acc: 0.8464 - val_loss: 0.6611 - val_acc: 0.8462\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.67045 to 0.66115, saving model to ./checkpoint/imdb/transfer_model_10.hdf5\n",
      "Epoch 23/30\n",
      " - 0s - loss: 0.6220 - acc: 0.8472 - val_loss: 0.6340 - val_acc: 0.9231\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.66115 to 0.63402, saving model to ./checkpoint/imdb/transfer_model_10.hdf5\n",
      "Epoch 24/30\n",
      " - 0s - loss: 0.6053 - acc: 0.8480 - val_loss: 0.6251 - val_acc: 0.9231\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.63402 to 0.62507, saving model to ./checkpoint/imdb/transfer_model_10.hdf5\n",
      "Epoch 25/30\n",
      " - 0s - loss: 0.5890 - acc: 0.8432 - val_loss: 0.5998 - val_acc: 0.9231\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.62507 to 0.59981, saving model to ./checkpoint/imdb/transfer_model_10.hdf5\n",
      "Epoch 26/30\n",
      " - 0s - loss: 0.5709 - acc: 0.8521 - val_loss: 0.5839 - val_acc: 0.9231\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.59981 to 0.58385, saving model to ./checkpoint/imdb/transfer_model_10.hdf5\n",
      "Epoch 27/30\n",
      " - 0s - loss: 0.5608 - acc: 0.8513 - val_loss: 0.5805 - val_acc: 0.9231\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.58385 to 0.58054, saving model to ./checkpoint/imdb/transfer_model_10.hdf5\n",
      "Epoch 28/30\n",
      " - 0s - loss: 0.5523 - acc: 0.8448 - val_loss: 0.5595 - val_acc: 0.9231\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.58054 to 0.55952, saving model to ./checkpoint/imdb/transfer_model_10.hdf5\n",
      "Epoch 29/30\n",
      " - 0s - loss: 0.5375 - acc: 0.8488 - val_loss: 0.5501 - val_acc: 0.9231\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.55952 to 0.55014, saving model to ./checkpoint/imdb/transfer_model_10.hdf5\n",
      "Epoch 30/30\n",
      " - 0s - loss: 0.5261 - acc: 0.8513 - val_loss: 0.5439 - val_acc: 0.9231\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.55014 to 0.54392, saving model to ./checkpoint/imdb/transfer_model_10.hdf5\n"
     ]
    }
   ],
   "source": [
    "# 모델 컴파일              \n",
    "imdb_model.get_classification_model().compile(loss=\"binary_crossentropy\", \n",
    "                                              optimizer='rmsprop',\n",
    "                                              metrics=[\"accuracy\"])\n",
    "\n",
    "# callback (1) - 체크포인트\n",
    "checkpointer = ModelCheckpoint(filepath=train_params.model_file_path,\n",
    "                                verbose=1,\n",
    "                                save_best_only=True,\n",
    "                                save_weights_only=True)\n",
    "\n",
    "# callback (2) - 조기종료\n",
    "early_stop = EarlyStopping(patience=2)\n",
    "\n",
    "# 학습 시작\n",
    "imdb_model.get_classification_model().fit(x_train, \n",
    "                                          y_train, \n",
    "                                          batch_size=train_params.batch_size,\n",
    "                                          epochs=train_params.num_epochs,\n",
    "                                          verbose=2,\n",
    "                                          validation_split=0.01,\n",
    "                                          callbacks=[checkpointer])\n",
    "\n",
    "# 모델 저장\n",
    "imdb_model._save_model(train_params.model_hyper_parameters)\n",
    "train_params.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5280678803443909, 0.8523999965667725]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 평가\n",
    "imdb_model.get_classification_model().evaluate(x_test, \n",
    "                                               y_test, \n",
    "                                               batch_size=train_params.batch_size*10,\n",
    "                                               verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM과의 성능 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report,accuracy_score,confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature names size : 20340\n"
     ]
    }
   ],
   "source": [
    "# build TFIDF features on train reviews\n",
    "tv = TfidfVectorizer(use_idf=True, \n",
    "                     min_df=0.00005, \n",
    "                     max_df=1.0, \n",
    "                     ngram_range=(1, 1), \n",
    "                     stop_words='english', \n",
    "                     sublinear_tf=True)\n",
    "\n",
    "tv_features = tv.fit_transform(train_df['review'].tolist())\n",
    "print(f'feature names size : {len(tv.get_feature_names())}')\n",
    "\n",
    "tv_train_features = tv.transform(corpus)\n",
    "tv_test_features = tv.transform(test_corpus)\n",
    "\n",
    "# SVC 모델 학습 & 평가\n",
    "clf = SVC(C=1,kernel='linear', random_state=1, gamma=0.01)\n",
    "svm = clf.fit(tv_train_features, target)\n",
    "preds_test = svm.predict(tv_test_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.83128\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test, preds_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
