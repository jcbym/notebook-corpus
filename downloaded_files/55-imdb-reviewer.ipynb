{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Model Training 2.0.ipynb","provenance":[{"file_id":"1Sc-NpL22evM4MYXrzfUJLIw6CYipCW91","timestamp":1617363250819},{"file_id":"1wpxtqalndUT8hQORavbefedocHZn_f90","timestamp":1616840050062},{"file_id":"14UgGueQBRz-g3KZa3u4On96hk3G5H0i0","timestamp":1614053223736}],"collapsed_sections":[],"toc_visible":true,"mount_file_id":"1WfmlzHXN-Pw_bwLVShjXN0GRAHPW_WPn","authorship_tag":"ABX9TyOPGtWx2vixQUiWLDOggU5C"},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"HdrydlTEeYw7"},"source":["#Loading the required libraries"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"InDakrB_7_Ao","executionInfo":{"status":"ok","timestamp":1619979907420,"user_tz":-330,"elapsed":74076,"user":{"displayName":"Shreyansh Bardia","photoUrl":"","userId":"12020969166647775692"}},"outputId":"f31d9405-4c5c-45f8-e5f2-3bdd26b32fab"},"source":["!pip install selenium\n","!apt-get update\n","!apt install chromium-chromedriver\n","!pip install pattern\n","!pip install asteval"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting selenium\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/d6/4294f0b4bce4de0abf13e17190289f9d0613b0a44e5dd6a7f5ca98459853/selenium-3.141.0-py2.py3-none-any.whl (904kB)\n","\u001b[K     |████████████████████████████████| 911kB 5.6MB/s \n","\u001b[?25hRequirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from selenium) (1.24.3)\n","Installing collected packages: selenium\n","Successfully installed selenium-3.141.0\n","Get:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n","Ign:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n","Get:3 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n","Ign:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n","Get:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release [697 B]\n","Hit:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n","Get:7 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n","Get:8 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release.gpg [836 B]\n","Hit:10 http://archive.ubuntu.com/ubuntu bionic InRelease\n","Get:11 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ Packages [53.9 kB]\n","Get:12 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n","Ign:13 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages\n","Get:13 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [742 kB]\n","Hit:14 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n","Hit:15 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n","Get:16 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [396 kB]\n","Get:17 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n","Get:18 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease [15.9 kB]\n","Get:19 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [31.6 kB]\n","Get:20 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,181 kB]\n","Get:21 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [24.7 kB]\n","Get:22 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,116 kB]\n","Get:23 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,759 kB]\n","Get:24 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,410 kB]\n","Get:25 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,546 kB]\n","Get:26 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [426 kB]\n","Get:27 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [900 kB]\n","Get:28 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic/main amd64 Packages [53.2 kB]\n","Fetched 12.9 MB in 7s (1,776 kB/s)\n","Reading package lists... Done\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","The following package was automatically installed and is no longer required:\n","  libnvidia-common-460\n","Use 'apt autoremove' to remove it.\n","The following additional packages will be installed:\n","  chromium-browser chromium-browser-l10n chromium-codecs-ffmpeg-extra\n","Suggested packages:\n","  webaccounts-chromium-extension unity-chromium-extension\n","The following NEW packages will be installed:\n","  chromium-browser chromium-browser-l10n chromium-chromedriver\n","  chromium-codecs-ffmpeg-extra\n","0 upgraded, 4 newly installed, 0 to remove and 69 not upgraded.\n","Need to get 86.6 MB of archives.\n","After this operation, 300 MB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-codecs-ffmpeg-extra amd64 90.0.4430.72-0ubuntu0.18.04.1 [1,128 kB]\n","Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser amd64 90.0.4430.72-0ubuntu0.18.04.1 [76.9 MB]\n","Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser-l10n all 90.0.4430.72-0ubuntu0.18.04.1 [3,858 kB]\n","Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-chromedriver amd64 90.0.4430.72-0ubuntu0.18.04.1 [4,743 kB]\n","Fetched 86.6 MB in 10s (9,048 kB/s)\n","Selecting previously unselected package chromium-codecs-ffmpeg-extra.\n","(Reading database ... 160690 files and directories currently installed.)\n","Preparing to unpack .../chromium-codecs-ffmpeg-extra_90.0.4430.72-0ubuntu0.18.04.1_amd64.deb ...\n","Unpacking chromium-codecs-ffmpeg-extra (90.0.4430.72-0ubuntu0.18.04.1) ...\n","Selecting previously unselected package chromium-browser.\n","Preparing to unpack .../chromium-browser_90.0.4430.72-0ubuntu0.18.04.1_amd64.deb ...\n","Unpacking chromium-browser (90.0.4430.72-0ubuntu0.18.04.1) ...\n","Selecting previously unselected package chromium-browser-l10n.\n","Preparing to unpack .../chromium-browser-l10n_90.0.4430.72-0ubuntu0.18.04.1_all.deb ...\n","Unpacking chromium-browser-l10n (90.0.4430.72-0ubuntu0.18.04.1) ...\n","Selecting previously unselected package chromium-chromedriver.\n","Preparing to unpack .../chromium-chromedriver_90.0.4430.72-0ubuntu0.18.04.1_amd64.deb ...\n","Unpacking chromium-chromedriver (90.0.4430.72-0ubuntu0.18.04.1) ...\n","Setting up chromium-codecs-ffmpeg-extra (90.0.4430.72-0ubuntu0.18.04.1) ...\n","Setting up chromium-browser (90.0.4430.72-0ubuntu0.18.04.1) ...\n","update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n","update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n","Setting up chromium-chromedriver (90.0.4430.72-0ubuntu0.18.04.1) ...\n","Setting up chromium-browser-l10n (90.0.4430.72-0ubuntu0.18.04.1) ...\n","Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n","Processing triggers for hicolor-icon-theme (0.17-2) ...\n","Processing triggers for mime-support (3.60ubuntu1) ...\n","Processing triggers for libc-bin (2.27-3ubuntu1.2) ...\n","/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n","\n","Collecting pattern\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/07/b0e61b6c818ed4b6145fe01d1c341223aa6cfbc3928538ad1f2b890924a3/Pattern-3.6.0.tar.gz (22.2MB)\n","\u001b[K     |████████████████████████████████| 22.3MB 7.2MB/s \n","\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pattern) (0.16.0)\n","Collecting backports.csv\n","  Downloading https://files.pythonhosted.org/packages/8e/26/a6bd68f13e0f38fbb643d6e497fc3462be83a0b6c4d43425c78bb51a7291/backports.csv-1.0.7-py2.py3-none-any.whl\n","Collecting mysqlclient\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/df/59cd2fa5e48d0804d213bdcb1acb4d08c403b61c7ff7ed4dd4a6a2deb3f7/mysqlclient-2.0.3.tar.gz (88kB)\n","\u001b[K     |████████████████████████████████| 92kB 9.7MB/s \n","\u001b[?25hRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from pattern) (4.6.3)\n","Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from pattern) (4.2.6)\n","Collecting feedparser\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1c/21/faf1bac028662cc8adb2b5ef7a6f3999a765baa2835331df365289b0ca56/feedparser-6.0.2-py3-none-any.whl (80kB)\n","\u001b[K     |████████████████████████████████| 81kB 8.9MB/s \n","\u001b[?25hCollecting pdfminer.six\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/93/f3/4fec7dabe8802ebec46141345bf714cd1fc7d93cb74ddde917e4b6d97d88/pdfminer.six-20201018-py3-none-any.whl (5.6MB)\n","\u001b[K     |████████████████████████████████| 5.6MB 44.9MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pattern) (1.19.5)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from pattern) (1.4.1)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from pattern) (3.2.5)\n","Collecting python-docx\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e4/83/c66a1934ed5ed8ab1dbb9931f1779079f8bca0f6bbc5793c06c4b5e7d671/python-docx-0.8.10.tar.gz (5.5MB)\n","\u001b[K     |████████████████████████████████| 5.5MB 42.5MB/s \n","\u001b[?25hCollecting cherrypy\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/f9/e11f893dcabe6bc222a1442bf5e14f0322a2d363c92910ed41947078a35a/CherryPy-18.6.0-py2.py3-none-any.whl (419kB)\n","\u001b[K     |████████████████████████████████| 419kB 43.9MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pattern) (2.23.0)\n","Collecting sgmllib3k\n","  Downloading https://files.pythonhosted.org/packages/9e/bd/3704a8c3e0942d711c1299ebf7b9091930adae6675d7c8f476a7ce48653c/sgmllib3k-1.0.0.tar.gz\n","Collecting cryptography\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b2/26/7af637e6a7e87258b963f1731c5982fb31cd507f0d90d91836e446955d02/cryptography-3.4.7-cp36-abi3-manylinux2014_x86_64.whl (3.2MB)\n","\u001b[K     |████████████████████████████████| 3.2MB 39.7MB/s \n","\u001b[?25hRequirement already satisfied: sortedcontainers in /usr/local/lib/python3.7/dist-packages (from pdfminer.six->pattern) (2.3.0)\n","Requirement already satisfied: chardet; python_version > \"3.0\" in /usr/local/lib/python3.7/dist-packages (from pdfminer.six->pattern) (3.0.4)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->pattern) (1.15.0)\n","Collecting portend>=2.1.1\n","  Downloading https://files.pythonhosted.org/packages/b8/a1/fd29409cced540facdd29abb986d988cb1f22c8170d10022ea73af77fa55/portend-2.7.1-py3-none-any.whl\n","Collecting zc.lockfile\n","  Downloading https://files.pythonhosted.org/packages/6c/2a/268389776288f0f26c7272c70c36c96dcc0bdb88ab6216ea18e19df1fadd/zc.lockfile-2.0-py2.py3-none-any.whl\n","Collecting jaraco.collections\n","  Downloading https://files.pythonhosted.org/packages/d5/1a/a0d6861d2aca6df92643c755966c8a60e40353e4c5e7a5c2f4e5ed733817/jaraco.collections-3.3.0-py3-none-any.whl\n","Collecting cheroot>=8.2.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/46/95/86fe6480af78fea7b0e7e1bf02e6acd4cb9e561ea200bd6d6e1398fe5426/cheroot-8.5.2-py2.py3-none-any.whl (97kB)\n","\u001b[K     |████████████████████████████████| 102kB 10.4MB/s \n","\u001b[?25hRequirement already satisfied: more-itertools in /usr/local/lib/python3.7/dist-packages (from cherrypy->pattern) (8.7.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pattern) (2020.12.5)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->pattern) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pattern) (2.10)\n","Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography->pdfminer.six->pattern) (1.14.5)\n","Collecting tempora>=1.8\n","  Downloading https://files.pythonhosted.org/packages/44/83/4d5c3de53bbc463f30ab6764e27bc2e8ed9b59736e8b40d95403ff802008/tempora-4.0.2-py3-none-any.whl\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from zc.lockfile->cherrypy->pattern) (56.0.0)\n","Collecting jaraco.text\n","  Downloading https://files.pythonhosted.org/packages/c1/74/2a3c4835c079df16db8a9c50263eebb0125849fee5b16de353a059b7545d/jaraco.text-3.5.0-py3-none-any.whl\n","Collecting jaraco.classes\n","  Downloading https://files.pythonhosted.org/packages/b8/74/bee5fc11594974746535117546404678fc7b899476e769c3c55bc0cfaa02/jaraco.classes-3.2.1-py3-none-any.whl\n","Collecting jaraco.functools\n","  Downloading https://files.pythonhosted.org/packages/b5/da/e51e7b58c8fe132990edd1e3ef25bcd9801eb7f91d0f642ac7f8d97e4a36/jaraco.functools-3.3.0-py3-none-any.whl\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography->pdfminer.six->pattern) (2.20)\n","Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from tempora>=1.8->portend>=2.1.1->cherrypy->pattern) (2018.9)\n","Building wheels for collected packages: pattern, mysqlclient, python-docx, sgmllib3k\n","  Building wheel for pattern (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pattern: filename=Pattern-3.6-cp37-none-any.whl size=22332724 sha256=4c397786f0b5b5f69ec5030dd1fab100ad0680491d231ee2a0f622a1b677dd06\n","  Stored in directory: /root/.cache/pip/wheels/dc/9a/0e/5fb1a603ed4e3aa8722a88e9cf4a82da7d1b63e3d2cc34bee5\n","  Building wheel for mysqlclient (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for mysqlclient: filename=mysqlclient-2.0.3-cp37-cp37m-linux_x86_64.whl size=100103 sha256=0f514785b186d41445164cc35d59e24101005ca95b2224c69d2e56d858bbd75a\n","  Stored in directory: /root/.cache/pip/wheels/75/ca/e8/ad4e7ce3df18bcd91c7d84dd28c7c08db491a2a2360efed363\n","  Building wheel for python-docx (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for python-docx: filename=python_docx-0.8.10-cp37-none-any.whl size=184491 sha256=b6f2c50755299ad971195479dd868a2baeb5dbb5bca0ee19ad79643e7d8888ab\n","  Stored in directory: /root/.cache/pip/wheels/18/0b/a0/1dd62ff812c857c9e487f27d80d53d2b40531bec1acecfa47b\n","  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-cp37-none-any.whl size=6067 sha256=aba3805ce2a6102d107fe9da9a372ebcd10e9cd9698553e8beb906217f1a95e2\n","  Stored in directory: /root/.cache/pip/wheels/f1/80/5a/444ba08a550cdd241bd9baf8bae44be750efe370adb944506a\n","Successfully built pattern mysqlclient python-docx sgmllib3k\n","Installing collected packages: backports.csv, mysqlclient, sgmllib3k, feedparser, cryptography, pdfminer.six, python-docx, jaraco.functools, tempora, portend, zc.lockfile, jaraco.text, jaraco.classes, jaraco.collections, cheroot, cherrypy, pattern\n","Successfully installed backports.csv-1.0.7 cheroot-8.5.2 cherrypy-18.6.0 cryptography-3.4.7 feedparser-6.0.2 jaraco.classes-3.2.1 jaraco.collections-3.3.0 jaraco.functools-3.3.0 jaraco.text-3.5.0 mysqlclient-2.0.3 pattern-3.6 pdfminer.six-20201018 portend-2.7.1 python-docx-0.8.10 sgmllib3k-1.0.0 tempora-4.0.2 zc.lockfile-2.0\n","Collecting asteval\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/06/efc5ea08dc44836abbabbd949d944d6fde08312ce756557e16ab1ca477a4/asteval-0.9.23.tar.gz (55kB)\n","\u001b[K     |████████████████████████████████| 61kB 3.2MB/s \n","\u001b[?25hBuilding wheels for collected packages: asteval\n","  Building wheel for asteval (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for asteval: filename=asteval-0.9.23-cp37-none-any.whl size=17419 sha256=9d2a92199af4b2e77a4cedd6481e1aa4fd52228ae2617a1d66e8319c7b1cf44e\n","  Stored in directory: /root/.cache/pip/wheels/56/d6/43/4b5d8a9fd98b976031768673658c8b8d08dc9dcc0fa1907ad0\n","Successfully built asteval\n","Installing collected packages: asteval\n","Successfully installed asteval-0.9.23\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"JaF9Tc3f_9Vb","executionInfo":{"status":"ok","timestamp":1619979907420,"user_tz":-330,"elapsed":74069,"user":{"displayName":"Shreyansh Bardia","photoUrl":"","userId":"12020969166647775692"}}},"source":["import selenium\n","import time\n","import pandas as pd\n","from tqdm import tqdm"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hRGOmltO8FYB","executionInfo":{"status":"ok","timestamp":1619979916363,"user_tz":-330,"elapsed":83008,"user":{"displayName":"Shreyansh Bardia","photoUrl":"","userId":"12020969166647775692"}},"outputId":"2753dad3-535b-4c5c-9c20-f71f622ec716"},"source":["# Creating Driver Instance\n","# install chromium, its driver, and selenium\n","!apt-get update\n","!apt install chromium-chromedriver\n","!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n","!pip install selenium\n","# set options to be headless, ..\n","from selenium import webdriver\n","options = webdriver.ChromeOptions()\n","options.add_argument('--headless')\n","options.add_argument('--no-sandbox')\n","options.add_argument('--disable-dev-shm-usage')\n","# open it, go to a website, and get results"],"execution_count":4,"outputs":[{"output_type":"stream","text":["\r0% [Working]\r            \rHit:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n","Ign:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n","Ign:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n","Hit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n","Hit:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n","Hit:6 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n","Get:7 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n","Hit:8 http://archive.ubuntu.com/ubuntu bionic InRelease\n","Hit:10 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n","Hit:12 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n","Hit:13 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n","Hit:14 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n","Hit:15 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n","Fetched 88.7 kB in 2s (36.4 kB/s)\n","Reading package lists... Done\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","chromium-chromedriver is already the newest version (90.0.4430.72-0ubuntu0.18.04.1).\n","The following package was automatically installed and is no longer required:\n","  libnvidia-common-460\n","Use 'apt autoremove' to remove it.\n","0 upgraded, 0 newly installed, 0 to remove and 69 not upgraded.\n","cp: '/usr/lib/chromium-browser/chromedriver' and '/usr/bin/chromedriver' are the same file\n","Requirement already satisfied: selenium in /usr/local/lib/python3.7/dist-packages (3.141.0)\n","Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from selenium) (1.24.3)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"zltTYl58lzBr"},"source":["# Loading the movies dataset"]},{"cell_type":"code","metadata":{"id":"q6fl81YhdqJb","executionInfo":{"status":"ok","timestamp":1619979916363,"user_tz":-330,"elapsed":83003,"user":{"displayName":"Shreyansh Bardia","photoUrl":"","userId":"12020969166647775692"}}},"source":["def load_data():\n","  import pandas as pd\n","  global movie\n","  global link\n","  movie = pd.read_csv('/content/drive/MyDrive/MovieLens Dataset(1)/movie.csv')\n","  link = pd.read_csv('/content/drive/MyDrive/MovieLens Dataset(1)/link.csv')\n","  global movie_links\n","  movie_links = pd.concat([movie,link],axis=1,join='inner')\n","  movie_links.drop('tmdbId',axis=1,inplace=True)\n","  df = movie_links['title'].str.split('(',expand=True)\n","  df.rename({0:'title',1:'year'},axis=1,inplace=True)\n","  df = df[['title','year']]\n","  df['year'] = df['year'].str.replace(')','')\n","  movie_links['year'] = df['year']\n","  movie_links['title'] = df['title']\n","  movie_links.index = movie_links['title']\n","\n","\n"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3UAt6K-itaF9"},"source":["#Scraping the Data"]},{"cell_type":"code","metadata":{"id":"I5184YGE3qDu","executionInfo":{"status":"ok","timestamp":1619979916364,"user_tz":-330,"elapsed":83000,"user":{"displayName":"Shreyansh Bardia","photoUrl":"","userId":"12020969166647775692"}}},"source":["def extend_page(wd):\n","  count = 20\n","  from selenium.common.exceptions import NoSuchElementException,ElementNotInteractableException\n","  # rev1 = []\n","  # rev1 = (wd.find_elements_by_xpath(\"//div[@class='text show-more__control clickable']\"))\n","  # headings = []\n","  # headers = wd.find_elements_by_class_name('title')\n","  # headings.extend(headers)\n","  prev_height = wd.execute_script('return document.body.scrollHeight')\n","\n","\n","  while(count>0):\n","    # wd.find_element_by_id(\"load-more-trigger\").click()\n","    # print(count)\n","    try:\n","      # print('In the try block')\n","      # button = wd.find_element_by_class_name('ipl-load-more__button')\n","      # button = wd.find_element_by_class_name(\"ipl-load-more__button\")\n","\n","      # print('button length:',len(button))\n","\n","      wd.find_element_by_id(\"load-more-trigger\").click()\n","      print(count)\n","      # driver.find_element_by_id(\"load-more-trigger\").click()\n","      # button.click()\n","\n","    except (ElementNotInteractableException,NoSuchElementException):\n","      # print('Click event not instantiated')\n","      page = wd.page_source\n","      return page\n","    # print(page[:10])\n","      # reviews = scrape_reviews(page)\n","      # if(len(reviews)>=200):\n","      #   return page\n","\n","      # else:\n","      #   return 0\n","    time.sleep(1)\n","\n","    wd.execute_script('window.scrollTo(0,document.body.scrollHeight);')\n","    # time.sleep(1)\n","    new_height = wd.execute_script('return document.body.scrollHeight')\n","\n","    # if new_height == prev_height:\n","    #   print('Height limit reached')\n","    #   break\n","    # headers = wd.find_elements_by_class_name('title')\n","    # print(headers[-1].text)\n","    # headings.extend(headers)\n","    # print(headings[-1].text)\n","    prev_height = new_height\n","    count-=1\n","  page = wd.page_source\n","  return page\n"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"2S0kVp564H84","executionInfo":{"status":"ok","timestamp":1619979916364,"user_tz":-330,"elapsed":82997,"user":{"displayName":"Shreyansh Bardia","photoUrl":"","userId":"12020969166647775692"}}},"source":["def scrape_reviews(page):\n","  from bs4 import BeautifulSoup as bsoup\n","  import requests\n","  soup = bsoup(page,'lxml')\n","  revs = soup.find_all('div',{'class':'text show-more__control'})\n","  revs2 = soup.find_all('div',{'class':'text show-more__control clickable'})\n","\n","  reviews = []\n","  global k\n","  k=0\n","  for j,i in tqdm(enumerate(revs)):\n","    reviews.append(i.text)\n","    k+=1\n","\n","  for j,i in tqdm(enumerate(revs2)):\n","    reviews.append(i.text)\n","    k+=1\n","\n","  print(k,'reviews scraped')\n","\n","  return reviews"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zgFC5zLMq6Fe"},"source":["# Preprocessing"]},{"cell_type":"code","metadata":{"id":"6KTHBu2xAFo_","executionInfo":{"status":"ok","timestamp":1619979916365,"user_tz":-330,"elapsed":82995,"user":{"displayName":"Shreyansh Bardia","photoUrl":"","userId":"12020969166647775692"}}},"source":["def preprocess(reviews):\n","    import re\n","    import gensim\n","    from gensim.parsing.preprocessing import remove_stopwords\n","    from gensim.models.phrases import Phrases,Phraser\n","    from gensim.utils import lemmatize\n","    from nltk.stem import WordNetLemmatizer\n","    lemmatizer = WordNetLemmatizer()\n","    cleaned_reviews = []\n","    for rev in reviews:\n","        text = re.sub('[^a-zA-Z]',' ',rev)\n","        text = text.lower()\n","        text = re.sub('(\\\\d|\\\\W)+',' ',text)\n","        text = remove_stopwords(text)\n","        cleaned_reviews.append(text.split())\n","\n","    print('text has been cleaned')\n","\n","\n","#     print('cleaned_reviews:',cleaned_reviews)\n","\n","\n","\n","    lemmatized_reviews = []\n","\n","\n","#     lem_without_tags_reviews = []\n","\n","\n","    for review in cleaned_reviews:\n","        lemmatized_review = []\n","#         print('review:',review)\n","        for word in review:\n","#             print(word)\n","            lemmatized_review.append(lemmatizer.lemmatize(word))\n","#             print(lemmatized_review)\n","    #         lemmatized_reviews.append(lemmatize(review,allowed_tags=re.compile('.*')))\n","        lemmatized_reviews.append(lemmatized_review)\n","        \n","#     print(len(lemmatized_reviews))\n","#     print(lemmatized_reviews)\n","\n","    print('text has been lemmatized')\n","\n","\n","\n","\n","\n","\n","# #     lemmatized_words = []\n","# #     for review in lemmatized_reviews:\n","# #         line = []\n","# #         for word in review:\n","# # #     #           line.append(word.decode('utf-8'))\n","# #           line.append(word)\n","\n","\n","# #         lemmatized_words.append(line)\n","        \n","        \n","\n","\n","#     for lines in lemmatized_reviews:\n","#         line = []\n","\n","#         for word in lines:\n","# #             print('word:',word)\n","#             line.append(word.split('/')[0])\n","            \n","# #             print('line:',line)\n","\n","\n","#         lem_without_tags_reviews.append(line)\n","\n","\n","\n","    phrases = Phrases(lemmatized_reviews)\n","    print('bigram step 1')\n","    bigram = Phraser(phrases)\n","    print('bigram step 2')\n","\n","\n","    final_phrases = bigram[lemmatized_reviews]\n","\n","    print('bigrams have been created')\n","\n","    \n","    return final_phrases\n"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oz0btwl6l8Hj"},"source":["# Data Labelling"]},{"cell_type":"code","metadata":{"id":"-ajlYwMKmAEn","executionInfo":{"status":"ok","timestamp":1619979916365,"user_tz":-330,"elapsed":82992,"user":{"displayName":"Shreyansh Bardia","photoUrl":"","userId":"12020969166647775692"}}},"source":["def labelling(reviews):\n","  print('labelling')\n","  import nltk\n","  import pandas as pd\n","  # nltk.download('all')\n","  nltk.download('vader_lexicon')\n","\n","  from nltk.sentiment.vader import SentimentIntensityAnalyzer\n","\n","  sid = SentimentIntensityAnalyzer()\n","\n","  labels = []\n","\n","  for review in reviews:\n","\n","    score = sid.polarity_scores(review)\n","\n","    if (score['compound'] >= 0.05 ):\n","      labels.append(1)\n","\n","    elif (score['compound'] <= -0.05):\n","      labels.append(2)\n","\n","    else:\n","      labels.append(0)\n","\n","\n","  df = pd.DataFrame()\n","  print('reviews:',len(reviews))\n","  print('labels:',len(labels))\n","\n","\n","  df['reviews'] = reviews\n","  df['label'] = labels\n","\n","  return df\n","\n","\n","\n"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qJ7499z2u747"},"source":["# embedding"]},{"cell_type":"code","metadata":{"id":"h-boiuWAmFp9","executionInfo":{"status":"ok","timestamp":1619979916365,"user_tz":-330,"elapsed":82988,"user":{"displayName":"Shreyansh Bardia","photoUrl":"","userId":"12020969166647775692"}}},"source":["def embedding(all_phrases1):\n","  print('embedding')\n","  print('Shape of phrases',all_phrases1.shape)\n","  import gensim\n","  import numpy as np\n","  from gensim.models import Word2Vec as wvec\n","  from sklearn.decomposition import PCA\n","\n","  from asteval import Interpreter\n","  asteval2 = Interpreter()\n","\n","  to_be_vectorized = []\n","\n","  for rev in all_phrases1['cleaned_reviews']:\n","    to_be_vectorized.append(asteval2(rev))\n","\n","  all_phrases1['cleaned_reviews2'] = to_be_vectorized\n","\n","  cbow = wvec(min_count=10,window=5,size=all_phrases1.shape[0],alpha=0.01,min_alpha=0.001,sample=6e-5,negative = 10)\n","\n","\n","  dictionary = gensim.corpora.Dictionary(all_phrases1['cleaned_reviews2'])\n","\n","  model = wvec(min_count=10,window=5,size=all_phrases1.shape[0],alpha=0.01,min_alpha=0.001,sample=6e-5,negative = 10)\n","\n","  model.build_vocab(sentences=all_phrases1['cleaned_reviews2'])\n","\n","  model.train(all_phrases1,total_examples=cbow.corpus_count,epochs=200,)\n","\n","  vectors = model.wv.vectors\n","\n","  print(vectors.shape)\n","\n","  pca = PCA(n_components=100)\n","\n","  v2 = vectors.T\n","  # v2 = vectors\n","\n","\n","  print(v2.shape)\n","\n","\n","  v2 = pca.fit_transform(v2) \n","\n","  return v2\n","\n","# v2 = np.array(v2)\n","\n","\n"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"Zl3KHNchsAfs","executionInfo":{"status":"ok","timestamp":1619979916366,"user_tz":-330,"elapsed":82987,"user":{"displayName":"Shreyansh Bardia","photoUrl":"","userId":"12020969166647775692"}}},"source":[""],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F9slkTXyq9j2"},"source":["# Data splitting"]},{"cell_type":"code","metadata":{"id":"KZAXsf_prnxe","executionInfo":{"status":"ok","timestamp":1619979916366,"user_tz":-330,"elapsed":82984,"user":{"displayName":"Shreyansh Bardia","photoUrl":"","userId":"12020969166647775692"}}},"source":["def split_data(vectors,labels):\n","\n","  print('vectors:',vectors.shape)\n","  print('labels:',labels.shape)\n","  import numpy as np\n","  from keras.utils import to_categorical\n","  # df = phrases.sample(frac=1).reset_index(drop=True)\n","\n","  # seen_data = vectors[:round(df.shape[0]*0.9),:]\n","  # unseen_data = vectors[round(df.shape[0]*0.9):,:]\n","\n","  seen_vectors = vectors[:round(vectors.shape[0]*0.9)]\n","  seen_labels = labels[:round(vectors.shape[0]*0.9)]\n","  \n","\n","  unseen_vectors = vectors[round(vectors.shape[0]*0.9):]\n","  unseen_labels = labels[round(vectors.shape[0]*0.9):]\n","\n","\n","  print('seen_vectors shape:',seen_vectors.shape)\n","  print('seen_labels shape:',seen_labels.shape)\n","  print('unseen_vectors shape:',unseen_vectors.shape)\n","  print('unseen_labels shape:',unseen_labels.shape)\n","\n","\n","\n","  from sklearn.model_selection import train_test_split\n","\n","  x_train,x_test,y_train,y_test = train_test_split(vectors,labels,test_size=0.1,random_state=0)\n","  print('data split')\n","\n","  print('x_train shape',x_train.shape)\n","  print('y_train shape',y_train.shape)\n","  print('x_test shape',x_test.shape)\n","  print('y_test shape',y_test.shape)\n","\n"," \n","\n","  x_train = x_train.reshape((x_train.shape[0],x_train.shape[1],1))\n","  x_test = x_test.reshape((x_test.shape[0],x_test.shape[1],1))\n","\n","  y_train = np.array(y_train)\n","  y_test = np.array(y_test)\n","  # y_train = y_train.reshape((-1,1))\n","  # y_test = y_test.reshape((-1,1))\n","\n","  y_train = to_categorical(y_train)\n","  y_test = to_categorical(y_test)\n","\n","  model = training(x_train,y_train,x_test,y_test)\n","  # training(x_train,y_train)\n","\n","  return model\n","\n","\n","\n","\n"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FYfddTxwu-cz"},"source":["\n","\n","\n","# Training"]},{"cell_type":"code","metadata":{"id":"jTr62jJnrrRI","executionInfo":{"status":"ok","timestamp":1619979916367,"user_tz":-330,"elapsed":82982,"user":{"displayName":"Shreyansh Bardia","photoUrl":"","userId":"12020969166647775692"}}},"source":["def training(x_train,y_train,x_test,y_test): # Add unseen_vectors,unseen_labels\n","  import keras\n","  from keras.layers import LSTM,Dropout,Flatten,Dense,Bidirectional,GRU,SimpleRNN\n","  from keras.models import Sequential\n","  import numpy as np\n","  from keras.callbacks import ReduceLROnPlateau\n","  from keras.utils import to_categorical\n","  from sklearn.metrics import accuracy_score,confusion_matrix\n","\n","  print('x_train shape:',x_train.shape)\n","  print('y_train shape:',y_train.shape)\n","  print('x_test shape:',x_test.shape)\n","  print('y_test shape:',y_test.shape)\n","\n","\n","  weights = {1:10000,0:1000,2:100}\n","\n","  lstm = Sequential()\n","  lstm.add(Bidirectional(LSTM(32,input_shape=(x_train.shape[1],x_train.shape[2]),activation='tanh',return_sequences=True)))\n","  lstm.add(Dropout(0.2))\n","  # lstm.add(Bidirectional(LSTM(32,activation='tanh',return_sequences=True)))\n","  # lstm.add(Dropout(0.2))\n","\n","  # lstm.add(Bidirectional(LSTM(32,activation='tanh',return_sequences=True)))\n","  # lstm.add(Dropout(0.2))\n","  # lstm.add(Bidirectional(LSTM(32,activation='tanh',return_sequences=True)))\n","  lstm.add(GRU(128,return_sequences=True))\n","  # lstm.add(SimpleRNN(128,return_sequences=True))\n","\n","  lstm.add(GRU(256,return_sequences=True))\n","  lstm.add(GRU(512,return_sequences=True))\n","\n","  \n","\n","  lstm.add(Dropout(0.3))\n","  lstm.add(Flatten())\n","\n","  lstm.add(Dense(3,activation='softmax'))\n","  # lstm.add(Dense(1,activation='sigmoid'))\n","\n","\n","  # lstm.compile(optimizer='adagrad',loss='binary_crossentropy',metrics=['accuracy'])\n","  lstm.compile(optimizer=keras.optimizers.Adam(lr=0.005,clipnorm=1.0),loss='categorical_crossentropy',metrics=['accuracy'])\n","\n","\n","  reduce_lr = ReduceLROnPlateau(monitor='loss',factor=0.01,patience=5,min_lr=0.0001)\n","\n","\n","  # print(lstm.summary())\n","  lstm.fit(x_train,y_train,epochs=30,class_weight=weights,callbacks=[reduce_lr])\n","\n","  print('evaluating:')\n","\n","  # lstm.evaluate(x_test,y_test)\n","\n","  pred = lstm.predict(x_test)\n","\n","  # lstm.save('imdb_classifier.h5')\n","\n","  print('shape of pred:',pred.shape)\n","\n","\n","  # # pred = pred[0][:]\n","  print('0th index:',pred[0])\n","  print('max value at 0th index:',np.max(pred[0]))\n","\n","  # print('mean value:',np.mean(pred[0][:]))\n","\n","  revs = []\n","\n","\n","  for i in range(len(pred)):\n","    # ind = np.argmax(pred[i],axis=1)\n","    revs.append(np.max(pred[i]))\n","    # if((pred[i][0])>pred[i][1]):\n","    #   if(pred[i][0]>pred[i][2]):\n","    #     revs.append(0)\n","\n","    # elif(pred[i][1]>pred[i][2]):\n","    #   revs.append(1)\n","\n","    # else:\n","    #   revs.append(2)\n","\n","  #   else:\n","  #     revs.append(0)\n","\n","\n","  revs = np.array(revs)\n","\n","  revs = to_categorical(revs,3)\n","\n","  # revs = revs.reshape((-1,1))\n","\n","\n","  print('Accuracy on testing data:',accuracy_score(revs,y_test))\n","\n","  print('\\n')\n","\n","  print(confusion_matrix(y_test.argmax(axis=1),revs.argmax(axis=1)))\n","\n","  return lstm "],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"xhgJ0HSoPfN4","executionInfo":{"status":"ok","timestamp":1619979917086,"user_tz":-330,"elapsed":83699,"user":{"displayName":"Shreyansh Bardia","photoUrl":"","userId":"12020969166647775692"}}},"source":["def get_movie():\n","  load_data()\n","  movie = input('Enter the name of the movie whose reviews you want to see:')\n","  movie = movie.lower()\n","  # movie = 'tom horn'\n","  # info = selection\n","  info = movie.title()\n","  info = info+' '\n","  print(info)\n","\n","  if (info not in movie_links['title']):\n","    print('Movie not recognized')\n","    return 0,1\n","  id = str(movie_links.imdbId.loc[info])\n","  id = 'tt0'+id\n","  wd = webdriver.Chrome('chromedriver',options=options)\n","\n","  wd.get('https://www.imdb.com/title/{}/reviews?ref_=tt_urv'.format(id))\n","\n","  print('Extending the page...')\n","  \n","  page = extend_page(wd)\n","\n","  reviews = scrape_reviews(page)\n","\n","  if(len(reviews)>=200):\n","    print('Labelling the reviews...')\n","    phrases = labelling(reviews)\n","    print('preprocessing the reviews...')\n","    phrases['cleaned_reviews'] = preprocess(phrases['reviews'])\n","\n","    print('Embedding...')\n","\n","    vectors = embedding(phrases)\n","\n","    print('Data collection and preprocessing complete.')\n","\n","    # v2 = vectors[:100,:]\n","    # phrases = phrases.iloc[:100,:]\n","    # print('Shape of vectors',vectors.shape)\n","    print('Shape of phrases',phrases.shape)\n","\n","    return phrases,vectors\n","    # return phrases\n","\n","\n","  else:\n","    print('There are not enough reviews given for this movie.')\n","    return 0,1\n","    # return 0\n","\n","\n"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"XgUbpU7kQtM3","executionInfo":{"status":"ok","timestamp":1619979917087,"user_tz":-330,"elapsed":83698,"user":{"displayName":"Shreyansh Bardia","photoUrl":"","userId":"12020969166647775692"}}},"source":["# phrases,vectors = get_movie()"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"S9xIFNwRnF-e","executionInfo":{"status":"ok","timestamp":1619979917087,"user_tz":-330,"elapsed":83694,"user":{"displayName":"Shreyansh Bardia","photoUrl":"","userId":"12020969166647775692"}}},"source":["# l1 = list(phrases['label'])\n","# print(set(l1))"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"b6u5nj30XFXX","executionInfo":{"status":"ok","timestamp":1619979917087,"user_tz":-330,"elapsed":83692,"user":{"displayName":"Shreyansh Bardia","photoUrl":"","userId":"12020969166647775692"}}},"source":["# split_data(phrases,vectors)"],"execution_count":16,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zrWjXvdCFPCG"},"source":["# Data Collection"]},{"cell_type":"code","metadata":{"id":"ydX_7gWmlcYP","executionInfo":{"status":"ok","timestamp":1619979918369,"user_tz":-330,"elapsed":84973,"user":{"displayName":"Shreyansh Bardia","photoUrl":"","userId":"12020969166647775692"}}},"source":["load_data()"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"id":"LB2-eFi_rFBp","executionInfo":{"status":"ok","timestamp":1619979918370,"user_tz":-330,"elapsed":84973,"user":{"displayName":"Shreyansh Bardia","photoUrl":"","userId":"12020969166647775692"}}},"source":["def data_collection():\n","  import numpy as np\n","  import random\n","  import pandas as pd\n","\n","  global all_phrases\n","  global all_vectors\n","  global already_selected\n","  global phrases\n","  global vectors\n","\n","\n","  already_selected = []\n","  final_selection = []\n","  all_phrases = 0\n","\n","  # selection = 'John Wick '\n","\n","\n","  while (type(all_phrases)==int):\n","\n","    selection = random.choice(movie_links['title'])\n","    # all_phrases,all_vectors = get_movie(selection)\n","    all_phrases = get_movie(selection)\n","    print(all_phrases)\n","    already_selected.append(selection)\n","\n","\n","  # print('returned from get_movie')\n","\n","  # if (type(all_phrases)==int):\n","  #   # print('all_phrases is 0')\n","  #   all_phrases,all_vectors = data_collection()\n","\n","  # else:\n","\n","  final_selection.append(selection)\n","  print('Initial Shape of all_phrases:',all_phrases.shape)\n","  print('Initial Shape of all_vectors:',all_vectors.shape)\n","\n","    # count = 1\n","\n","  while(len(final_selection)!=100):\n","\n","    # all_phrases,all_vectors,already_selected,final_selection = data_collection_2(all_phrases,all_vectors,already_selected,final_selection)\n","    all_phrases,already_selected,final_selection = data_collection_2(all_phrases,already_selected,final_selection)\n","\n","\n","  print('all_phrases2:',all_phrases.shape)\n","  print('already_selected:',len(already_selected))\n","  print('LENGTH OF FINAL_SELECTION:',len(final_selection))\n","  print('movies selected:',final_selection)\n","\n","  # return all_phrases,all_vectors\n","  return all_phrases\n"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"id":"jIYv3j1X9LUF","executionInfo":{"status":"ok","timestamp":1619979918370,"user_tz":-330,"elapsed":84972,"user":{"displayName":"Shreyansh Bardia","photoUrl":"","userId":"12020969166647775692"}}},"source":["def data_collection_2(all_phrases,already_selected,final_selection):\n","\n","  import random\n","  import numpy as np\n","  import pandas as pd\n","\n","  # print('all_phrases type:',type(all_phrases))\n","  # print('all_vectors type:',type(all_vectors))\n","  # print('already_selected type:',type(already_selected))\n","\n","\n","  phrases = 0\n","\n","  print('In data_collection_2:')\n","    # selection = 'Iron Man '\n","\n","    # print('length of already_selected:',len(already_selected))\n","\n","\n","\n","\n","  while (type(phrases)==int):\n","    print('all_phrases is 0, continuing')\n","    selection = random.choice(movie_links['title'])\n","    if (selection not in already_selected):\n","      already_selected.append(selection)\n","      # phrases,vectors = get_movie(selection)\n","      phrases = get_movie(selection)\n","\n","\n","    # already_selected.append(selection)\n","    # phrases,vectors = get_movie(selection)\n","    # all_phrases,all_vectors,already_selected,final_selection = data_collection_2(all_phrases,all_vectors,already_selected,final_selection)\n","    \n","  final_selection.append(selection)\n","\n","  \n","  # final_selection.append(selection)\n","  print('ELngth of final_selection:',len(final_selection))\n","  print('Shape of phrases:',phrases.shape)\n","  # print('Shape of vectors:',all_vectors.shape)\n","  print('Shape of all_phrases:',all_phrases.shape)\n","  # print('Shape of all_vectors:',all_vectors.shape)\n","\n","  all_phrases = pd.concat([all_phrases,phrases],axis=0)\n","\n","  # all_vectors = np.append(all_vectors,vectors,axis=0)\n","\n","\n","      # count+=1\n","\n","  # return all_phrases,all_vectors,already_selected,final_selection\n","  return all_phrases,already_selected,final_selection\n","\n"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"id":"y-U31Kvq6Nh5","executionInfo":{"status":"ok","timestamp":1619979918370,"user_tz":-330,"elapsed":84970,"user":{"displayName":"Shreyansh Bardia","photoUrl":"","userId":"12020969166647775692"}}},"source":["# all_phrases1 = data_collection()\n"],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"id":"lc16vWbsvbJE","executionInfo":{"status":"ok","timestamp":1619979918371,"user_tz":-330,"elapsed":84970,"user":{"displayName":"Shreyansh Bardia","photoUrl":"","userId":"12020969166647775692"}}},"source":["# all_phrases1,all_vectors1 = data_collection()"],"execution_count":21,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UBaCtVafFVPh"},"source":["# Loading the scraped reviews"]},{"cell_type":"code","metadata":{"id":"nHcddhV3aurx","executionInfo":{"status":"ok","timestamp":1619979920147,"user_tz":-330,"elapsed":86741,"user":{"displayName":"Shreyansh Bardia","photoUrl":"","userId":"12020969166647775692"}}},"source":["import pandas as pd\n","\n","all_phrases1 = pd.read_csv('/content/drive/MyDrive/NLP Project/model_movies.csv')"],"execution_count":22,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":195},"id":"0yykGOR5Wrp4","executionInfo":{"status":"ok","timestamp":1619979920157,"user_tz":-330,"elapsed":86745,"user":{"displayName":"Shreyansh Bardia","photoUrl":"","userId":"12020969166647775692"}},"outputId":"07c74c86-d33a-4d98-be5e-ae5a2b7cb423"},"source":["all_phrases1.head()"],"execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>reviews</th>\n","      <th>label</th>\n","      <th>cleaned_reviews</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>First off, this is a film that is made with a ...</td>\n","      <td>1</td>\n","      <td>['film', 'lot', 'artistry', 'little', 'heart',...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>Casey Affleck (Ben Affleck's younger brother) ...</td>\n","      <td>1</td>\n","      <td>['casey_affleck', 'ben', 'affleck', 's', 'youn...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>I went to see The Killer Inside Me not really ...</td>\n","      <td>0</td>\n","      <td>['went', 'killer_inside', 'knowing', 'expect',...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>Lou Ford is a young small town deputy sheriff ...</td>\n","      <td>0</td>\n","      <td>['lou_ford', 'young', 'small_town', 'deputy_sh...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>The Killer Inside Me is tough. Directed by Mic...</td>\n","      <td>0</td>\n","      <td>['killer_inside', 'tough', 'directed', 'michae...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Unnamed: 0  ...                                    cleaned_reviews\n","0           0  ...  ['film', 'lot', 'artistry', 'little', 'heart',...\n","1           1  ...  ['casey_affleck', 'ben', 'affleck', 's', 'youn...\n","2           2  ...  ['went', 'killer_inside', 'knowing', 'expect',...\n","3           3  ...  ['lou_ford', 'young', 'small_town', 'deputy_sh...\n","4           4  ...  ['killer_inside', 'tough', 'directed', 'michae...\n","\n","[5 rows x 4 columns]"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RaArYXUR71wO","executionInfo":{"status":"ok","timestamp":1619979920158,"user_tz":-330,"elapsed":86740,"user":{"displayName":"Shreyansh Bardia","photoUrl":"","userId":"12020969166647775692"}},"outputId":"9fd4d30e-3dc3-4865-a644-a06170050fdd"},"source":["print(all_phrases1.shape)\n","# print(all_vectors1.shape)"],"execution_count":24,"outputs":[{"output_type":"stream","text":["(24527, 4)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YrMRyqXCcGmU","executionInfo":{"status":"ok","timestamp":1619979920158,"user_tz":-330,"elapsed":86735,"user":{"displayName":"Shreyansh Bardia","photoUrl":"","userId":"12020969166647775692"}},"outputId":"76c0864c-6562-4ee2-b3ff-c40635aaa33e"},"source":["len(all_phrases1[all_phrases1['label']==0])"],"execution_count":25,"outputs":[{"output_type":"execute_result","data":{"text/plain":["6640"]},"metadata":{"tags":[]},"execution_count":25}]},{"cell_type":"code","metadata":{"id":"hUqZnvkWbEb6","executionInfo":{"status":"ok","timestamp":1619979920159,"user_tz":-330,"elapsed":86731,"user":{"displayName":"Shreyansh Bardia","photoUrl":"","userId":"12020969166647775692"}}},"source":["all_phrases1.drop('Unnamed: 0',axis=1,inplace=True)"],"execution_count":26,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VhISE3Q-Ha6K","executionInfo":{"status":"ok","timestamp":1619979980485,"user_tz":-330,"elapsed":147053,"user":{"displayName":"Shreyansh Bardia","photoUrl":"","userId":"12020969166647775692"}},"outputId":"4251247d-faca-4af0-a72b-582b78929a31"},"source":["new_labels = labelling(all_phrases1['reviews'])\n"],"execution_count":27,"outputs":[{"output_type":"stream","text":["labelling\n","[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n","  warnings.warn(\"The twython library has not been installed. \"\n"],"name":"stderr"},{"output_type":"stream","text":["reviews: 24527\n","labels: 24527\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":195},"id":"5SrNMcP0LMMb","executionInfo":{"status":"ok","timestamp":1619979980486,"user_tz":-330,"elapsed":147048,"user":{"displayName":"Shreyansh Bardia","photoUrl":"","userId":"12020969166647775692"}},"outputId":"cdb254d3-8a26-4461-b2f9-f922b2a9e5bb"},"source":["new_labels[:5]"],"execution_count":28,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>reviews</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>First off, this is a film that is made with a ...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Casey Affleck (Ben Affleck's younger brother) ...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>I went to see The Killer Inside Me not really ...</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Lou Ford is a young small town deputy sheriff ...</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>The Killer Inside Me is tough. Directed by Mic...</td>\n","      <td>2</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                             reviews  label\n","0  First off, this is a film that is made with a ...      1\n","1  Casey Affleck (Ben Affleck's younger brother) ...      1\n","2  I went to see The Killer Inside Me not really ...      2\n","3  Lou Ford is a young small town deputy sheriff ...      2\n","4  The Killer Inside Me is tough. Directed by Mic...      2"]},"metadata":{"tags":[]},"execution_count":28}]},{"cell_type":"code","metadata":{"id":"R0Fi1UvDKqb2","executionInfo":{"status":"ok","timestamp":1619979980486,"user_tz":-330,"elapsed":147043,"user":{"displayName":"Shreyansh Bardia","photoUrl":"","userId":"12020969166647775692"}}},"source":["all_phrases1['label'] = new_labels['label']"],"execution_count":29,"outputs":[]},{"cell_type":"code","metadata":{"id":"t8iVYdszIe6W","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619980046528,"user_tz":-330,"elapsed":213080,"user":{"displayName":"Shreyansh Bardia","photoUrl":"","userId":"12020969166647775692"}},"outputId":"81b40ff2-d9df-469b-daa0-8e7750abad32"},"source":["import gensim\n","import numpy as np\n","from gensim.models import Word2Vec as wvec\n","from sklearn.decomposition import PCA\n","\n","from asteval import Interpreter\n","asteval2 = Interpreter()\n","\n","to_be_vectorized = []\n","\n","for rev in all_phrases1['cleaned_reviews']:\n","  to_be_vectorized.append(asteval2(rev))\n","\n","# all_phrases1['cleaned_reviews2'] = to_be_vectorized\n","\n","cbow = wvec(min_count=10,window=5,size=all_phrases1.shape[0],alpha=0.01,min_alpha=0.001,sample=6e-5,negative = 10)\n","\n","\n","dictionary = gensim.corpora.Dictionary(to_be_vectorized)\n","\n","model = wvec(min_count=10,window=5,size=all_phrases1.shape[0],alpha=0.01,min_alpha=0.001,sample=6e-5,negative = 10)\n","\n","model.build_vocab(sentences=to_be_vectorized)\n","\n","model.train(all_phrases1,total_examples=cbow.corpus_count,epochs=200,)\n","\n","vectors = model.wv.vectors\n","\n","print(vectors.shape)\n","\n","pca = PCA(n_components=100)\n","\n","v2 = vectors.T\n","# v2 = vectors\n","\n","\n","print(v2.shape)\n","\n","\n","v2 = pca.fit_transform(v2) \n","print(v2.shape)\n"],"execution_count":30,"outputs":[{"output_type":"stream","text":["(13624, 24527)\n","(24527, 13624)\n","(24527, 100)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2rjUk74tqBYa","executionInfo":{"status":"ok","timestamp":1619980046529,"user_tz":-330,"elapsed":213077,"user":{"displayName":"Shreyansh Bardia","photoUrl":"","userId":"12020969166647775692"}}},"source":["import numpy as np\n","labels = np.array(new_labels['label'])\n"],"execution_count":31,"outputs":[]},{"cell_type":"code","metadata":{"id":"_UF77bup5RD-","executionInfo":{"status":"ok","timestamp":1619980046529,"user_tz":-330,"elapsed":213076,"user":{"displayName":"Shreyansh Bardia","photoUrl":"","userId":"12020969166647775692"}}},"source":["# model = split_data(v2,labels)"],"execution_count":32,"outputs":[]},{"cell_type":"code","metadata":{"id":"HwhNiJDzXH9D","executionInfo":{"status":"ok","timestamp":1619980046529,"user_tz":-330,"elapsed":213074,"user":{"displayName":"Shreyansh Bardia","photoUrl":"","userId":"12020969166647775692"}}},"source":["# all_vectors1 = embedding(all_phrases1)"],"execution_count":33,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oKw9cHyuH22S"},"source":["# Resampling of minor class"]},{"cell_type":"code","metadata":{"id":"yeOeBpAMJb-o","executionInfo":{"status":"ok","timestamp":1619980046530,"user_tz":-330,"elapsed":213074,"user":{"displayName":"Shreyansh Bardia","photoUrl":"","userId":"12020969166647775692"}}},"source":["# all_phrases1 = all_phrases1.sample(frac=1)"],"execution_count":34,"outputs":[]},{"cell_type":"code","metadata":{"id":"Uc87-yBKwCba","executionInfo":{"status":"ok","timestamp":1619980046530,"user_tz":-330,"elapsed":213071,"user":{"displayName":"Shreyansh Bardia","photoUrl":"","userId":"12020969166647775692"}}},"source":["subset_phrases = all_phrases1.iloc[:5000,:]\n","subset_vectors = v2[:5000,:]"],"execution_count":35,"outputs":[]},{"cell_type":"code","metadata":{"id":"LBa8g21AH1uy","executionInfo":{"status":"ok","timestamp":1619980046530,"user_tz":-330,"elapsed":213068,"user":{"displayName":"Shreyansh Bardia","photoUrl":"","userId":"12020969166647775692"}}},"source":["import numpy as np\n","from sklearn.utils import resample\n","  # df = phrases.sample(frac=1).reset_index(drop=True)\n","\n","seen_data = subset_phrases.iloc[:round(subset_phrases.shape[0]*0.9),:]\n","unseen_data = subset_phrases.iloc[round(subset_phrases.shape[0]*0.9):,:]\n","\n","seen_vectors = subset_vectors[:round(subset_vectors.shape[0]*0.9),:]\n","\n","unseen_vectors = subset_vectors[round(subset_vectors.shape[0]*0.9):,:]\n","\n","from sklearn.model_selection import train_test_split\n","\n","x_train,x_test,y_train,y_test = train_test_split(seen_vectors,seen_data['label'],test_size=0.1,random_state=0)\n","\n","\n","x_train = x_train.reshape((x_train.shape[0],x_train.shape[1],1))\n","x_test = x_test.reshape((x_test.shape[0],x_test.shape[1],1))\n","\n","\n","\n","y_train = np.array(y_train)\n","y_test = np.array(y_test)\n","y_train = y_train.reshape((-1,1))\n","y_test = y_test.reshape((-1,1))\n"],"execution_count":36,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h7QdIKd3lo2g","executionInfo":{"status":"ok","timestamp":1619980049392,"user_tz":-330,"elapsed":215926,"user":{"displayName":"Shreyansh Bardia","photoUrl":"","userId":"12020969166647775692"}},"outputId":"42115468-2561-40f3-fa42-95036001dd86"},"source":["from keras.utils import to_categorical\n","print(x_train.shape)\n","print(y_train.shape)\n","print(x_test.shape)\n","print(y_test.shape)\n","print('positive class in training set:',len(y_train[y_train==1]))\n","print('negative class in training set:',len(y_train[y_train==2]))\n","print('neutral class in training set:',len(y_train[y_train==0]))\n","\n","\n","print('positive class in testing set:',len(y_test[y_test==1]))\n","print('negative class in testing set:',len(y_test[y_test==2]))\n","print('neutral class in testing set:',len(y_test[y_test==0]))\n","\n","pos_train = x_train[y_train[y_train==1]]\n","neg_train = x_train[y_train[y_train==2]]\n","neu_train = x_train[y_train[y_train==0]]\n","\n","\n","print(pos_train.shape)\n","print(neg_train.shape)\n","print(neu_train.shape)\n","\n","upsampled_neg = resample(neg_train,n_samples=5000,replace=True)\n","upsampled_pos = resample(pos_train,n_samples=5000,replace=True)\n","upsampled_neu = resample(neu_train,n_samples=5000,replace=True)\n","\n","\n","print('upsampled_neg shape:',upsampled_neg.shape)\n","print('upsampled_pos shape:',upsampled_pos.shape)\n","print('upsampled_neu shape:',upsampled_neu.shape)\n","\n","all_vectors2 = np.append(upsampled_neg,upsampled_pos,axis=0)\n","all_vectors2 = np.append(all_vectors2,upsampled_neu,axis=0)\n","print('all_vectors2 shape:',all_vectors2.shape)\n","pos_y_train = y_train[y_train==1]\n","neg_y_train = [2 for i in range(len(upsampled_neg))]\n","pos_y_train = [1 for i in range(len(upsampled_pos))]\n","neu_y_train = [0 for i in range(len(upsampled_neu))]\n","print('neg_y_train len',len(neg_y_train))\n","print('pos_y_train len',len(pos_y_train))\n","print('neu_y_train len',len(neu_y_train))\n","\n","\n","new_y_train = np.append(neg_y_train,pos_y_train)\n","new_y_train = np.append(new_y_train,neu_y_train)\n","\n","# new_y_train = to_categorical(new_y_train,3)\n","\n","# print(new_y_train.shape)\n","new_y_train = new_y_train.reshape((-1,1))\n","print('new_y_train shape',new_y_train.shape)"],"execution_count":37,"outputs":[{"output_type":"stream","text":["(4050, 100, 1)\n","(4050, 1)\n","(450, 100, 1)\n","(450, 1)\n","positive class in training set: 2656\n","negative class in training set: 1344\n","neutral class in training set: 50\n","positive class in testing set: 319\n","negative class in testing set: 127\n","neutral class in testing set: 4\n","(2656, 100, 1)\n","(1344, 100, 1)\n","(50, 100, 1)\n","upsampled_neg shape: (5000, 100, 1)\n","upsampled_pos shape: (5000, 100, 1)\n","upsampled_neu shape: (5000, 100, 1)\n","all_vectors2 shape: (15000, 100, 1)\n","neg_y_train len 5000\n","pos_y_train len 5000\n","neu_y_train len 5000\n","new_y_train shape (15000, 1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_s4px6A6oOtE","colab":{"base_uri":"https://localhost:8080/"},"outputId":"a00cd7a5-276f-49c5-cd15-fcda6f6fbeab"},"source":["model = split_data(all_vectors2,new_y_train)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["vectors: (15000, 100, 1)\n","labels: (15000, 1)\n","seen_vectors shape: (13500, 100, 1)\n","seen_labels shape: (13500, 1)\n","unseen_vectors shape: (1500, 100, 1)\n","unseen_labels shape: (1500, 1)\n","data split\n","x_train shape (13500, 100, 1)\n","y_train shape (13500, 1)\n","x_test shape (1500, 100, 1)\n","y_test shape (1500, 1)\n","x_train shape: (13500, 100, 1)\n","y_train shape: (13500, 3)\n","x_test shape: (1500, 100, 1)\n","y_test shape: (1500, 3)\n","Epoch 1/30\n","422/422 [==============================] - 599s 1s/step - loss: 2143.5953 - accuracy: 0.3325\n","Epoch 2/30\n","422/422 [==============================] - 618s 1s/step - loss: 37323.3718 - accuracy: 0.3318\n","Epoch 3/30\n","422/422 [==============================] - 592s 1s/step - loss: 77218.3745 - accuracy: 0.3270\n","Epoch 4/30\n","422/422 [==============================] - 597s 1s/step - loss: 73241.9668 - accuracy: 0.3379\n","Epoch 5/30\n","330/422 [======================>.......] - ETA: 2:12 - loss: 58424.2828 - accuracy: 0.3184"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"J1qsVDMmqwXV"},"source":[""],"execution_count":null,"outputs":[]}]}