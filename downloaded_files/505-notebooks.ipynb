{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0ejrLq_-Zl6"
      },
      "source": [
        "## How-to guide Streaming Feature Store on the Abacus.AI platform\n",
        "This notebook provides you with a hands on environment to build and deploy a feature store using Abacus.AI\n",
        "\n",
        "We'll be using the [Retail Interaction Logs](https://s3.amazonaws.com/abacusai.exampledatasets/pers_promotion/events.csv) and [Item Categories](https://s3.amazonaws.com/abacusai.exampledatasets/pers_promotion/item_categories.csv) datasets, which contain information about user interactions and item attributes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIhrIhXeqJJE"
      },
      "source": [
        "1. Install the Abacus.AI library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JSsm7EQrmvey"
      },
      "outputs": [],
      "source": [
        "!pip install abacusai\n",
        "!pip install fsspec\n",
        "!pip install s3fs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jD5QL0lJsBga"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izrh6I2aqlTa"
      },
      "source": [
        "2. Add your Abacus.AI [API Key](https://abacus.ai/app/profile/apikey) generated using the API dashboard as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d8TJ_-Qamz5x"
      },
      "outputs": [],
      "source": [
        "#@title Abacus.AI API Key\n",
        "\n",
        "api_key = ''  #@param {type: \"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqpb-UfFqmzs"
      },
      "source": [
        "3. Import the Abacus.AI library and instantiate a client."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U8M1MoB0m1DV"
      },
      "outputs": [],
      "source": [
        "from abacusai import ApiClient, ApiException\n",
        "client = ApiClient(api_key)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tXzo8ZRqodi"
      },
      "source": [
        "## 1. Create a Project\n",
        "\n",
        "In this notebook, we're going to create and deploy a feature store that automatically featurizes input data using the Item Categorials and a streamed retail interactions log."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8idfft0im5ci"
      },
      "outputs": [],
      "source": [
        "project = client.create_project(name='Demo Feature Store Streaming Project', use_case='FEATURE_STORE')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wS8T5vcLrIlg"
      },
      "source": [
        "## 2. Creating Datasets\n",
        "\n",
        "Using the Create Dataset API, we can tell Abacus.AI the public S3 URI of where to find our batch dataset.\n",
        "- [Items Dataset](https://s3.amazonaws.com/abacusai.exampledatasets/pers_promotion/item_categories.csv)\n",
        "This dataset contains information about the item categories.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_Ccd2iIm0sX"
      },
      "source": [
        "### Data Preview\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "qDK9JNZ0m2Er",
        "outputId": "7e1ca4f4-7efd-43ef-fb96-6489b85dcfd6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "pd.read_csv('s3://abacusai.exampledatasets/pers_promotion/item_categories.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2R3p7tAkrwPv"
      },
      "source": [
        "### Add the datasets to Abacus.AI\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mDVrJYKrzB9"
      },
      "source": [
        "Using the Create Dataset API, we can tell Abacus.AI the public S3 URI of where to find the datasets.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Asz9GAiLHJQn"
      },
      "outputs": [],
      "source": [
        "# if the datasets already exist, skip creation\n",
        "try: \n",
        "  items_dataset = client.describe_dataset(client.describe_feature_group_by_table_name('items_categories').dataset_id)\n",
        "  streaming_dataset_items = client.describe_dataset(client.describe_feature_group_by_table_name('streaming_item_interactions').dataset_id)\n",
        "  batch_events_dataset = client.describe_dataset(client.describe_feature_group_by_table_name('events_batch_data').dataset_id)\n",
        "except ApiException: # datasets not found\n",
        "  items_dataset = client.create_dataset_from_file_connector(name='Items Dataset', table_name='items_categories', location='s3://abacusai.exampledatasets/pers_promotion/item_categories.csv')\n",
        "  streaming_dataset_items = client.create_streaming_dataset(name='Item Interactions', \n",
        "                                                            table_name='streaming_item_interactions')\n",
        "  batch_events_dataset = client.create_dataset_from_file_connector(name='Item Interactions Batch Data', \n",
        "                                                                 table_name='events_batch_data', \n",
        "                                                                 location='s3://abacusai.exampledatasets/pers_promotion/events.csv')\n",
        "  items_dataset.wait_for_inspection()\n",
        "  batch_events_dataset.wait_for_inspection()\n",
        "\n",
        "streaming_feature_group = client.describe_feature_group_by_table_name(table_name=streaming_dataset_items.feature_group_table_name)\n",
        "items_feature_group = client.describe_feature_group_by_table_name(table_name=items_dataset.feature_group_table_name)\n",
        "batch_feature_group = client.describe_feature_group_by_table_name(table_name=batch_events_dataset.feature_group_table_name)\n",
        "\n",
        "items_feature_group.add_to_project(project.project_id)\n",
        "streaming_feature_group.add_to_project(project.project_id)\n",
        "batch_feature_group.add_to_project(project.project_id)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UvT_qOdprKzp"
      },
      "outputs": [],
      "source": [
        "items_feature_group.set_indexing_config(primary_key='itemid')\n",
        "batch_feature_group.set_indexing_config(update_timestamp_key='timestamp')\n",
        "streaming_dataset_items.set_streaming_retention_policy(retention_hours=48, retention_row_count=2_000_000_000)\n",
        "streaming_feature_group.set_schema([{'name': 'timestamp', 'dataType': 'DATETIME'}, \n",
        "                                    {'name': 'itemid', 'dataType': 'STRING'}, \n",
        "                                    {'name': 'event', 'dataType': 'STRING'}, \n",
        "                                    {'name': 'visitorid', 'dataType': 'STRING'}])\n",
        "streaming_feature_group.set_indexing_config(lookup_keys=['visitorid'], update_timestamp_key='timestamp')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-lnw8eyrcNl"
      },
      "source": [
        "### Stream data to the streaming dataset\n",
        "\n",
        "We'll first test streaming some mock data, invalidate that, then upload real data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6I9a8h8ttKzI"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "streaming_tokens = client.list_streaming_tokens()\n",
        "if streaming_tokens:\n",
        "  streaming_token = streaming_tokens[0].streaming_token\n",
        "else:\n",
        "  streaming_token = client.create_streaming_token().streaming_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MCZ9BWiLxsLt"
      },
      "outputs": [],
      "source": [
        "streaming_feature_group.append_data(streaming_token=streaming_token, \n",
        "                                    data={'visitorid': '123', 'itemid': '1', 'event': 'click', 'timestamp': time.time()})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "__gawFoRyFKM"
      },
      "outputs": [],
      "source": [
        "streaming_feature_group.invalidate_streaming_data(invalid_before_timestamp=time.time())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQpSnJXxxo4B"
      },
      "source": [
        "To get some real data, we're going to load the data from a batch dataset locally and stream it all into the new streaming dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W4zOu9UMrpj0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "raw_event_data = pd.read_csv('s3://abacusai.exampledatasets/pers_promotion/events.csv')\n",
        "raw_event_data['visitorid'] = raw_event_data.index\n",
        "raw_event_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_C3A0E_Lsa9M"
      },
      "outputs": [],
      "source": [
        "from numpy import isnan\n",
        "for row in raw_event_data.head(10).to_dict(orient=\"records\"):\n",
        "    row['event_timestamp'] =  time.time()\n",
        "    row['visitorid'] = str(row['visitorid'])\n",
        "    if isnan(row['transactionid']):\n",
        "        row['transactionid'] = None\n",
        "    streaming_feature_group.append_data(streaming_token=streaming_token, data=row)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PQdlF-HWfrz"
      },
      "source": [
        "Verify recently streamed data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zjmw4nCpWh_u"
      },
      "outputs": [],
      "source": [
        "streaming_feature_group.get_recent_streamed_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uu5reOzojzmD"
      },
      "source": [
        "### Concatenate Batch with streaming\n",
        "\n",
        "If you have a batch dataset like above, it's more efficient to load it into Abacus.AI first and the streaming feature group data into the batch dataset (or vice versa)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ugIoEQVjzQZ"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "batch_feature_group.concatenate_data(streaming_feature_group.feature_group_id, \n",
        "                                         merge_type='UNION', \n",
        "                                         replace_until_timestamp=datetime(2021, 9, 1).timestamp())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIIxOKVxzmEo"
      },
      "source": [
        "## Join Metadata with Feature Group Data\n",
        "\n",
        "We can join batch with streaming either with SQL or a python feature group"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wn17EkJAzwwp"
      },
      "source": [
        "### Python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LFv_IxYNzyDa"
      },
      "outputs": [],
      "source": [
        "function_code = '''\n",
        "import pandas as pd\n",
        "\n",
        "def join_tables(events_df, items_df):\n",
        "    return pd.merge(events_df, items_df, on='itemid')\n",
        "'''\n",
        "python_feature_group = client.create_feature_group_from_function(table_name='python_interactions_joined_items', \n",
        "                                                                 function_source_code=function_code, \n",
        "                                                                 function_name='join_tables', \n",
        "                                                                 input_feature_groups=['events_batch_data', 'items_categories'])\n",
        "python_feature_group.add_to_project(project.project_id)\n",
        "python_feature_group.set_schema([{'name': 'timestamp', 'dataType': 'DATETIME'}, \n",
        "                                    {'name': 'itemid', 'dataType': 'STRING'}, \n",
        "                                    {'name': 'categoryid', 'dataType': 'STRING'}, \n",
        "                                    {'name': 'visitorid', 'dataType': 'STRING'},\n",
        "                                    {'name': 'event', 'dataType': 'STRING'},\n",
        "                                    {'name': 'transactionid', 'dataType': 'STRING'}])\n",
        "python_feature_group.set_indexing_config(lookup_keys=['visitorid'], update_timestamp_key='timestamp')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKsa6mrT1Ooz"
      },
      "source": [
        "### SQL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3RzlgP5ej64W"
      },
      "outputs": [],
      "source": [
        "feature_group = client.create_feature_group(table_name='interactions_joined_items', sql='SELECT * FROM events_batch_data JOIN items_categories USING (itemid)')\n",
        "feature_group.set_indexing_config(lookup_keys=['visitorid'])\n",
        "feature_group.add_to_project(project.project_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9zdfMpRLRB3"
      },
      "source": [
        "### Materialize Feature Group Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ompwZJ4nLkqw"
      },
      "outputs": [],
      "source": [
        "feature_group_version = feature_group.create_version()\n",
        "feature_group_version.wait_for_results()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mx4JE8stMCkp"
      },
      "outputs": [],
      "source": [
        "feature_group_version.load_as_pandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1ZjBprIEIzw"
      },
      "source": [
        "### Deploy feature group for online featurization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZleD66xQnCY_"
      },
      "outputs": [],
      "source": [
        "deployment_token = client.create_deployment_token(project_id=project.project_id).deployment_token\n",
        "deployment = client.create_deployment(feature_group_id=feature_group.feature_group_id, project_id=project.project_id) \n",
        "deployment.wait_for_deployment()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RVEBK_UenDXB"
      },
      "outputs": [],
      "source": [
        "client.lookup_features(deployment_id=deployment.deployment_id, deployment_token=deployment_token, query_data={'visitorid': ['466806', '273888']})"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Notebook 2: Abacus.AI API Streaming Feature Store",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
