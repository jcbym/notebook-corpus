{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mehdi2277/Documents/HarveyMuddWork/Neural_Nets_Research/neural_nets_research\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neural_nets_library import training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A lot of inspiration from https://github.com/loudinthecloud/pytorch-ntm. Hyperparameters were chosen based\n",
    "# upon his experiments.\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_seed(seed=None):\n",
    "    \"\"\"Seed the RNGs for predicatability/reproduction purposes.\"\"\"\n",
    "    if seed is None:\n",
    "        seed = int(get_ms() // 1000)\n",
    "\n",
    "    print(\"Using seed=%d\", seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNC_Memory(nn.Module):\n",
    "    \"\"\"\n",
    "    Class that stores memory so that a single representation of memory can be passed easily\n",
    "    throughout the program.\n",
    "    \"\"\"\n",
    "    def __init__(self, address_count, address_dimension, batch_size):\n",
    "        \"\"\"\n",
    "        Initializes DNC_Memory and prepares for training\n",
    "        \n",
    "        :param address_count: Number of addresses in memory\n",
    "        :param address_dimension: The size of a current location in memory\n",
    "        \"\"\"\n",
    "        super(DNC_Memory, self).__init__()\n",
    "        self.initial_memory = nn.Parameter(torch.zeros(1, address_count, address_dimension))\n",
    "        self.batch_size = batch_size\n",
    "        self.reset_parameters()\n",
    "        self.initialize_state()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        \"\"\"\n",
    "        Resets the parameters in memory, should be used before beginning training\n",
    "        \"\"\"\n",
    "        _, N, M = self.initial_memory.size()\n",
    "        stdev = 1 / np.sqrt(N + M)\n",
    "        nn.init.uniform(self.initial_memory, -stdev, stdev)\n",
    "    \n",
    "    def initialize_state(self):\n",
    "        \"\"\"\n",
    "        Initializes the state of DNC_Memory, specifically expands memory to match batch_size for training\n",
    "        \"\"\"\n",
    "        self.memory = self.initial_memory.repeat(self.batch_size, 1, 1)\n",
    "    \n",
    "    def content_address_memory(self, key_vec, β):\n",
    "        \"\"\"\n",
    "        Used to find memory addresses to read/write to based on controller output for read/write heads\n",
    "        by cosine similarity.\n",
    "        \n",
    "        :param key_vec: A vector of size address_count that points to places in memory to access\n",
    "        :param β: The write strength outputted by the controller\n",
    "        :return: A softmax over the addresses based on cosine similarity of key_vec to keys in memory\n",
    "        \"\"\"\n",
    "        result = F.cosine_similarity(key_vec.unsqueeze(1).expand_as(self.memory), \n",
    "                                     self.memory, dim = 2)\n",
    "        result = β * result\n",
    "        result = result.exp()\n",
    "        result = result / result.sum(1, keepdim=True)\n",
    "        return result\n",
    "    \n",
    "    def read_memory(self, address_vec):\n",
    "        \"\"\"\n",
    "        :param address_vec: Vector of length address_count corresponding to the read weight\n",
    "                at each memory location.\n",
    "        :return: the result of a read to memory based on an address_vec\n",
    "        \"\"\"\n",
    "        return torch.bmm(self.memory.transpose(1,2), address_vec.unsqueeze(2)).squeeze(2)\n",
    "    \n",
    "    def update_memory(self, address_vec, erase_vec, add_vec):\n",
    "        \"\"\"\n",
    "        Updates memory based on the results of a write head and controller output\n",
    "        :param address_vec: Vector of length address_count corresponding to the write weight\n",
    "                at each memory location.\n",
    "        :param erase_vec: A of length address_count vector used in conjuntion with the address_vec to determine where\n",
    "                in memory to erase\n",
    "        :param add_vec: A of length address_count vector used in conjuntion with the address_vec to determine where\n",
    "                in memory to write\n",
    "        \"\"\"\n",
    "        self.memory = self.memory * (1 - torch.bmm(address_vec.unsqueeze(2), erase_vec.unsqueeze(1)))\n",
    "        self.memory += torch.bmm(address_vec.unsqueeze(2), add_vec.unsqueeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNC_Usage(nn.Module):\n",
    "    \"\"\"\n",
    "    Class that represents the usage vector for the DNC, or the vector that tracks which places in memory have\n",
    "    been recently written to or recently read from in order to determine which memory locations should be freed.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, address_count, batch_size):\n",
    "        \"\"\"\n",
    "        Initialize a usage vector\n",
    "        :param address_count: The number of memory locations in the DNC\n",
    "        :param batch_size: The batch size for training\n",
    "        \"\"\"\n",
    "        super(DNC_Usage, self).__init__()\n",
    "        self.register_buffer('initial_usage', Variable(torch.zeros(1, address_count)))\n",
    "        self.batch_size = batch_size\n",
    "        self.initialize_state()\n",
    "        \n",
    "    def initialize_state(self):\n",
    "        \"\"\"\n",
    "        Initializes the usage vector to the proper demention, should be used before training starts.\n",
    "        \"\"\"\n",
    "        self.usage = self.initial_usage.repeat(self.batch_size, 1)\n",
    "    \n",
    "    def read_update_usage(self, address_vec, rfree_weights):\n",
    "        \"\"\"\n",
    "        Updates the usage vector based on recent reads, decreases usage where locations are heavily read or\n",
    "        controller output dictates\n",
    "        :param address_vec: Vector that was used to read from locations in memory (length address_count)\n",
    "        :param rfree_weights: Controller output for how much usage should be freed after a round of reading.\n",
    "        \"\"\"\n",
    "        self.usage *= 1 - rfree_weights * address_vec\n",
    "    \n",
    "    def write_update_usage(self, address_vec):\n",
    "        \"\"\"\n",
    "        Updates usage vector based on recent reads, increases usage where locations are heavily written to.\n",
    "        :param address_vec: Vector that was used to write to memory (length address_count)\n",
    "        \"\"\"\n",
    "        self.usage += (1 - self.usage) * address_vec\n",
    "    \n",
    "    def allocation_weights(self):\n",
    "        \"\"\"\n",
    "        Calculates the allocation vector used to calculate where the next writes can occur.\n",
    "        :return: Allocation vector that tells the DNC where it can write to without freeing up memory.\n",
    "        \"\"\"\n",
    "        sorted_usage, indices_usage = torch.sort(self.usage)\n",
    "        prod_sorted_usage = torch.cumprod(torch.cat((Variable(torch.ones(self.batch_size, 1).cuda()), \n",
    "                                                     sorted_usage), dim=1), dim=1)[:, :-1]\n",
    "        sorted_allocation = (1 - sorted_usage) * prod_sorted_usage\n",
    "        return sorted_allocation.gather(1, indices_usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _split_cols(mat, lengths):\n",
    "    \"\"\"Split a 2D matrix to variable length columns.\"\"\"\n",
    "    assert mat.size()[1] == sum(lengths), \"Lengths must be summed to num columns\"\n",
    "    l = np.cumsum([0] + lengths)\n",
    "    results = []\n",
    "    for s, e in zip(l[:-1], l[1:]):\n",
    "        results += [mat[:, s:e]]\n",
    "    return results\n",
    "\n",
    "class DNC_Head(nn.Module):\n",
    "    \"\"\"\n",
    "    Parent class of read/write heads, used to read and write to memory.\n",
    "    \"\"\"\n",
    "    def __init__(self, address_count, address_dimension, \n",
    "                 controller_output_size):\n",
    "        \"\"\"\n",
    "        Initializes a read or write head\n",
    "        :param address_count: Number of addresses in memory\n",
    "        :param address_dimension: Size of each memory location\n",
    "        :param controller_output_size: Size of controller output\n",
    "        \"\"\"\n",
    "        super(DNC_Head, self).__init__()\n",
    "        \n",
    "        self.controller_output_size = controller_output_size\n",
    "        self.N = address_count\n",
    "        self.M = address_dimension\n",
    "    \n",
    "    def is_read_head(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def initialize_state(self):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNC_Read_Head(DNC_Head):\n",
    "    def __init__(self, address_count, address_dimension, controller_output_size, batch_size, num_write_heads):\n",
    "        \"\"\"\n",
    "        Initializes a read head\n",
    "        :param address_count: Number of addresses in memory\n",
    "        :param address_dimension: Size of each memory location\n",
    "        :param controller_output_size: Size of controller output\n",
    "        :param num_write_heads: Number of write heads in the DNC\n",
    "        \"\"\"\n",
    "        super(DNC_Read_Head, self).__init__(address_count, address_dimension, controller_output_size)\n",
    "        # key_vec, β, read_mode, rfree_gate\n",
    "        # self.M is the number of rows\n",
    "        self.num_write_heads = num_write_heads\n",
    "        \n",
    "        key_vec_dim = self.M\n",
    "        β_dim = 1\n",
    "        # There are 2 * num_write_heads + 1 read modes per head, 1 forward 1 backward per head,\n",
    "        # 1 Content based address.\n",
    "        read_mode_dim = 2 * self.num_write_heads + 1\n",
    "        rfree_gate_dim = self.N\n",
    "        \n",
    "        self.read_parameters_lengths = [key_vec_dim, β_dim, read_mode_dim, rfree_gate_dim]\n",
    "        self.fc_read_parameters = nn.Linear(controller_output_size, sum(self.read_parameters_lengths))\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.reset_parameters()\n",
    "        self.initialize_state()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        \"\"\"\n",
    "        Resets the parameters in read head, should be used before beginning training\n",
    "        \"\"\"\n",
    "        nn.init.xavier_uniform(self.fc_read_parameters.weight, gain=1.4)\n",
    "        nn.init.normal(self.fc_read_parameters.bias, std=0.01)\n",
    "        \n",
    "        self.initial_address_vec = nn.Parameter(torch.zeros(self.N))\n",
    "        self.initial_read = nn.Parameter(torch.randn(1, self.M) * 0.01)\n",
    "    \n",
    "    def initialize_state(self):\n",
    "        \"\"\"\n",
    "        Initializes the read head, should be used before training starts.\n",
    "        \"\"\"\n",
    "        self.prev_address_vec = self.initial_address_vec.repeat(self.batch_size, 1)\n",
    "        self.prev_read = self.initial_read.repeat(self.batch_size, 1)\n",
    "    \n",
    "    def is_read_head(self):\n",
    "        \"\"\"\n",
    "        :returns: True for read heads\n",
    "        \"\"\"\n",
    "        return True\n",
    "    \n",
    "    def forward(self, x, memory, usage, linked_matrices):\n",
    "        \"\"\"\n",
    "        Function called on a forward pass of the network\n",
    "        :param x: The output of the controller\n",
    "        :param memory: The DNC_Memory object to read from\n",
    "        :param usage: The usage vector to update after a read\n",
    "        :param linked_matrices: A list of linked matrices from each read head, used to read forward and backward\n",
    "        for different read modes\n",
    "        :return: The value of a read\n",
    "        \"\"\"\n",
    "        read_parameters = self.fc_read_parameters(x)\n",
    "        # -------------------- Controller Parameter Description ----------------------\n",
    "        # Key_Vec represents the vector key for content based addressing and is used to find read locations\n",
    "        # β corresponds to the write strength and is not used in the read head\n",
    "        # read_modes is a weighted vector representing how strong the read should be based on content based\n",
    "        #     addressing and the forward and backward moves along the linked matrices of different write heads\n",
    "        # rfree_weight determines how much locations should be erased after this round of reading\n",
    "        key_vec, β, read_modes, rfree_weight = _split_cols(read_parameters, self.read_parameters_lengths)\n",
    "        β = F.softplus(β)\n",
    "        rfree_weight = F.sigmoid(rfree_weight)\n",
    "        read_modes = F.softmax(read_modes, dim = 1)\n",
    "        content_address_vec = memory.content_address_memory(key_vec, β)\n",
    "        \n",
    "        forward = []\n",
    "        backward = []\n",
    "        \n",
    "        address_vec = content_address_vec * read_modes[:, 0].unsqueeze(1)\n",
    "        \n",
    "        for i, linked_matrix in enumerate(linked_matrices):\n",
    "            address_vec += read_modes[:, i+1].unsqueeze(1) * torch.bmm(linked_matrix, self.prev_address_vec.unsqueeze(2)).squeeze(2)\n",
    "            address_vec += read_modes[:, i+self.num_write_heads+1].unsqueeze(1) * torch.bmm(linked_matrix.transpose(1,2), \n",
    "                                                                               self.prev_address_vec.unsqueeze(2)).squeeze(2)\n",
    "        \n",
    "        new_read = memory.read_memory(address_vec)\n",
    "        self.prev_address_vec = address_vec\n",
    "        self.prev_read = new_read\n",
    "        usage.read_update_usage(address_vec, rfree_weight)\n",
    "        return new_read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNC_Write_Head(DNC_Head):\n",
    "    \n",
    "    def __init__(self, address_count, address_dimension, controller_output_size, batch_size):\n",
    "        \"\"\"\n",
    "        Initializes a write head\n",
    "        :param address_count: Number of addresses in memory\n",
    "        :param address_dimension: Size of each memory location\n",
    "        :param controller_output_size: Size of controller output\n",
    "        \n",
    "        \"\"\"\n",
    "        super(DNC_Write_Head, self).__init__(address_count, address_dimension, controller_output_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.write_parameters_lengths = [self.M, 1, 1, 1, self.M, self.M]\n",
    "        self.fc_write_parameters = nn.Linear(controller_output_size, sum(self.write_parameters_lengths))\n",
    "        \n",
    "        #initialize the precedence vector\n",
    "        self.register_buffer('initial_precedence_vec', Variable(torch.zeros(batch_size, self.N)))\n",
    "        #initialize the link matrix\n",
    "        self.register_buffer('initial_link_matrix', Variable(torch.zeros(batch_size, self.N, self.N)))\n",
    "        \n",
    "        self.reset_parameters()\n",
    "        self.initialize_state()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        \"\"\"\n",
    "        Resets the parameters in write head, should be used before beginning training\n",
    "        \"\"\"        \n",
    "        nn.init.xavier_uniform(self.fc_write_parameters.weight, gain=1.4)\n",
    "        nn.init.normal(self.fc_write_parameters.bias, std=0.01)\n",
    "        \n",
    "        self.initial_address_vec = nn.Parameter(torch.zeros(self.N))\n",
    "    \n",
    "    def initialize_state(self):\n",
    "        \"\"\"\n",
    "        Initializes the write head, should be used before training starts.\n",
    "        \"\"\"\n",
    "        self.prev_address_vec = self.initial_address_vec.clone()\n",
    "        self.precedence_vec = self.initial_precedence_vec\n",
    "        self.link_matrix = self.initial_link_matrix\n",
    "    \n",
    "    def is_read_head(self):\n",
    "        \"\"\"\n",
    "        :return: If the head is a read head (False)\n",
    "        \"\"\"\n",
    "        return False\n",
    "    \n",
    "    def forward(self, x, memory, usage):\n",
    "        \"\"\"\n",
    "        Function called on a forward pass of the network\n",
    "        :param x: The output of the controller\n",
    "        :param memory: The DNC_Memory object to write to\n",
    "        :param usage: The usage vector to update after a write\n",
    "        \"\"\"\n",
    "        write_parameters = self.fc_write_parameters(x)\n",
    "        # -------------------- Controller Parameter Description ----------------------\n",
    "        # Key_Vec represents the vector key for content based addressing and is used to find write locations\n",
    "        # β is the write strength, multiplied by write weights, gives controller control over writes\n",
    "        # g is the write gate, used to determine where final writes occur\n",
    "        # alloc_gate Determines how strongly allocated memory locations are\n",
    "        # erase_vec Similar to the forget get in LSTM, helps the write head get rid of info it no longer needs\n",
    "        # add_vec A vector that represents what the write head will write to memory\n",
    "        key_vec, β, g, alloc_gate, erase_vec, add_vec = _split_cols(write_parameters, \n",
    "                                                                    self.write_parameters_lengths)\n",
    "        β = F.softplus(β)\n",
    "        g = F.sigmoid(g)\n",
    "        alloc_gate = F.sigmoid(alloc_gate)\n",
    "        erase_vec = F.sigmoid(erase_vec)\n",
    "                                               \n",
    "        content_address_vec = memory.content_address_memory(key_vec, β)\n",
    "        address_vec = g * (alloc_gate * usage.allocation_weights() + (1 - alloc_gate) * content_address_vec)\n",
    "        self.prev_address_vec = address_vec\n",
    "        memory.update_memory(address_vec, erase_vec, add_vec)\n",
    "        \n",
    "        # Update the link matrix and precedence vector.\n",
    "        tempMatrix = 1 - address_vec.unsqueeze(1).repeat(1,self.N,1) - address_vec.unsqueeze(1).repeat(1,self.N,1)\n",
    "        tempMatrix2 = torch.bmm(address_vec.unsqueeze(2), self.precedence_vec.unsqueeze(1))\n",
    "        self.link_matrix = tempMatrix * self.link_matrix + tempMatrix2\n",
    "        diag_mask = 1 - Variable(torch.eye(self.N).unsqueeze(0).expand_as(self.link_matrix).cuda())\n",
    "        self.link_matrix *= diag_mask\n",
    "        \n",
    "        self.precedence_vec = (1 - torch.sum(address_vec, 1, keepdim=True)) * self.precedence_vec + address_vec\n",
    "        \n",
    "        # Update usage.\n",
    "        usage.write_update_usage(address_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNC(nn.Module):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, batch_size, controller, controller_output_size, \n",
    "                 output_size, address_count, address_dimension, heads):\n",
    "        super(DNC, self).__init__()\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # Initialize controller\n",
    "        self.controller = controller\n",
    "        \n",
    "        # Create output gate. No activation function is used with it because\n",
    "        # I used BCEWithLogitsLoss which deals with the sigmoid in a more\n",
    "        # numerically stable manner.\n",
    "        self.outputGate = nn.Linear(controller_output_size, output_size)\n",
    "        \n",
    "        # Initialize memory\n",
    "        self.memory = DNC_Memory(address_count, address_dimension, batch_size)\n",
    "\n",
    "        # Construct the heads.\n",
    "        self.heads = nn.ModuleList()\n",
    "        \n",
    "        # Initialize usage vector, might not need batch size\n",
    "        self.usage = DNC_Usage(address_count, batch_size)\n",
    "        num_writes = heads.count(1)\n",
    "        \n",
    "        self.linked_matrices = []\n",
    "        for head_id in heads:\n",
    "            if head_id == 0:\n",
    "                self.heads.append(DNC_Read_Head(address_count, address_dimension, controller_output_size, batch_size, num_writes))\n",
    "            else:\n",
    "                self.heads.append(DNC_Write_Head(address_count, address_dimension, controller_output_size, batch_size))\n",
    "        \n",
    "        self.initialize_state()\n",
    "        \n",
    "    def initialize_state(self):\n",
    "        self.prev_reads = []\n",
    "        self.linked_matrices = []\n",
    "        for head in self.heads:\n",
    "            head.initialize_state()\n",
    "            if head.is_read_head():\n",
    "                self.prev_reads.append(head.prev_read)\n",
    "            #Added this else statement to initialize the linked matrices for write heads\n",
    "            else:\n",
    "                self.linked_matrices.append(head.link_matrix)\n",
    "        \n",
    "        self.memory.initialize_state()\n",
    "        self.usage.initialize_state()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_uniform(self.outputGate.weight)\n",
    "        nn.init.normal(self.outputGate.bias, std=0.01)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.initialize_state()\n",
    "        outputs = []\n",
    "        \n",
    "        for current_observation in x.transpose(0,1):\n",
    "            self.prev_reads.append(current_observation)\n",
    "            controller_input = torch.cat(self.prev_reads, 1)\n",
    "            controller_output = self.controller(controller_input).squeeze()\n",
    "\n",
    "            self.prev_reads = []\n",
    "            write_index = 0\n",
    "            for i in range(len(self.heads)):\n",
    "                head = self.heads[i]\n",
    "                if head.is_read_head():\n",
    "                    self.prev_reads.append(head(controller_output, self.memory, self.usage, self.linked_matrices))\n",
    "                else:\n",
    "                    head(controller_output, self.memory, self.usage)\n",
    "                    self.linked_matrices[write_index] = head.link_matrix\n",
    "                    write_index += 1\n",
    "            \n",
    "            current_output = self.outputGate(controller_output)\n",
    "            outputs.append(current_output)\n",
    "        \n",
    "        return torch.stack(outputs).transpose(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CopyTaskDataset(data.Dataset):\n",
    "    def __init__(self, num_batches, batch_size, lower, upper, seq_size):\n",
    "        self.input_list = []\n",
    "        self.label_list = []\n",
    "        \n",
    "        for _ in range(num_batches):\n",
    "            data, label = self.generate_batch(batch_size, lower, upper, seq_size)\n",
    "            self.input_list.append(data)\n",
    "            self.label_list.append(label)\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def generate_batch(self, batch_size, lower, upper, seq_size):\n",
    "        seq_length = random.randint(lower, upper)\n",
    "        label = torch.from_numpy(\n",
    "                np.random.binomial(1, 0.5, (seq_length, batch_size, seq_size))).float()\n",
    "        end_marker = torch.zeros(seq_length, batch_size, 1)\n",
    "        seq = torch.cat((label, end_marker), 2)\n",
    "        delimiter_column = torch.zeros(1, batch_size, seq_size+1)\n",
    "        delimiter_column[0, :, seq_size] = 1\n",
    "        seq = torch.cat((seq, delimiter_column), 0)\n",
    "        output_time = torch.zeros(seq_length, batch_size, seq_size+1)\n",
    "        seq = torch.cat((seq, output_time), 0)\n",
    "        return seq, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_list)*self.batch_size\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        batch_index = i//self.batch_size\n",
    "        index_in_batch = i % self.batch_size\n",
    "        return self.input_list[batch_index][:, index_in_batch, :], self.label_list[batch_index][:, index_in_batch, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncapsulatedLSTM(nn.Module):\n",
    "    def __init__(self, batch_size, all_hiddens, *args, **kwargs):\n",
    "        super(EncapsulatedLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(*args, **kwargs)\n",
    "        self.all_hiddens = all_hiddens\n",
    "        self.batch_size = batch_size\n",
    "                \n",
    "        self.num_inputs = args[0]\n",
    "        self.hidden_size = args[1]\n",
    "        self.num_layers = args[2]\n",
    "        \n",
    "        self.reset_parameters()\n",
    "        self.initialize_state()\n",
    "          \n",
    "    def initialize_state(self):\n",
    "        self.state_tuple = (self.initial_hidden_state.repeat(1, self.batch_size, 1), \n",
    "                            self.initial_cell_state.repeat(1, self.batch_size, 1))\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        self.initial_hidden_state = nn.Parameter(torch.randn(self.num_layers, 1, self.hidden_size) * 0.05)\n",
    "        self.initial_cell_state = nn.Parameter(torch.randn(self.num_layers, 1, self.hidden_size) * 0.05)\n",
    "        \n",
    "        for p in self.lstm.parameters():\n",
    "            if p.dim() == 1:\n",
    "                nn.init.constant(p, 0)\n",
    "            else:\n",
    "                stdev = 5 / (np.sqrt(self.num_inputs +  self.hidden_size))\n",
    "                nn.init.uniform(p, -stdev, stdev)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        self.initialize_state()\n",
    "        output, self.state_tuple = self.lstm(input.unsqueeze(0), self.state_tuple)\n",
    "        \n",
    "        if self.all_hiddens:\n",
    "            return self.state_tuple[0]\n",
    "        else:\n",
    "            return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_task_loss(output, label):\n",
    "    \n",
    "    _, seq_length, _ = label.size()\n",
    "    return F.binary_cross_entropy_with_logits(output[:, -seq_length:, :], label.type(torch.FloatTensor).cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_bits_per_sequence(output, label):\n",
    "    batch_size, seq_length, _ = label.size()\n",
    "    binarized_output = output[:, -seq_length:, :].sign()/2 + 0.5\n",
    "    \n",
    "    # The cost is the number of error bits per sequence\n",
    "    return torch.sum(torch.abs(binarized_output - label))/batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def construct_clipped_optimizer(optimizer_type):\n",
    "#     class ClippedOptimizer(optimizer_type):\n",
    "#         def step(closure=None):\n",
    "#             for group in self.param_groups:\n",
    "#                 for p in group['params']:\n",
    "#                     if p.grad is not None:\n",
    "#                         p.grad = p.grad.clamp(-10,10)\n",
    "            \n",
    "#             super().step(closure)\n",
    "    \n",
    "#     return ClippedOptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ClippedRMSProp = construct_clipped_optimizer(optim.RMSprop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "hidden_size = 100\n",
    "num_layers = 3\n",
    "seq_size = 8\n",
    "address_size = 20\n",
    "controller = EncapsulatedLSTM(batch_size, False, # all hiddens\n",
    "                              seq_size + address_size + 1, hidden_size, \n",
    "                              num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "address_count = 128\n",
    "controller_output_size = hidden_size\n",
    "\n",
    "dnc = DNC(batch_size, controller, controller_output_size, \n",
    "          seq_size, address_count, address_size, [0, 1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnc = dnc.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_seq_length = 3\n",
    "upper_seq_length = 10\n",
    "num_batches = 10000\n",
    "\n",
    "dataset = CopyTaskDataset(num_batches, batch_size, lower_seq_length, upper_seq_length, seq_size)\n",
    "data_loader = data.DataLoader(dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.RMSprop(dnc.parameters(), momentum=0.9,\n",
    "                          alpha=0.95, lr=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/0\n",
      "----------\n",
      "Epoch Number: 0, Batch Number: 25, Training Loss: 0.6937, Validation Loss: 24.9906\n",
      "Time so far is 0m 7s\n",
      "Epoch Number: 0, Batch Number: 50, Training Loss: 0.6936, Validation Loss: 27.1350\n",
      "Time so far is 0m 14s\n",
      "Epoch Number: 0, Batch Number: 75, Training Loss: 0.6937, Validation Loss: 28.0269\n",
      "Time so far is 0m 22s\n",
      "Epoch Number: 0, Batch Number: 100, Training Loss: 0.6934, Validation Loss: 25.3400\n",
      "Time so far is 0m 29s\n",
      "Epoch Number: 0, Batch Number: 125, Training Loss: 0.6935, Validation Loss: 25.0900\n",
      "Time so far is 0m 36s\n",
      "Epoch Number: 0, Batch Number: 150, Training Loss: 0.6935, Validation Loss: 29.6250\n",
      "Time so far is 0m 44s\n",
      "Epoch Number: 0, Batch Number: 175, Training Loss: 0.6936, Validation Loss: 26.1244\n",
      "Time so far is 0m 51s\n",
      "Epoch Number: 0, Batch Number: 200, Training Loss: 0.6935, Validation Loss: 27.0206\n",
      "Time so far is 0m 58s\n",
      "Epoch Number: 0, Batch Number: 225, Training Loss: 0.6934, Validation Loss: 23.1831\n",
      "Time so far is 1m 5s\n",
      "Epoch Number: 0, Batch Number: 250, Training Loss: 0.6935, Validation Loss: 25.0437\n",
      "Time so far is 1m 11s\n",
      "Epoch Number: 0, Batch Number: 275, Training Loss: 0.6933, Validation Loss: 25.6544\n",
      "Time so far is 1m 16s\n",
      "Epoch Number: 0, Batch Number: 300, Training Loss: 0.6934, Validation Loss: 24.7894\n",
      "Time so far is 1m 22s\n",
      "Epoch Number: 0, Batch Number: 325, Training Loss: 0.6934, Validation Loss: 25.5744\n",
      "Time so far is 1m 28s\n",
      "Epoch Number: 0, Batch Number: 350, Training Loss: 0.6933, Validation Loss: 28.8888\n",
      "Time so far is 1m 34s\n",
      "Epoch Number: 0, Batch Number: 375, Training Loss: 0.6933, Validation Loss: 25.0938\n",
      "Time so far is 1m 39s\n",
      "Epoch Number: 0, Batch Number: 400, Training Loss: 0.6933, Validation Loss: 27.9306\n",
      "Time so far is 1m 43s\n",
      "Epoch Number: 0, Batch Number: 425, Training Loss: 0.6932, Validation Loss: 26.1612\n",
      "Time so far is 1m 47s\n",
      "Epoch Number: 0, Batch Number: 450, Training Loss: 0.6932, Validation Loss: 26.1737\n",
      "Time so far is 1m 52s\n",
      "Epoch Number: 0, Batch Number: 475, Training Loss: 0.6933, Validation Loss: 25.3331\n",
      "Time so far is 1m 57s\n",
      "Epoch Number: 0, Batch Number: 500, Training Loss: 0.6932, Validation Loss: 23.4319\n",
      "Time so far is 2m 3s\n",
      "Epoch Number: 0, Batch Number: 525, Training Loss: 0.6934, Validation Loss: 25.0544\n",
      "Time so far is 2m 9s\n",
      "Epoch Number: 0, Batch Number: 550, Training Loss: 0.6932, Validation Loss: 25.4556\n",
      "Time so far is 2m 16s\n",
      "Epoch Number: 0, Batch Number: 575, Training Loss: 0.6933, Validation Loss: 24.2138\n",
      "Time so far is 2m 21s\n",
      "Epoch Number: 0, Batch Number: 600, Training Loss: 0.6933, Validation Loss: 26.1244\n",
      "Time so far is 2m 28s\n",
      "Epoch Number: 0, Batch Number: 625, Training Loss: 0.6933, Validation Loss: 27.4675\n",
      "Time so far is 2m 36s\n",
      "Epoch Number: 0, Batch Number: 650, Training Loss: 0.6932, Validation Loss: 27.8219\n",
      "Time so far is 2m 43s\n",
      "Epoch Number: 0, Batch Number: 675, Training Loss: 0.6932, Validation Loss: 23.5613\n",
      "Time so far is 2m 49s\n",
      "Epoch Number: 0, Batch Number: 700, Training Loss: 0.6932, Validation Loss: 25.6869\n",
      "Time so far is 2m 56s\n",
      "Epoch Number: 0, Batch Number: 725, Training Loss: 0.6932, Validation Loss: 26.0731\n",
      "Time so far is 3m 2s\n",
      "Epoch Number: 0, Batch Number: 750, Training Loss: 0.6932, Validation Loss: 29.1487\n",
      "Time so far is 3m 9s\n",
      "Epoch Number: 0, Batch Number: 775, Training Loss: 0.6932, Validation Loss: 24.8375\n",
      "Time so far is 3m 14s\n",
      "Epoch Number: 0, Batch Number: 800, Training Loss: 0.6933, Validation Loss: 25.2606\n",
      "Time so far is 3m 20s\n",
      "Epoch Number: 0, Batch Number: 825, Training Loss: 0.6932, Validation Loss: 24.7694\n",
      "Time so far is 3m 26s\n",
      "Epoch Number: 0, Batch Number: 850, Training Loss: 0.6932, Validation Loss: 27.1031\n",
      "Time so far is 3m 33s\n",
      "Epoch Number: 0, Batch Number: 875, Training Loss: 0.6932, Validation Loss: 24.8719\n",
      "Time so far is 3m 37s\n",
      "Epoch Number: 0, Batch Number: 900, Training Loss: 0.6932, Validation Loss: 25.7338\n",
      "Time so far is 3m 41s\n",
      "Epoch Number: 0, Batch Number: 925, Training Loss: 0.6931, Validation Loss: 25.0319\n",
      "Time so far is 3m 45s\n",
      "Epoch Number: 0, Batch Number: 950, Training Loss: 0.6932, Validation Loss: 28.0250\n",
      "Time so far is 3m 50s\n",
      "Epoch Number: 0, Batch Number: 975, Training Loss: 0.6932, Validation Loss: 24.4531\n",
      "Time so far is 3m 57s\n",
      "Epoch Number: 0, Batch Number: 1000, Training Loss: 0.6932, Validation Loss: 24.7256\n",
      "Time so far is 4m 4s\n",
      "Epoch Number: 0, Batch Number: 1025, Training Loss: 0.6932, Validation Loss: 25.5087\n",
      "Time so far is 4m 11s\n",
      "Epoch Number: 0, Batch Number: 1050, Training Loss: 0.6931, Validation Loss: 26.3025\n",
      "Time so far is 4m 18s\n",
      "Epoch Number: 0, Batch Number: 1075, Training Loss: 0.6931, Validation Loss: 24.4125\n",
      "Time so far is 4m 25s\n",
      "Epoch Number: 0, Batch Number: 1100, Training Loss: 0.6932, Validation Loss: 25.0012\n",
      "Time so far is 4m 32s\n",
      "Epoch Number: 0, Batch Number: 1125, Training Loss: 0.6932, Validation Loss: 24.6675\n",
      "Time so far is 4m 39s\n",
      "Epoch Number: 0, Batch Number: 1150, Training Loss: 0.6931, Validation Loss: 26.8912\n",
      "Time so far is 4m 46s\n",
      "Epoch Number: 0, Batch Number: 1175, Training Loss: 0.6932, Validation Loss: 23.0256\n",
      "Time so far is 4m 53s\n",
      "Epoch Number: 0, Batch Number: 1200, Training Loss: 0.6932, Validation Loss: 25.5750\n",
      "Time so far is 4m 58s\n",
      "Epoch Number: 0, Batch Number: 1225, Training Loss: 0.6932, Validation Loss: 27.5469\n",
      "Time so far is 5m 3s\n",
      "Epoch Number: 0, Batch Number: 1250, Training Loss: 0.6931, Validation Loss: 29.9775\n",
      "Time so far is 5m 7s\n",
      "Epoch Number: 0, Batch Number: 1275, Training Loss: 0.6932, Validation Loss: 25.3372\n",
      "Time so far is 5m 13s\n",
      "Epoch Number: 0, Batch Number: 1300, Training Loss: 0.6932, Validation Loss: 23.3706\n",
      "Time so far is 5m 19s\n",
      "Epoch Number: 0, Batch Number: 1325, Training Loss: 0.6931, Validation Loss: 25.4606\n",
      "Time so far is 5m 26s\n",
      "Epoch Number: 0, Batch Number: 1350, Training Loss: 0.6932, Validation Loss: 24.5669\n",
      "Time so far is 5m 34s\n",
      "Epoch Number: 0, Batch Number: 1375, Training Loss: 0.6931, Validation Loss: 25.7081\n",
      "Time so far is 5m 41s\n",
      "Epoch Number: 0, Batch Number: 1400, Training Loss: 0.6931, Validation Loss: 26.1775\n",
      "Time so far is 5m 47s\n",
      "Epoch Number: 0, Batch Number: 1425, Training Loss: 0.6932, Validation Loss: 27.0725\n",
      "Time so far is 5m 53s\n",
      "Epoch Number: 0, Batch Number: 1450, Training Loss: 0.6932, Validation Loss: 24.9612\n",
      "Time so far is 5m 60s\n",
      "Epoch Number: 0, Batch Number: 1475, Training Loss: 0.6932, Validation Loss: 25.1194\n",
      "Time so far is 6m 7s\n",
      "Epoch Number: 0, Batch Number: 1500, Training Loss: 0.6931, Validation Loss: 28.8175\n",
      "Time so far is 6m 12s\n",
      "Epoch Number: 0, Batch Number: 1525, Training Loss: 0.6932, Validation Loss: 22.5106\n",
      "Time so far is 6m 16s\n",
      "Epoch Number: 0, Batch Number: 1550, Training Loss: 0.6931, Validation Loss: 25.0963\n",
      "Time so far is 6m 21s\n",
      "Epoch Number: 0, Batch Number: 1575, Training Loss: 0.6932, Validation Loss: 24.0081\n",
      "Time so far is 6m 28s\n",
      "Epoch Number: 0, Batch Number: 1600, Training Loss: 0.6932, Validation Loss: 26.7112\n",
      "Time so far is 6m 36s\n",
      "Epoch Number: 0, Batch Number: 1625, Training Loss: 0.6932, Validation Loss: 24.3819\n",
      "Time so far is 6m 40s\n",
      "Epoch Number: 0, Batch Number: 1650, Training Loss: 0.6931, Validation Loss: 23.1275\n",
      "Time so far is 6m 44s\n",
      "Epoch Number: 0, Batch Number: 1675, Training Loss: 0.6932, Validation Loss: 26.4163\n",
      "Time so far is 6m 50s\n",
      "Epoch Number: 0, Batch Number: 1700, Training Loss: 0.6932, Validation Loss: 25.9281\n",
      "Time so far is 6m 56s\n",
      "Epoch Number: 0, Batch Number: 1725, Training Loss: 0.6932, Validation Loss: 24.8956\n",
      "Time so far is 7m 2s\n",
      "Epoch Number: 0, Batch Number: 1750, Training Loss: 0.6932, Validation Loss: 26.0888\n",
      "Time so far is 7m 8s\n",
      "Epoch Number: 0, Batch Number: 1775, Training Loss: 0.6931, Validation Loss: 24.3194\n",
      "Time so far is 7m 13s\n",
      "Epoch Number: 0, Batch Number: 1800, Training Loss: 0.6932, Validation Loss: 25.6337\n",
      "Time so far is 7m 19s\n",
      "Epoch Number: 0, Batch Number: 1825, Training Loss: 0.6931, Validation Loss: 23.2906\n",
      "Time so far is 7m 25s\n",
      "Epoch Number: 0, Batch Number: 1850, Training Loss: 0.6932, Validation Loss: 25.6694\n",
      "Time so far is 7m 32s\n",
      "Epoch Number: 0, Batch Number: 1875, Training Loss: 0.6931, Validation Loss: 27.8375\n",
      "Time so far is 7m 39s\n",
      "Epoch Number: 0, Batch Number: 1900, Training Loss: 0.6932, Validation Loss: 30.8875\n",
      "Time so far is 7m 45s\n",
      "Epoch Number: 0, Batch Number: 1925, Training Loss: 0.6931, Validation Loss: 28.3231\n",
      "Time so far is 7m 50s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Number: 0, Batch Number: 1950, Training Loss: 0.6931, Validation Loss: 27.1737\n",
      "Time so far is 7m 55s\n",
      "Epoch Number: 0, Batch Number: 1975, Training Loss: 0.6932, Validation Loss: 25.8531\n",
      "Time so far is 8m 0s\n",
      "Epoch Number: 0, Batch Number: 2000, Training Loss: 0.6931, Validation Loss: 30.6300\n",
      "Time so far is 8m 7s\n",
      "Epoch Number: 0, Batch Number: 2025, Training Loss: 0.6931, Validation Loss: 26.7006\n",
      "Time so far is 8m 14s\n",
      "Epoch Number: 0, Batch Number: 2050, Training Loss: 0.6932, Validation Loss: 25.4262\n",
      "Time so far is 8m 20s\n",
      "Epoch Number: 0, Batch Number: 2075, Training Loss: 0.6932, Validation Loss: 28.8181\n",
      "Time so far is 8m 25s\n",
      "Epoch Number: 0, Batch Number: 2100, Training Loss: 0.6932, Validation Loss: 22.7144\n",
      "Time so far is 8m 29s\n",
      "Epoch Number: 0, Batch Number: 2125, Training Loss: 0.6931, Validation Loss: 26.8244\n",
      "Time so far is 8m 34s\n",
      "Epoch Number: 0, Batch Number: 2150, Training Loss: 0.6932, Validation Loss: 26.4537\n",
      "Time so far is 8m 41s\n",
      "Epoch Number: 0, Batch Number: 2175, Training Loss: 0.6932, Validation Loss: 25.9706\n",
      "Time so far is 8m 49s\n",
      "Epoch Number: 0, Batch Number: 2200, Training Loss: 0.6932, Validation Loss: 28.3062\n",
      "Time so far is 8m 55s\n",
      "Epoch Number: 0, Batch Number: 2225, Training Loss: 0.6931, Validation Loss: 24.6238\n",
      "Time so far is 9m 2s\n",
      "Epoch Number: 0, Batch Number: 2250, Training Loss: 0.6931, Validation Loss: 23.4513\n",
      "Time so far is 9m 10s\n",
      "Epoch Number: 0, Batch Number: 2275, Training Loss: 0.6931, Validation Loss: 29.4306\n",
      "Time so far is 9m 17s\n",
      "Epoch Number: 0, Batch Number: 2300, Training Loss: 0.6931, Validation Loss: 26.4163\n",
      "Time so far is 9m 25s\n",
      "Epoch Number: 0, Batch Number: 2325, Training Loss: 0.6931, Validation Loss: 25.2213\n",
      "Time so far is 9m 31s\n",
      "Epoch Number: 0, Batch Number: 2350, Training Loss: 0.6931, Validation Loss: 28.8231\n",
      "Time so far is 9m 39s\n",
      "Epoch Number: 0, Batch Number: 2375, Training Loss: 0.6932, Validation Loss: 26.6356\n",
      "Time so far is 9m 47s\n",
      "Epoch Number: 0, Batch Number: 2400, Training Loss: 0.6931, Validation Loss: 24.6006\n",
      "Time so far is 9m 54s\n",
      "Epoch Number: 0, Batch Number: 2425, Training Loss: 0.6931, Validation Loss: 24.9291\n",
      "Time so far is 10m 0s\n",
      "Epoch Number: 0, Batch Number: 2450, Training Loss: 0.6931, Validation Loss: 24.2844\n",
      "Time so far is 10m 8s\n",
      "Epoch Number: 0, Batch Number: 2475, Training Loss: 0.6931, Validation Loss: 26.8656\n",
      "Time so far is 10m 16s\n",
      "Epoch Number: 0, Batch Number: 2500, Training Loss: 0.6931, Validation Loss: 24.6212\n",
      "Time so far is 10m 23s\n",
      "Epoch Number: 0, Batch Number: 2525, Training Loss: 0.6931, Validation Loss: 26.7606\n",
      "Time so far is 10m 29s\n",
      "Epoch Number: 0, Batch Number: 2550, Training Loss: 0.6932, Validation Loss: 26.2169\n",
      "Time so far is 10m 35s\n",
      "Epoch Number: 0, Batch Number: 2575, Training Loss: 0.6931, Validation Loss: 26.2325\n",
      "Time so far is 10m 43s\n",
      "Epoch Number: 0, Batch Number: 2600, Training Loss: 0.6932, Validation Loss: 29.9213\n",
      "Time so far is 10m 51s\n",
      "Epoch Number: 0, Batch Number: 2625, Training Loss: 0.6931, Validation Loss: 24.5756\n",
      "Time so far is 10m 58s\n",
      "Epoch Number: 0, Batch Number: 2650, Training Loss: 0.6931, Validation Loss: 28.5200\n",
      "Time so far is 11m 6s\n",
      "Epoch Number: 0, Batch Number: 2675, Training Loss: 0.6932, Validation Loss: 26.5450\n",
      "Time so far is 11m 14s\n",
      "Epoch Number: 0, Batch Number: 2700, Training Loss: 0.6932, Validation Loss: 26.1400\n",
      "Time so far is 11m 21s\n",
      "Epoch Number: 0, Batch Number: 2725, Training Loss: 0.6931, Validation Loss: 22.4875\n",
      "Time so far is 11m 28s\n",
      "Epoch Number: 0, Batch Number: 2750, Training Loss: 0.6931, Validation Loss: 24.7488\n",
      "Time so far is 11m 36s\n",
      "Epoch Number: 0, Batch Number: 2775, Training Loss: 0.6931, Validation Loss: 25.1244\n",
      "Time so far is 11m 43s\n",
      "Epoch Number: 0, Batch Number: 2800, Training Loss: 0.6931, Validation Loss: 27.7688\n",
      "Time so far is 11m 50s\n",
      "Epoch Number: 0, Batch Number: 2825, Training Loss: 0.6931, Validation Loss: 26.7787\n",
      "Time so far is 11m 58s\n",
      "Epoch Number: 0, Batch Number: 2850, Training Loss: 0.6931, Validation Loss: 26.4406\n",
      "Time so far is 12m 2s\n",
      "Epoch Number: 0, Batch Number: 2875, Training Loss: 0.6932, Validation Loss: 26.9088\n",
      "Time so far is 12m 6s\n",
      "Epoch Number: 0, Batch Number: 2900, Training Loss: 0.6931, Validation Loss: 26.6831\n",
      "Time so far is 12m 11s\n",
      "Epoch Number: 0, Batch Number: 2925, Training Loss: 0.6931, Validation Loss: 23.6931\n",
      "Time so far is 12m 14s\n",
      "Epoch Number: 0, Batch Number: 2950, Training Loss: 0.6932, Validation Loss: 27.8612\n",
      "Time so far is 12m 19s\n",
      "Epoch Number: 0, Batch Number: 2975, Training Loss: 0.6931, Validation Loss: 29.4438\n",
      "Time so far is 12m 24s\n",
      "Epoch Number: 0, Batch Number: 3000, Training Loss: 0.6931, Validation Loss: 23.8150\n",
      "Time so far is 12m 32s\n",
      "Epoch Number: 0, Batch Number: 3025, Training Loss: 0.6932, Validation Loss: 23.7594\n",
      "Time so far is 12m 39s\n",
      "Epoch Number: 0, Batch Number: 3050, Training Loss: 0.6931, Validation Loss: 25.8331\n",
      "Time so far is 12m 47s\n",
      "Epoch Number: 0, Batch Number: 3075, Training Loss: 0.6931, Validation Loss: 27.7650\n",
      "Time so far is 12m 55s\n",
      "Epoch Number: 0, Batch Number: 3100, Training Loss: 0.6931, Validation Loss: 27.1100\n",
      "Time so far is 13m 2s\n",
      "Epoch Number: 0, Batch Number: 3125, Training Loss: 0.6932, Validation Loss: 25.5419\n",
      "Time so far is 13m 10s\n",
      "Epoch Number: 0, Batch Number: 3150, Training Loss: 0.6931, Validation Loss: 28.2431\n",
      "Time so far is 13m 17s\n",
      "Epoch Number: 0, Batch Number: 3175, Training Loss: 0.6931, Validation Loss: 28.7906\n",
      "Time so far is 13m 26s\n",
      "Epoch Number: 0, Batch Number: 3200, Training Loss: 0.6932, Validation Loss: 25.4119\n",
      "Time so far is 13m 34s\n",
      "Epoch Number: 0, Batch Number: 3225, Training Loss: 0.6931, Validation Loss: 26.1950\n",
      "Time so far is 13m 42s\n",
      "Epoch Number: 0, Batch Number: 3250, Training Loss: 0.6931, Validation Loss: 24.4919\n",
      "Time so far is 13m 48s\n",
      "Epoch Number: 0, Batch Number: 3275, Training Loss: 0.6931, Validation Loss: 26.8025\n",
      "Time so far is 13m 56s\n",
      "Epoch Number: 0, Batch Number: 3300, Training Loss: 0.6931, Validation Loss: 25.4194\n",
      "Time so far is 14m 3s\n",
      "Epoch Number: 0, Batch Number: 3325, Training Loss: 0.6932, Validation Loss: 24.9969\n",
      "Time so far is 14m 10s\n",
      "Epoch Number: 0, Batch Number: 3350, Training Loss: 0.6931, Validation Loss: 27.1856\n",
      "Time so far is 14m 18s\n",
      "Epoch Number: 0, Batch Number: 3375, Training Loss: 0.6932, Validation Loss: 26.9481\n",
      "Time so far is 14m 26s\n",
      "Epoch Number: 0, Batch Number: 3400, Training Loss: 0.6931, Validation Loss: 30.3562\n",
      "Time so far is 14m 34s\n",
      "Epoch Number: 0, Batch Number: 3425, Training Loss: 0.6931, Validation Loss: 24.5403\n",
      "Time so far is 14m 42s\n",
      "Epoch Number: 0, Batch Number: 3450, Training Loss: 0.6931, Validation Loss: 23.1837\n",
      "Time so far is 14m 49s\n",
      "Epoch Number: 0, Batch Number: 3475, Training Loss: 0.6931, Validation Loss: 26.7625\n",
      "Time so far is 14m 57s\n",
      "Epoch Number: 0, Batch Number: 3500, Training Loss: 0.6931, Validation Loss: 27.0169\n",
      "Time so far is 15m 5s\n",
      "Epoch Number: 0, Batch Number: 3525, Training Loss: 0.6931, Validation Loss: 23.9756\n",
      "Time so far is 15m 11s\n",
      "Epoch Number: 0, Batch Number: 3550, Training Loss: 0.6931, Validation Loss: 26.8706\n",
      "Time so far is 15m 15s\n",
      "Epoch Number: 0, Batch Number: 3575, Training Loss: 0.6931, Validation Loss: 23.2294\n",
      "Time so far is 15m 19s\n",
      "Epoch Number: 0, Batch Number: 3600, Training Loss: 0.6931, Validation Loss: 25.7413\n",
      "Time so far is 15m 23s\n",
      "Epoch Number: 0, Batch Number: 3625, Training Loss: 0.6932, Validation Loss: 25.0012\n",
      "Time so far is 15m 27s\n",
      "Epoch Number: 0, Batch Number: 3650, Training Loss: 0.6931, Validation Loss: 24.1687\n",
      "Time so far is 15m 31s\n",
      "Epoch Number: 0, Batch Number: 3675, Training Loss: 0.6931, Validation Loss: 25.1794\n",
      "Time so far is 15m 36s\n",
      "Epoch Number: 0, Batch Number: 3700, Training Loss: 0.6931, Validation Loss: 26.8269\n",
      "Time so far is 15m 40s\n",
      "Epoch Number: 0, Batch Number: 3725, Training Loss: 0.6932, Validation Loss: 26.9500\n",
      "Time so far is 15m 45s\n",
      "Epoch Number: 0, Batch Number: 3750, Training Loss: 0.6932, Validation Loss: 23.3194\n",
      "Time so far is 15m 51s\n",
      "Epoch Number: 0, Batch Number: 3775, Training Loss: 0.6931, Validation Loss: 24.8181\n",
      "Time so far is 15m 59s\n",
      "Epoch Number: 0, Batch Number: 3800, Training Loss: 0.6931, Validation Loss: 23.1262\n",
      "Time so far is 16m 6s\n",
      "Epoch Number: 0, Batch Number: 3825, Training Loss: 0.6931, Validation Loss: 26.1650\n",
      "Time so far is 16m 13s\n",
      "Epoch Number: 0, Batch Number: 3850, Training Loss: 0.6931, Validation Loss: 26.3419\n",
      "Time so far is 16m 21s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Number: 0, Batch Number: 3875, Training Loss: 0.6931, Validation Loss: 25.5369\n",
      "Time so far is 16m 29s\n",
      "Epoch Number: 0, Batch Number: 3900, Training Loss: 0.6931, Validation Loss: 25.4444\n",
      "Time so far is 16m 36s\n",
      "Epoch Number: 0, Batch Number: 3925, Training Loss: 0.6931, Validation Loss: 24.2100\n",
      "Time so far is 16m 44s\n",
      "Epoch Number: 0, Batch Number: 3950, Training Loss: 0.6931, Validation Loss: 25.6469\n",
      "Time so far is 16m 51s\n",
      "Epoch Number: 0, Batch Number: 3975, Training Loss: 0.6931, Validation Loss: 31.2881\n",
      "Time so far is 17m 0s\n",
      "Epoch Number: 0, Batch Number: 4000, Training Loss: 0.6931, Validation Loss: 25.3412\n",
      "Time so far is 17m 8s\n",
      "Epoch Number: 0, Batch Number: 4025, Training Loss: 0.6931, Validation Loss: 22.8412\n",
      "Time so far is 17m 15s\n",
      "Epoch Number: 0, Batch Number: 4050, Training Loss: 0.6931, Validation Loss: 24.9225\n",
      "Time so far is 17m 23s\n",
      "Epoch Number: 0, Batch Number: 4075, Training Loss: 0.6931, Validation Loss: 22.8681\n",
      "Time so far is 17m 30s\n",
      "Epoch Number: 0, Batch Number: 4100, Training Loss: 0.6932, Validation Loss: 27.0650\n",
      "Time so far is 17m 37s\n",
      "Epoch Number: 0, Batch Number: 4125, Training Loss: 0.6931, Validation Loss: 24.2063\n",
      "Time so far is 17m 44s\n",
      "Epoch Number: 0, Batch Number: 4150, Training Loss: 0.6932, Validation Loss: 28.5750\n",
      "Time so far is 17m 52s\n",
      "Epoch Number: 0, Batch Number: 4175, Training Loss: 0.6931, Validation Loss: 23.9109\n",
      "Time so far is 17m 60s\n",
      "Epoch Number: 0, Batch Number: 4200, Training Loss: 0.6931, Validation Loss: 27.4381\n",
      "Time so far is 18m 8s\n",
      "Epoch Number: 0, Batch Number: 4225, Training Loss: 0.6931, Validation Loss: 23.8025\n",
      "Time so far is 18m 14s\n",
      "Epoch Number: 0, Batch Number: 4250, Training Loss: 0.6931, Validation Loss: 23.5250\n",
      "Time so far is 18m 21s\n",
      "Epoch Number: 0, Batch Number: 4275, Training Loss: 0.6931, Validation Loss: 23.8750\n",
      "Time so far is 18m 27s\n",
      "Epoch Number: 0, Batch Number: 4300, Training Loss: 0.6931, Validation Loss: 26.5069\n",
      "Time so far is 18m 35s\n",
      "Epoch Number: 0, Batch Number: 4325, Training Loss: 0.6931, Validation Loss: 27.6031\n",
      "Time so far is 18m 43s\n",
      "Epoch Number: 0, Batch Number: 4350, Training Loss: 0.6931, Validation Loss: 27.8581\n",
      "Time so far is 18m 51s\n",
      "Epoch Number: 0, Batch Number: 4375, Training Loss: 0.6931, Validation Loss: 24.5256\n",
      "Time so far is 18m 59s\n",
      "Epoch Number: 0, Batch Number: 4400, Training Loss: 0.6931, Validation Loss: 25.8487\n",
      "Time so far is 19m 6s\n",
      "Epoch Number: 0, Batch Number: 4425, Training Loss: 0.6931, Validation Loss: 25.1337\n",
      "Time so far is 19m 13s\n",
      "Epoch Number: 0, Batch Number: 4450, Training Loss: 0.6931, Validation Loss: 26.4700\n",
      "Time so far is 19m 21s\n",
      "Epoch Number: 0, Batch Number: 4475, Training Loss: 0.6932, Validation Loss: 27.6762\n",
      "Time so far is 19m 29s\n",
      "Epoch Number: 0, Batch Number: 4500, Training Loss: 0.6931, Validation Loss: 23.3100\n",
      "Time so far is 19m 36s\n",
      "Epoch Number: 0, Batch Number: 4525, Training Loss: 0.6931, Validation Loss: 24.4356\n",
      "Time so far is 19m 43s\n",
      "Epoch Number: 0, Batch Number: 4550, Training Loss: 0.6931, Validation Loss: 25.0394\n",
      "Time so far is 19m 50s\n",
      "Epoch Number: 0, Batch Number: 4575, Training Loss: 0.6932, Validation Loss: 26.6612\n",
      "Time so far is 19m 58s\n",
      "Epoch Number: 0, Batch Number: 4600, Training Loss: 0.6931, Validation Loss: 27.2712\n",
      "Time so far is 20m 6s\n",
      "Epoch Number: 0, Batch Number: 4625, Training Loss: 0.6931, Validation Loss: 24.0606\n",
      "Time so far is 20m 12s\n",
      "Epoch Number: 0, Batch Number: 4650, Training Loss: 0.6931, Validation Loss: 28.4269\n",
      "Time so far is 20m 17s\n",
      "Epoch Number: 0, Batch Number: 4675, Training Loss: 0.6931, Validation Loss: 23.0763\n",
      "Time so far is 20m 20s\n",
      "Epoch Number: 0, Batch Number: 4700, Training Loss: 0.6931, Validation Loss: 25.7387\n",
      "Time so far is 20m 25s\n",
      "Epoch Number: 0, Batch Number: 4725, Training Loss: 0.6931, Validation Loss: 26.6137\n",
      "Time so far is 20m 29s\n",
      "Epoch Number: 0, Batch Number: 4750, Training Loss: 0.6931, Validation Loss: 24.5725\n",
      "Time so far is 20m 33s\n",
      "Epoch Number: 0, Batch Number: 4775, Training Loss: 0.6931, Validation Loss: 28.4500\n",
      "Time so far is 20m 38s\n",
      "Epoch Number: 0, Batch Number: 4800, Training Loss: 0.6931, Validation Loss: 25.3825\n",
      "Time so far is 20m 42s\n",
      "Epoch Number: 0, Batch Number: 4825, Training Loss: 0.6931, Validation Loss: 24.3456\n",
      "Time so far is 20m 46s\n",
      "Epoch Number: 0, Batch Number: 4850, Training Loss: 0.6931, Validation Loss: 25.4325\n",
      "Time so far is 20m 53s\n",
      "Epoch Number: 0, Batch Number: 4875, Training Loss: 0.6931, Validation Loss: 24.7712\n",
      "Time so far is 21m 1s\n",
      "Epoch Number: 0, Batch Number: 4900, Training Loss: 0.6931, Validation Loss: 24.0431\n",
      "Time so far is 21m 8s\n",
      "Epoch Number: 0, Batch Number: 4925, Training Loss: 0.6931, Validation Loss: 23.9050\n",
      "Time so far is 21m 15s\n",
      "Epoch Number: 0, Batch Number: 4950, Training Loss: 0.6931, Validation Loss: 22.1656\n",
      "Time so far is 21m 22s\n",
      "Epoch Number: 0, Batch Number: 4975, Training Loss: 0.6931, Validation Loss: 28.3388\n",
      "Time so far is 21m 30s\n",
      "Epoch Number: 0, Batch Number: 5000, Training Loss: 0.6932, Validation Loss: 23.9919\n",
      "Time so far is 21m 38s\n",
      "Epoch Number: 0, Batch Number: 5025, Training Loss: 0.6931, Validation Loss: 27.6288\n",
      "Time so far is 21m 45s\n",
      "Epoch Number: 0, Batch Number: 5050, Training Loss: 0.6931, Validation Loss: 23.1681\n",
      "Time so far is 21m 52s\n",
      "Epoch Number: 0, Batch Number: 5075, Training Loss: 0.6932, Validation Loss: 28.0219\n",
      "Time so far is 21m 60s\n",
      "Epoch Number: 0, Batch Number: 5100, Training Loss: 0.6931, Validation Loss: 26.6069\n",
      "Time so far is 22m 7s\n",
      "Epoch Number: 0, Batch Number: 5125, Training Loss: 0.6931, Validation Loss: 23.0744\n",
      "Time so far is 22m 15s\n",
      "Epoch Number: 0, Batch Number: 5150, Training Loss: 0.6932, Validation Loss: 26.0344\n",
      "Time so far is 22m 22s\n",
      "Epoch Number: 0, Batch Number: 5175, Training Loss: 0.6931, Validation Loss: 26.5912\n",
      "Time so far is 22m 30s\n",
      "Epoch Number: 0, Batch Number: 5200, Training Loss: 0.6931, Validation Loss: 24.6719\n",
      "Time so far is 22m 38s\n",
      "Epoch Number: 0, Batch Number: 5225, Training Loss: 0.6931, Validation Loss: 29.2969\n",
      "Time so far is 22m 46s\n",
      "Epoch Number: 0, Batch Number: 5250, Training Loss: 0.6931, Validation Loss: 25.5794\n",
      "Time so far is 22m 54s\n",
      "Epoch Number: 0, Batch Number: 5275, Training Loss: 0.6931, Validation Loss: 21.8463\n",
      "Time so far is 23m 1s\n",
      "Epoch Number: 0, Batch Number: 5300, Training Loss: 0.6931, Validation Loss: 26.0269\n",
      "Time so far is 23m 8s\n",
      "Epoch Number: 0, Batch Number: 5325, Training Loss: 0.6931, Validation Loss: 28.5700\n",
      "Time so far is 23m 16s\n",
      "Epoch Number: 0, Batch Number: 5350, Training Loss: 0.6931, Validation Loss: 26.3712\n",
      "Time so far is 23m 25s\n",
      "Epoch Number: 0, Batch Number: 5375, Training Loss: 0.6931, Validation Loss: 24.6388\n",
      "Time so far is 23m 32s\n",
      "Epoch Number: 0, Batch Number: 5400, Training Loss: 0.6931, Validation Loss: 23.2975\n",
      "Time so far is 23m 40s\n",
      "Epoch Number: 0, Batch Number: 5425, Training Loss: 0.6932, Validation Loss: 30.1913\n",
      "Time so far is 23m 48s\n",
      "Epoch Number: 0, Batch Number: 5450, Training Loss: 0.6931, Validation Loss: 26.1094\n",
      "Time so far is 23m 56s\n",
      "Epoch Number: 0, Batch Number: 5475, Training Loss: 0.6931, Validation Loss: 25.0306\n",
      "Time so far is 24m 3s\n",
      "Epoch Number: 0, Batch Number: 5500, Training Loss: 0.6931, Validation Loss: 26.9119\n",
      "Time so far is 24m 11s\n",
      "Epoch Number: 0, Batch Number: 5525, Training Loss: 0.6931, Validation Loss: 29.2831\n",
      "Time so far is 24m 19s\n",
      "Epoch Number: 0, Batch Number: 5550, Training Loss: 0.6931, Validation Loss: 24.6163\n",
      "Time so far is 24m 27s\n",
      "Epoch Number: 0, Batch Number: 5575, Training Loss: 0.6931, Validation Loss: 25.8125\n",
      "Time so far is 24m 34s\n",
      "Epoch Number: 0, Batch Number: 5600, Training Loss: 0.6931, Validation Loss: 25.4225\n",
      "Time so far is 24m 42s\n",
      "Epoch Number: 0, Batch Number: 5625, Training Loss: 0.6931, Validation Loss: 23.1644\n",
      "Time so far is 24m 48s\n",
      "Epoch Number: 0, Batch Number: 5650, Training Loss: 0.6931, Validation Loss: 27.0825\n",
      "Time so far is 24m 55s\n",
      "Epoch Number: 0, Batch Number: 5675, Training Loss: 0.6932, Validation Loss: 27.3244\n",
      "Time so far is 24m 60s\n",
      "Epoch Number: 0, Batch Number: 5700, Training Loss: 0.6931, Validation Loss: 26.8456\n",
      "Time so far is 25m 4s\n",
      "Epoch Number: 0, Batch Number: 5725, Training Loss: 0.6931, Validation Loss: 26.7162\n",
      "Time so far is 25m 8s\n",
      "Epoch Number: 0, Batch Number: 5750, Training Loss: 0.6932, Validation Loss: 25.4913\n",
      "Time so far is 25m 12s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Number: 0, Batch Number: 5775, Training Loss: 0.6931, Validation Loss: 25.5087\n",
      "Time so far is 25m 16s\n",
      "Epoch Number: 0, Batch Number: 5800, Training Loss: 0.6931, Validation Loss: 27.7131\n",
      "Time so far is 25m 21s\n",
      "Epoch Number: 0, Batch Number: 5825, Training Loss: 0.6931, Validation Loss: 27.3356\n",
      "Time so far is 25m 25s\n",
      "Epoch Number: 0, Batch Number: 5850, Training Loss: 0.6931, Validation Loss: 25.3881\n",
      "Time so far is 25m 29s\n",
      "Epoch Number: 0, Batch Number: 5875, Training Loss: 0.6931, Validation Loss: 24.3100\n",
      "Time so far is 25m 33s\n",
      "Epoch Number: 0, Batch Number: 5900, Training Loss: 0.6932, Validation Loss: 26.0550\n",
      "Time so far is 25m 38s\n",
      "Epoch Number: 0, Batch Number: 5925, Training Loss: 0.6931, Validation Loss: 27.2269\n",
      "Time so far is 25m 45s\n",
      "Epoch Number: 0, Batch Number: 5950, Training Loss: 0.6931, Validation Loss: 26.6506\n",
      "Time so far is 25m 53s\n",
      "Epoch Number: 0, Batch Number: 5975, Training Loss: 0.6931, Validation Loss: 23.9256\n",
      "Time so far is 26m 1s\n",
      "Epoch Number: 0, Batch Number: 6000, Training Loss: 0.6931, Validation Loss: 26.2200\n",
      "Time so far is 26m 8s\n",
      "Epoch Number: 0, Batch Number: 6025, Training Loss: 0.6931, Validation Loss: 24.7419\n",
      "Time so far is 26m 16s\n",
      "Epoch Number: 0, Batch Number: 6050, Training Loss: 0.6931, Validation Loss: 26.2344\n",
      "Time so far is 26m 24s\n",
      "Epoch Number: 0, Batch Number: 6075, Training Loss: 0.6931, Validation Loss: 26.4181\n",
      "Time so far is 26m 32s\n",
      "Epoch Number: 0, Batch Number: 6100, Training Loss: 0.6931, Validation Loss: 25.3469\n",
      "Time so far is 26m 40s\n",
      "Epoch Number: 0, Batch Number: 6125, Training Loss: 0.6931, Validation Loss: 26.8238\n",
      "Time so far is 26m 48s\n",
      "Epoch Number: 0, Batch Number: 6150, Training Loss: 0.6931, Validation Loss: 26.5244\n",
      "Time so far is 26m 56s\n",
      "Epoch Number: 0, Batch Number: 6175, Training Loss: 0.6931, Validation Loss: 24.0162\n",
      "Time so far is 27m 3s\n",
      "Epoch Number: 0, Batch Number: 6200, Training Loss: 0.6931, Validation Loss: 27.9581\n",
      "Time so far is 27m 11s\n",
      "Epoch Number: 0, Batch Number: 6225, Training Loss: 0.6931, Validation Loss: 27.2125\n",
      "Time so far is 27m 19s\n",
      "Epoch Number: 0, Batch Number: 6250, Training Loss: 0.6931, Validation Loss: 24.2675\n",
      "Time so far is 27m 27s\n",
      "Epoch Number: 0, Batch Number: 6275, Training Loss: 0.6931, Validation Loss: 29.6612\n",
      "Time so far is 27m 36s\n",
      "Epoch Number: 0, Batch Number: 6300, Training Loss: 0.6931, Validation Loss: 27.0900\n",
      "Time so far is 27m 44s\n",
      "Epoch Number: 0, Batch Number: 6325, Training Loss: 0.6931, Validation Loss: 23.9356\n",
      "Time so far is 27m 51s\n",
      "Epoch Number: 0, Batch Number: 6350, Training Loss: 0.6931, Validation Loss: 24.5400\n",
      "Time so far is 27m 59s\n",
      "Epoch Number: 0, Batch Number: 6375, Training Loss: 0.6931, Validation Loss: 26.1194\n",
      "Time so far is 28m 7s\n",
      "Epoch Number: 0, Batch Number: 6400, Training Loss: 0.6931, Validation Loss: 23.9244\n",
      "Time so far is 28m 14s\n",
      "Epoch Number: 0, Batch Number: 6425, Training Loss: 0.6931, Validation Loss: 23.5206\n",
      "Time so far is 28m 21s\n",
      "Epoch Number: 0, Batch Number: 6450, Training Loss: 0.6931, Validation Loss: 25.6475\n",
      "Time so far is 28m 29s\n",
      "Epoch Number: 0, Batch Number: 6475, Training Loss: 0.6931, Validation Loss: 24.8231\n",
      "Time so far is 28m 37s\n",
      "Epoch Number: 0, Batch Number: 6500, Training Loss: 0.6931, Validation Loss: 26.5369\n",
      "Time so far is 28m 45s\n",
      "Epoch Number: 0, Batch Number: 6525, Training Loss: 0.6931, Validation Loss: 26.0413\n",
      "Time so far is 28m 53s\n",
      "Epoch Number: 0, Batch Number: 6550, Training Loss: 0.6931, Validation Loss: 24.6631\n",
      "Time so far is 29m 0s\n",
      "Epoch Number: 0, Batch Number: 6575, Training Loss: 0.6931, Validation Loss: 23.8744\n",
      "Time so far is 29m 8s\n",
      "Epoch Number: 0, Batch Number: 6600, Training Loss: 0.6931, Validation Loss: 24.3912\n",
      "Time so far is 29m 15s\n",
      "Epoch Number: 0, Batch Number: 6625, Training Loss: 0.6931, Validation Loss: 24.4369\n",
      "Time so far is 29m 22s\n",
      "Epoch Number: 0, Batch Number: 6650, Training Loss: 0.6931, Validation Loss: 28.5275\n",
      "Time so far is 29m 30s\n",
      "Epoch Number: 0, Batch Number: 6675, Training Loss: 0.6931, Validation Loss: 25.0487\n",
      "Time so far is 29m 38s\n",
      "Epoch Number: 0, Batch Number: 6700, Training Loss: 0.6931, Validation Loss: 27.0269\n",
      "Time so far is 29m 45s\n",
      "Epoch Number: 0, Batch Number: 6725, Training Loss: 0.6931, Validation Loss: 23.8875\n",
      "Time so far is 29m 53s\n",
      "Epoch Number: 0, Batch Number: 6750, Training Loss: 0.6931, Validation Loss: 26.7975\n",
      "Time so far is 30m 1s\n",
      "Epoch Number: 0, Batch Number: 6775, Training Loss: 0.6931, Validation Loss: 22.8263\n",
      "Time so far is 30m 8s\n",
      "Epoch Number: 0, Batch Number: 6800, Training Loss: 0.6931, Validation Loss: 26.5463\n",
      "Time so far is 30m 16s\n",
      "Epoch Number: 0, Batch Number: 6825, Training Loss: 0.6931, Validation Loss: 28.0425\n",
      "Time so far is 30m 24s\n",
      "Epoch Number: 0, Batch Number: 6850, Training Loss: 0.6931, Validation Loss: 24.3869\n",
      "Time so far is 30m 31s\n",
      "Epoch Number: 0, Batch Number: 6875, Training Loss: 0.6931, Validation Loss: 24.6950\n",
      "Time so far is 30m 39s\n",
      "Epoch Number: 0, Batch Number: 6900, Training Loss: 0.6931, Validation Loss: 24.5162\n",
      "Time so far is 30m 47s\n",
      "Epoch Number: 0, Batch Number: 6925, Training Loss: 0.6931, Validation Loss: 26.5969\n",
      "Time so far is 30m 55s\n",
      "Epoch Number: 0, Batch Number: 6950, Training Loss: 0.6931, Validation Loss: 28.1144\n",
      "Time so far is 31m 3s\n",
      "Epoch Number: 0, Batch Number: 6975, Training Loss: 0.6931, Validation Loss: 27.6281\n",
      "Time so far is 31m 12s\n",
      "Epoch Number: 0, Batch Number: 7000, Training Loss: 0.6931, Validation Loss: 26.7175\n",
      "Time so far is 31m 20s\n",
      "Epoch Number: 0, Batch Number: 7025, Training Loss: 0.6931, Validation Loss: 29.5669\n",
      "Time so far is 31m 28s\n",
      "Epoch Number: 0, Batch Number: 7050, Training Loss: 0.6931, Validation Loss: 25.3231\n",
      "Time so far is 31m 36s\n",
      "Epoch Number: 0, Batch Number: 7075, Training Loss: 0.6931, Validation Loss: 26.0987\n",
      "Time so far is 31m 44s\n",
      "Epoch Number: 0, Batch Number: 7100, Training Loss: 0.6931, Validation Loss: 24.8156\n",
      "Time so far is 31m 52s\n",
      "Epoch Number: 0, Batch Number: 7125, Training Loss: 0.6931, Validation Loss: 27.4550\n",
      "Time so far is 31m 60s\n",
      "Epoch Number: 0, Batch Number: 7150, Training Loss: 0.6931, Validation Loss: 28.4187\n",
      "Time so far is 32m 8s\n",
      "Epoch Number: 0, Batch Number: 7175, Training Loss: 0.6931, Validation Loss: 23.8231\n",
      "Time so far is 32m 15s\n",
      "Epoch Number: 0, Batch Number: 7200, Training Loss: 0.6931, Validation Loss: 24.8788\n",
      "Time so far is 32m 22s\n",
      "Epoch Number: 0, Batch Number: 7225, Training Loss: 0.6931, Validation Loss: 26.7437\n",
      "Time so far is 32m 29s\n",
      "Epoch Number: 0, Batch Number: 7250, Training Loss: 0.6931, Validation Loss: 26.6288\n",
      "Time so far is 32m 37s\n",
      "Epoch Number: 0, Batch Number: 7275, Training Loss: 0.6931, Validation Loss: 27.0994\n",
      "Time so far is 32m 45s\n",
      "Epoch Number: 0, Batch Number: 7300, Training Loss: 0.6931, Validation Loss: 24.5556\n",
      "Time so far is 32m 52s\n",
      "Epoch Number: 0, Batch Number: 7325, Training Loss: 0.6931, Validation Loss: 25.4831\n",
      "Time so far is 33m 0s\n",
      "Epoch Number: 0, Batch Number: 7350, Training Loss: 0.6931, Validation Loss: 27.6419\n",
      "Time so far is 33m 8s\n",
      "Epoch Number: 0, Batch Number: 7375, Training Loss: 0.6931, Validation Loss: 23.5413\n",
      "Time so far is 33m 16s\n",
      "Epoch Number: 0, Batch Number: 7400, Training Loss: 0.6931, Validation Loss: 27.5138\n",
      "Time so far is 33m 24s\n",
      "Epoch Number: 0, Batch Number: 7425, Training Loss: 0.6931, Validation Loss: 26.9006\n",
      "Time so far is 33m 32s\n",
      "Epoch Number: 0, Batch Number: 7450, Training Loss: 0.6931, Validation Loss: 24.1019\n",
      "Time so far is 33m 39s\n",
      "Epoch Number: 0, Batch Number: 7475, Training Loss: 0.6931, Validation Loss: 24.9219\n",
      "Time so far is 33m 47s\n",
      "Epoch Number: 0, Batch Number: 7500, Training Loss: 0.6931, Validation Loss: 21.7669\n",
      "Time so far is 33m 54s\n",
      "Epoch Number: 0, Batch Number: 7525, Training Loss: 0.6931, Validation Loss: 26.7206\n",
      "Time so far is 34m 2s\n",
      "Epoch Number: 0, Batch Number: 7550, Training Loss: 0.6931, Validation Loss: 23.5422\n",
      "Time so far is 34m 10s\n",
      "Epoch Number: 0, Batch Number: 7575, Training Loss: 0.6931, Validation Loss: 24.3525\n",
      "Time so far is 34m 18s\n",
      "Epoch Number: 0, Batch Number: 7600, Training Loss: 0.6931, Validation Loss: 25.4594\n",
      "Time so far is 34m 25s\n",
      "Epoch Number: 0, Batch Number: 7625, Training Loss: 0.6931, Validation Loss: 24.8406\n",
      "Time so far is 34m 33s\n",
      "Epoch Number: 0, Batch Number: 7650, Training Loss: 0.6931, Validation Loss: 28.4506\n",
      "Time so far is 34m 41s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Number: 0, Batch Number: 7675, Training Loss: 0.6931, Validation Loss: 26.2681\n",
      "Time so far is 34m 48s\n",
      "Epoch Number: 0, Batch Number: 7700, Training Loss: 0.6931, Validation Loss: 24.4956\n",
      "Time so far is 34m 55s\n",
      "Epoch Number: 0, Batch Number: 7725, Training Loss: 0.6931, Validation Loss: 24.5675\n",
      "Time so far is 35m 2s\n",
      "Epoch Number: 0, Batch Number: 7750, Training Loss: 0.6931, Validation Loss: 22.7606\n",
      "Time so far is 35m 8s\n",
      "Epoch Number: 0, Batch Number: 7775, Training Loss: 0.6931, Validation Loss: 25.6131\n",
      "Time so far is 35m 16s\n",
      "Epoch Number: 0, Batch Number: 7800, Training Loss: 0.6931, Validation Loss: 23.0444\n",
      "Time so far is 35m 22s\n",
      "Epoch Number: 0, Batch Number: 7825, Training Loss: 0.6931, Validation Loss: 25.2544\n",
      "Time so far is 35m 30s\n",
      "Epoch Number: 0, Batch Number: 7850, Training Loss: 0.6931, Validation Loss: 24.2900\n",
      "Time so far is 35m 37s\n",
      "Epoch Number: 0, Batch Number: 7875, Training Loss: 0.6931, Validation Loss: 25.8244\n",
      "Time so far is 35m 44s\n",
      "Epoch Number: 0, Batch Number: 7900, Training Loss: 0.6931, Validation Loss: 24.0038\n",
      "Time so far is 35m 51s\n",
      "Epoch Number: 0, Batch Number: 7925, Training Loss: 0.6931, Validation Loss: 25.7456\n",
      "Time so far is 35m 58s\n",
      "Epoch Number: 0, Batch Number: 7950, Training Loss: 0.6931, Validation Loss: 26.5044\n",
      "Time so far is 36m 6s\n",
      "Epoch Number: 0, Batch Number: 7975, Training Loss: 0.6931, Validation Loss: 22.5419\n",
      "Time so far is 36m 12s\n",
      "Epoch Number: 0, Batch Number: 8000, Training Loss: 0.6931, Validation Loss: 24.4694\n",
      "Time so far is 36m 20s\n",
      "Epoch Number: 0, Batch Number: 8025, Training Loss: 0.6931, Validation Loss: 25.2475\n",
      "Time so far is 36m 27s\n",
      "Epoch Number: 0, Batch Number: 8050, Training Loss: 0.6931, Validation Loss: 28.2875\n",
      "Time so far is 36m 35s\n",
      "Epoch Number: 0, Batch Number: 8075, Training Loss: 0.6931, Validation Loss: 23.9097\n",
      "Time so far is 36m 43s\n",
      "Epoch Number: 0, Batch Number: 8100, Training Loss: 0.6931, Validation Loss: 23.7244\n",
      "Time so far is 36m 50s\n",
      "Epoch Number: 0, Batch Number: 8125, Training Loss: 0.6931, Validation Loss: 24.3569\n",
      "Time so far is 36m 57s\n",
      "Epoch Number: 0, Batch Number: 8150, Training Loss: 0.6931, Validation Loss: 24.5206\n",
      "Time so far is 37m 5s\n",
      "Epoch Number: 0, Batch Number: 8175, Training Loss: 0.6931, Validation Loss: 27.0800\n",
      "Time so far is 37m 13s\n",
      "Epoch Number: 0, Batch Number: 8200, Training Loss: 0.6931, Validation Loss: 27.2044\n",
      "Time so far is 37m 21s\n",
      "Epoch Number: 0, Batch Number: 8225, Training Loss: 0.6931, Validation Loss: 24.7850\n",
      "Time so far is 37m 28s\n",
      "Epoch Number: 0, Batch Number: 8250, Training Loss: 0.6931, Validation Loss: 23.9106\n",
      "Time so far is 37m 36s\n",
      "Epoch Number: 0, Batch Number: 8275, Training Loss: 0.6931, Validation Loss: 23.5306\n",
      "Time so far is 37m 43s\n",
      "Epoch Number: 0, Batch Number: 8300, Training Loss: 0.6931, Validation Loss: 24.7538\n",
      "Time so far is 37m 50s\n",
      "Epoch Number: 0, Batch Number: 8325, Training Loss: 0.6931, Validation Loss: 26.3269\n",
      "Time so far is 37m 57s\n",
      "Epoch Number: 0, Batch Number: 8350, Training Loss: 0.6931, Validation Loss: 24.4925\n",
      "Time so far is 38m 4s\n",
      "Epoch Number: 0, Batch Number: 8375, Training Loss: 0.6931, Validation Loss: 26.0069\n",
      "Time so far is 38m 11s\n",
      "Epoch Number: 0, Batch Number: 8400, Training Loss: 0.6931, Validation Loss: 26.1537\n",
      "Time so far is 38m 18s\n",
      "Epoch Number: 0, Batch Number: 8425, Training Loss: 0.6931, Validation Loss: 24.8912\n",
      "Time so far is 38m 26s\n",
      "Epoch Number: 0, Batch Number: 8450, Training Loss: 0.6931, Validation Loss: 26.3222\n",
      "Time so far is 38m 34s\n",
      "Epoch Number: 0, Batch Number: 8475, Training Loss: 0.6931, Validation Loss: 31.1003\n",
      "Time so far is 38m 43s\n",
      "Epoch Number: 0, Batch Number: 8500, Training Loss: 0.6931, Validation Loss: 25.8163\n",
      "Time so far is 38m 51s\n",
      "Epoch Number: 0, Batch Number: 8525, Training Loss: 0.6931, Validation Loss: 28.1556\n",
      "Time so far is 38m 59s\n",
      "Epoch Number: 0, Batch Number: 8550, Training Loss: 0.6931, Validation Loss: 27.1825\n",
      "Time so far is 39m 7s\n",
      "Epoch Number: 0, Batch Number: 8575, Training Loss: 0.6931, Validation Loss: 22.5487\n",
      "Time so far is 39m 14s\n",
      "Epoch Number: 0, Batch Number: 8600, Training Loss: 0.6931, Validation Loss: 27.2063\n",
      "Time so far is 39m 22s\n",
      "Epoch Number: 0, Batch Number: 8625, Training Loss: 0.6931, Validation Loss: 26.6288\n",
      "Time so far is 39m 29s\n",
      "Epoch Number: 0, Batch Number: 8650, Training Loss: 0.6931, Validation Loss: 23.9725\n",
      "Time so far is 39m 37s\n",
      "Epoch Number: 0, Batch Number: 8675, Training Loss: 0.6931, Validation Loss: 24.1644\n",
      "Time so far is 39m 44s\n",
      "Epoch Number: 0, Batch Number: 8700, Training Loss: 0.6931, Validation Loss: 27.2281\n",
      "Time so far is 39m 52s\n",
      "Epoch Number: 0, Batch Number: 8725, Training Loss: 0.6931, Validation Loss: 24.8656\n",
      "Time so far is 39m 59s\n",
      "Epoch Number: 0, Batch Number: 8750, Training Loss: 0.6931, Validation Loss: 25.1944\n",
      "Time so far is 40m 7s\n",
      "Epoch Number: 0, Batch Number: 8775, Training Loss: 0.6931, Validation Loss: 24.7888\n",
      "Time so far is 40m 14s\n",
      "Epoch Number: 0, Batch Number: 8800, Training Loss: 0.6931, Validation Loss: 25.7937\n",
      "Time so far is 40m 22s\n",
      "Epoch Number: 0, Batch Number: 8825, Training Loss: 0.6931, Validation Loss: 26.2950\n",
      "Time so far is 40m 29s\n",
      "Epoch Number: 0, Batch Number: 8850, Training Loss: 0.6931, Validation Loss: 27.1112\n",
      "Time so far is 40m 34s\n",
      "Epoch Number: 0, Batch Number: 8875, Training Loss: 0.6931, Validation Loss: 27.2794\n",
      "Time so far is 40m 39s\n",
      "Epoch Number: 0, Batch Number: 8900, Training Loss: 0.6931, Validation Loss: 25.7150\n",
      "Time so far is 40m 47s\n",
      "Epoch Number: 0, Batch Number: 8925, Training Loss: 0.6931, Validation Loss: 25.1288\n",
      "Time so far is 40m 54s\n",
      "Epoch Number: 0, Batch Number: 8950, Training Loss: 0.6931, Validation Loss: 23.9531\n",
      "Time so far is 41m 2s\n",
      "Epoch Number: 0, Batch Number: 8975, Training Loss: 0.6931, Validation Loss: 24.1181\n",
      "Time so far is 41m 9s\n",
      "Epoch Number: 0, Batch Number: 9000, Training Loss: 0.6931, Validation Loss: 24.3350\n",
      "Time so far is 41m 17s\n",
      "Epoch Number: 0, Batch Number: 9025, Training Loss: 0.6931, Validation Loss: 24.3637\n",
      "Time so far is 41m 24s\n",
      "Epoch Number: 0, Batch Number: 9050, Training Loss: 0.6931, Validation Loss: 26.5309\n",
      "Time so far is 41m 32s\n",
      "Epoch Number: 0, Batch Number: 9075, Training Loss: 0.6931, Validation Loss: 20.6650\n",
      "Time so far is 41m 39s\n",
      "Epoch Number: 0, Batch Number: 9100, Training Loss: 0.6931, Validation Loss: 25.6119\n",
      "Time so far is 41m 47s\n",
      "Epoch Number: 0, Batch Number: 9125, Training Loss: 0.6931, Validation Loss: 27.1163\n",
      "Time so far is 41m 55s\n",
      "Epoch Number: 0, Batch Number: 9150, Training Loss: 0.6931, Validation Loss: 25.8081\n",
      "Time so far is 42m 2s\n",
      "Epoch Number: 0, Batch Number: 9175, Training Loss: 0.6931, Validation Loss: 26.0756\n",
      "Time so far is 42m 10s\n",
      "Epoch Number: 0, Batch Number: 9200, Training Loss: 0.6931, Validation Loss: 25.0119\n",
      "Time so far is 42m 18s\n",
      "Epoch Number: 0, Batch Number: 9225, Training Loss: 0.6931, Validation Loss: 25.9834\n",
      "Time so far is 42m 25s\n",
      "Epoch Number: 0, Batch Number: 9250, Training Loss: 0.6931, Validation Loss: 23.5912\n",
      "Time so far is 42m 29s\n",
      "Epoch Number: 0, Batch Number: 9275, Training Loss: 0.6931, Validation Loss: 26.4769\n",
      "Time so far is 42m 33s\n",
      "Epoch Number: 0, Batch Number: 9300, Training Loss: 0.6931, Validation Loss: 28.3094\n",
      "Time so far is 42m 38s\n",
      "Epoch Number: 0, Batch Number: 9325, Training Loss: 0.6931, Validation Loss: 23.7756\n",
      "Time so far is 42m 42s\n",
      "Epoch Number: 0, Batch Number: 9350, Training Loss: 0.6931, Validation Loss: 28.4400\n",
      "Time so far is 42m 47s\n",
      "Epoch Number: 0, Batch Number: 9375, Training Loss: 0.6931, Validation Loss: 24.1194\n",
      "Time so far is 42m 51s\n",
      "Epoch Number: 0, Batch Number: 9400, Training Loss: 0.6931, Validation Loss: 24.5369\n",
      "Time so far is 42m 55s\n",
      "Epoch Number: 0, Batch Number: 9425, Training Loss: 0.6931, Validation Loss: 25.6250\n",
      "Time so far is 42m 59s\n",
      "Epoch Number: 0, Batch Number: 9450, Training Loss: 0.6931, Validation Loss: 26.1194\n",
      "Time so far is 43m 7s\n",
      "Epoch Number: 0, Batch Number: 9475, Training Loss: 0.6931, Validation Loss: 20.8962\n",
      "Time so far is 43m 14s\n",
      "Epoch Number: 0, Batch Number: 9500, Training Loss: 0.6931, Validation Loss: 28.8294\n",
      "Time so far is 43m 22s\n",
      "Epoch Number: 0, Batch Number: 9525, Training Loss: 0.6931, Validation Loss: 25.8256\n",
      "Time so far is 43m 30s\n",
      "Epoch Number: 0, Batch Number: 9550, Training Loss: 0.6931, Validation Loss: 25.0425\n",
      "Time so far is 43m 37s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Number: 0, Batch Number: 9575, Training Loss: 0.6931, Validation Loss: 26.8525\n",
      "Time so far is 43m 45s\n",
      "Epoch Number: 0, Batch Number: 9600, Training Loss: 0.6931, Validation Loss: 25.9850\n",
      "Time so far is 43m 53s\n",
      "Epoch Number: 0, Batch Number: 9625, Training Loss: 0.6931, Validation Loss: 25.6062\n",
      "Time so far is 44m 1s\n",
      "Epoch Number: 0, Batch Number: 9650, Training Loss: 0.6931, Validation Loss: 26.8631\n",
      "Time so far is 44m 9s\n",
      "Epoch Number: 0, Batch Number: 9675, Training Loss: 0.6931, Validation Loss: 27.1206\n",
      "Time so far is 44m 17s\n",
      "Epoch Number: 0, Batch Number: 9700, Training Loss: 0.6931, Validation Loss: 26.6837\n",
      "Time so far is 44m 25s\n",
      "Epoch Number: 0, Batch Number: 9725, Training Loss: 0.6931, Validation Loss: 22.8994\n",
      "Time so far is 44m 32s\n",
      "Epoch Number: 0, Batch Number: 9750, Training Loss: 0.6931, Validation Loss: 27.7838\n",
      "Time so far is 44m 40s\n",
      "Epoch Number: 0, Batch Number: 9775, Training Loss: 0.6931, Validation Loss: 27.2138\n",
      "Time so far is 44m 48s\n",
      "Epoch Number: 0, Batch Number: 9800, Training Loss: 0.6931, Validation Loss: 31.2725\n",
      "Time so far is 44m 56s\n",
      "Epoch Number: 0, Batch Number: 9825, Training Loss: 0.6931, Validation Loss: 29.0331\n",
      "Time so far is 45m 1s\n",
      "Epoch Number: 0, Batch Number: 9850, Training Loss: 0.6931, Validation Loss: 27.5794\n",
      "Time so far is 45m 6s\n",
      "Epoch Number: 0, Batch Number: 9875, Training Loss: 0.6931, Validation Loss: 26.1244\n",
      "Time so far is 45m 10s\n",
      "Epoch Number: 0, Batch Number: 9900, Training Loss: 0.6931, Validation Loss: 24.4069\n",
      "Time so far is 45m 14s\n",
      "Epoch Number: 0, Batch Number: 9925, Training Loss: 0.6931, Validation Loss: 24.8888\n",
      "Time so far is 45m 18s\n",
      "Epoch Number: 0, Batch Number: 9950, Training Loss: 0.6931, Validation Loss: 22.6575\n",
      "Time so far is 45m 22s\n",
      "Epoch Number: 0, Batch Number: 9975, Training Loss: 0.6931, Validation Loss: 26.3400\n",
      "Time so far is 45m 26s\n",
      "Epoch Number: 0, Batch Number: 10000, Training Loss: 0.6931, Validation Loss: 26.3825\n",
      "Time so far is 45m 31s\n",
      "\n",
      "Training complete in 45m 31s\n",
      "Best loss: 0.693158\n"
     ]
    }
   ],
   "source": [
    "best_model, train_plot_losses, validation_plot_losses = training.train_model(dnc, data_loader, copy_task_loss, optimizer, None, print_every=25, num_epochs = 1, deep_copy_desired=False, validation_criterion=error_bits_per_sequence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
